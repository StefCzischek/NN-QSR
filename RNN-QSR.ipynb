{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45797e5b",
   "metadata": {},
   "source": [
    "# Exploring RNN architectures for Quantum state representation\n",
    "\n",
    "A couple types of rnn:\n",
    "    \n",
    "    Gru\n",
    "    \n",
    "    Lstm\n",
    "    \n",
    "    2D lstm (if we have a 2d input)\n",
    "    \n",
    "    Tranformer (masked) (masked makes it causal whereas unmasked would be non causal)\n",
    "    \n",
    "Extra networks:\n",
    "\n",
    "    Echo state networks  -> look into\n",
    "    \n",
    "    Resevoir computing cells -> look into\n",
    "        \n",
    "        Apparently you only train the output weights -> avoids a bunch of backprop\n",
    "        \n",
    "        but we use it in a recurrent fashion so you need to backprop still\n",
    "        \n",
    "        Look at BYOL for this (might not be possible)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4fa75",
   "metadata": {},
   "source": [
    "# Quantum state to represent\n",
    "\n",
    "- Start with Rydberg system\n",
    "\n",
    "Transverse and longitudinal view of ising model\n",
    "\n",
    "Excited state encourages nearby (within radius $R_b$)states to tend towards ground states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7f3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math,time\n",
    "import torch\n",
    "from torch import nn\n",
    "ngpu=1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89741030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.40522\n"
     ]
    }
   ],
   "source": [
    "# Hamiltonian parameters\n",
    "Lx = 8      # Linear size in x direction\n",
    "Ly = 8      # Linear size in y direction\n",
    "N = Lx*Ly   # Total number of atoms\n",
    "V = 7.0     # Strength of Van der Waals interaction\n",
    "Omega = 1.0 # Rabi frequency\n",
    "delta = 1.0 # Detuning \n",
    "exact_energy = {16:-0.45776822,64:-0.40522,144:-0.38852,256:0.38052}[Lx*Ly]\n",
    "print(exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d13afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(nn.Module):\n",
    "    def __init__(self,device=device):\n",
    "        self.device=device\n",
    "        super(Sampler, self).__init__()\n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def sample_with_labels(self,B,L,grad=False):\n",
    "        \"\"\"Inputs:\n",
    "            B (int) - The number of states to generate in parallel\n",
    "            L (int) - The length of generated vectors\n",
    "            grad (boolean) - Whether or not to use gradients\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logppl - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i+B*j] had one spin flipped at position j\n",
    "                    relative to s[i]\n",
    "        \"\"\"\n",
    "        sample=self.sample(B,L)\n",
    "        sflip = torch.zeros([B,L,L,1],device=self.device)\n",
    "        for j in range(L):\n",
    "            #get all of the states with one spin flipped\n",
    "            sflip[:,j] = sample*1.0\n",
    "            sflip[:,j,j] = 1-sflip[:,j,j]\n",
    "        #compute all of their logscale probabilities\n",
    "        if not grad:\n",
    "            with torch.no_grad():\n",
    "                probs = self.logprobability(sflip.view([B*L,L,1]))\n",
    "        else:\n",
    "            probs = self.logprobability(sflip.view([B*L,L,1]))\n",
    "            \n",
    "        #might make sflip shape [B,L] in the future\n",
    "        return sample,probs.reshape([B,L])\n",
    "    \n",
    "    def sample_with_labelsALT(self,B,L,grad=False):\n",
    "        \"\"\"Returns:\n",
    "            samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
    "                       when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
    "            sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
    "        \"\"\"\n",
    "        sample,probs = self.sample_with_labels(B,L,grad)\n",
    "        #get the average of our logprobabilities and divide by 2\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        #compute the sum with a constant multiplied to keep the sum closeish to 1\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5dec33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sampler in module __main__:\n",
      "\n",
      "class Sampler(torch.nn.modules.module.Module)\n",
      " |  Sampler(device=device(type='cuda', index=0))\n",
      " |  \n",
      " |  Base class for all neural network modules.\n",
      " |  \n",
      " |  Your models should also subclass this class.\n",
      " |  \n",
      " |  Modules can also contain other Modules, allowing to nest them in\n",
      " |  a tree structure. You can assign the submodules as regular attributes::\n",
      " |  \n",
      " |      import torch.nn as nn\n",
      " |      import torch.nn.functional as F\n",
      " |  \n",
      " |      class Model(nn.Module):\n",
      " |          def __init__(self):\n",
      " |              super(Model, self).__init__()\n",
      " |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      " |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      " |  \n",
      " |          def forward(self, x):\n",
      " |              x = F.relu(self.conv1(x))\n",
      " |              return F.relu(self.conv2(x))\n",
      " |  \n",
      " |  Submodules assigned in this way will be registered, and will have their\n",
      " |  parameters converted too when you call :meth:`to`, etc.\n",
      " |  \n",
      " |  :ivar training: Boolean represents whether this module is in training or\n",
      " |                  evaluation mode.\n",
      " |  :vartype training: bool\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sampler\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, device=device(type='cuda', index=0))\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  logprobability(self, input)\n",
      " |      Compute the logscale probability of a given state\n",
      " |      Inputs:\n",
      " |          input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |      Returns:\n",
      " |          logp - [B] size vector of logscale probability labels\n",
      " |  \n",
      " |  sample(self, B, L)\n",
      " |      Generates a set states\n",
      " |      Inputs:\n",
      " |          B (int)            - The number of states to generate in parallel\n",
      " |          L (int)            - The length of generated vectors\n",
      " |      Returns:\n",
      " |          samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |  \n",
      " |  sample_with_labels(self, B, L, grad=False)\n",
      " |      Inputs:\n",
      " |          B (int) - The number of states to generate in parallel\n",
      " |          L (int) - The length of generated vectors\n",
      " |          grad (boolean) - Whether or not to use gradients\n",
      " |      Returns:\n",
      " |          samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |          logppl - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i+B*j] had one spin flipped at position j\n",
      " |                  relative to s[i]\n",
      " |  \n",
      " |  sample_with_labelsALT(self, B, L, grad=False)\n",
      " |      Returns:\n",
      " |      samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |      logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
      " |                 when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
      " |      sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |          or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb026d6",
   "metadata": {},
   "source": [
    "# Simple RNN to start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e73a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Sampler):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    def __init__(self,rnntype=\"GRU\",Nh=128,device=device, **kwargs):\n",
    "        super(RNN, self).__init__(device=device)\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=1,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(128,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.rnntype=rnntype\n",
    "        self.to(device)\n",
    "    def forward(self, input):\n",
    "        # h0 is shape [d*numlayers,B,H] but D=numlayers=1 so\n",
    "        # h0 has shape [1,B,H]\n",
    "        \n",
    "        if self.rnntype==\"LSTM\":\n",
    "            h0=[torch.zeros([1,input.shape[0],128]).to(device),\n",
    "               torch.zeros([1,input.shape[0],128]).to(device)]\n",
    "            #h0 and c0\n",
    "        else:\n",
    "            h0=torch.zeros([1,input.shape[0],128]).to(device)\n",
    "        out,h=self.rnn(input,h0)\n",
    "        return self.lin(out)\n",
    "    \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        #Input should have shape [B,L,1]\n",
    "        B,L,one=input.shape\n",
    "        \n",
    "        #first prediction is with the zero input vector\n",
    "        data=torch.zeros([B,L,one]).to(self.device)\n",
    "        #data is the input vector shifted one to the right, with the very first entry set to zero instead of using pbc\n",
    "        data[:,1:,:]=input[:,:-1,:]\n",
    "        \n",
    "        #real is going to be a set of actual values\n",
    "        real=input\n",
    "        #and pred is going to be a set of probabilities\n",
    "        #if real[i]=1 than you muptiply your conditional probability by pred[i]\n",
    "        #if real[i]=0 than you muliply by 1-pred[i]\n",
    "        \n",
    "        #probability predictions should be done WITH gradients\n",
    "        #with torch.no_grad():\n",
    "        \n",
    "        pred = self.forward(data)\n",
    "        ones = real*pred\n",
    "        zeros=(1-real)*(1-pred)\n",
    "        total = ones+zeros\n",
    "        #this is the sum you see in the cell above\n",
    "        logp=torch.sum(torch.log(total),dim=1).squeeze(1)\n",
    "        return logp\n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        if self.rnntype==\"LSTM\":\n",
    "            h=[torch.zeros([1,B,128]).to(device),\n",
    "               torch.zeros([1,B,128]).to(device)]\n",
    "            #h is h0 and c0\n",
    "        else:\n",
    "            h=torch.zeros([1,B,128]).to(device)\n",
    "        #Sample set will have shape [N,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [N,L+1,1] \n",
    "        input = torch.zeros([B,L+1,1],device=device)\n",
    "        #sampling can be done without gradients\n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #run the rnn on shape [B,1,1]\n",
    "            \n",
    "            out,h=self.rnn(input[:,idx-1:idx,:],h)\n",
    "            out=out[:,0,:]\n",
    "            #if probs[i]=1 then there should be a 100% chance that sample[i]=1\n",
    "            #if probs[i]=0 then there should be a 0% chance that sample[i]=1\n",
    "            #stands that we generate a random uniform u and take int(u<probs) as our sample\n",
    "            probs=self.lin(out)\n",
    "            sample = (torch.rand([B,1],device=device)<probs).to(torch.float32)\n",
    "            input[:,idx,:]=sample\n",
    "        #input's first entry is zero to get a predction for the first atom\n",
    "        return input[:,1:,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e73636",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def Vij(Ly,Lx,Rcutoff,V,matrix):\n",
    "    #matrix will be size [Lx*Ly,Lx*Ly]\n",
    "    \n",
    "    i,j=cuda.grid(2)\n",
    "    if i>Ly or j>Lx:\n",
    "        return\n",
    "    R=Rcutoff**6\n",
    "    #flatten two indices into one\n",
    "    idx = Ly*j+i\n",
    "    # only fill in the upper diagonal\n",
    "    for k in range(idx+1,Lx*Ly):\n",
    "        #expand one index into two\n",
    "        i2 = k%Ly\n",
    "        j2=k//Ly\n",
    "        div = ((i2-i)**2+(j2-j)**2)**3\n",
    "        if div<=R:\n",
    "            matrix[idx][k]=V/div\n",
    "    \n",
    "\n",
    "class Hamiltonian():\n",
    "    def __init__(self,Lx,Ly,V,Omega,delta,R=2.01,device=device):\n",
    "        self.Lx       = Lx              # Size along x\n",
    "        self.Ly       = Ly              # Size along y\n",
    "        self.V        = V               # Van der Waals potential\n",
    "        self.Omega    = Omega           # Rabi frequency\n",
    "        self.delta    = delta           # Detuning\n",
    "        self.L        = Lx * Ly         # Number of spins\n",
    "        self.device   = device\n",
    "        self.R=R\n",
    "        self.buildlattice()\n",
    "        \n",
    "    def buildlattice(self):\n",
    "        Lx,Ly=self.Lx,self.Ly\n",
    "        \n",
    "        #diagonal hamiltonian portion can be written as a matrix multiplication then a dot product\n",
    "        self.Vij=nn.Linear(self.L,self.L).to(device)\n",
    "        \n",
    "        mat=np.zeros([self.L,self.L])\n",
    "        \n",
    "        Vij[(1,1),(Lx,Ly)](Lx,Ly,self.R,self.V,mat)\n",
    "        with torch.no_grad():\n",
    "            self.Vij.weight[:,:]=torch.Tensor(mat)\n",
    "            self.Vij.bias.fill_(-self.delta)\n",
    "\n",
    "\n",
    "    def localenergy(self,samples,logp,logppj):\n",
    "        \"\"\"\n",
    "        Takes in s, ln[p(s)] and ln[p(s')] (for all s'), then computes Hloc(s) for N samples s.\n",
    "        \n",
    "        Inputs:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logp - size B vector of logscale probabilities ln[p(s)]\n",
    "            logppj - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i][j] had one spin flipped at position j\n",
    "                    relative to s[i]\n",
    "        Returns:\n",
    "            size B vector of energies Hloc(s)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Going to calculate Eloc for each sample in a separate spot\n",
    "        # so eloc will have shape [B]\n",
    "        # recall samples has shape [B,L,1]\n",
    "        B=samples.shape[0]\n",
    "        eloc = torch.zeros(B).to(self.device)\n",
    "        # Chemical potential\n",
    "        with torch.no_grad():\n",
    "            tmp=self.Vij(samples.squeeze(2))\n",
    "            eloc += torch.sum(tmp*samples.squeeze(2),axis=1)\n",
    "        # Off-diagonal part\n",
    "        #flip ONE spin here and get sqrt(p(s)/p(s'))\n",
    "        #then sum over all spin flips\n",
    "        #think of ways to cheat\n",
    "        \n",
    "        #logppj is shape [B*L]\n",
    "        # the first N labels in logppj are the log probabilities for\n",
    "        # the N states but with the first state flipped (ground-> excited and excited-> ground)\n",
    "        # the second N all were calculated with only the second state flipped \n",
    "        # etc...\n",
    "        for j in range(self.L):\n",
    "            #logpflip is log(p(1-s))\n",
    "            #logp is log(p(s))?\n",
    "            #s' has one spin flipped at j\n",
    "            #with psi(s)=sqrt(p(s)), sigma_i^x = psi(s')/psi(s)?\n",
    "            \n",
    "            #make sure torch.exp is a thing\n",
    "            eloc += -0.5*self.Omega * torch.exp((logppj[:,j]-logp)/2)\n",
    "\n",
    "        return eloc\n",
    "    def localenergyALT(self,samples,logp,sumsqrtp,logsqrtp):\n",
    "        \"\"\"\n",
    "        Takes in s, ln[p(s)] and exp(-logsqrtp)*sum(sqrt[p(s')]), then computes Hloc(s) for N samples s.\n",
    "        \n",
    "        Inputs:\n",
    "            samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logp     - size B vector of logscale probabilities ln[p(s)]\n",
    "            logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
    "                       when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
    "            sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
    "        Returns:\n",
    "            size B vector of energies Hloc(s)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Going to calculate Eloc for each sample in a separate spot\n",
    "        # so eloc will have shape [B]\n",
    "        # recall samples has shape [B,L,1]\n",
    "        B=samples.shape[0]\n",
    "        eloc = torch.zeros(B).to(self.device)\n",
    "        # Chemical potential\n",
    "        with torch.no_grad():\n",
    "            tmp=self.Vij(samples.squeeze(2))\n",
    "            eloc += torch.sum(tmp*samples.squeeze(2),axis=1)\n",
    "        # Off-diagonal part\n",
    "        #flip ONE spin here and get sqrt(p(s)/p(s'))\n",
    "        #then sum over all spin flips\n",
    "        #think of ways to cheat\n",
    "        \n",
    "        eloc += -0.5*self.Omega *sumsqrtp* torch.exp(logsqrtp-logp/2)\n",
    "\n",
    "        return eloc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00252ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Hamiltonian in module __main__:\n",
      "\n",
      "class Hamiltonian(builtins.object)\n",
      " |  Hamiltonian(Lx, Ly, V, Omega, delta, R=2.01, device=device(type='cuda', index=0))\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, Lx, Ly, V, Omega, delta, R=2.01, device=device(type='cuda', index=0))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  buildlattice(self)\n",
      " |  \n",
      " |  localenergy(self, samples, logp, logppj)\n",
      " |      Takes in s, ln[p(s)] and ln[p(s')] (for all s'), then computes Hloc(s) for N samples s.\n",
      " |      \n",
      " |      Inputs:\n",
      " |          samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |          logp - size B vector of logscale probabilities ln[p(s)]\n",
      " |          logppj - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i][j] had one spin flipped at position j\n",
      " |                  relative to s[i]\n",
      " |      Returns:\n",
      " |          size B vector of energies Hloc(s)\n",
      " |  \n",
      " |  localenergyALT(self, samples, logp, sumsqrtp, logsqrtp)\n",
      " |      Takes in s, ln[p(s)] and exp(-logsqrtp)*sum(sqrt[p(s')]), then computes Hloc(s) for N samples s.\n",
      " |      \n",
      " |      Inputs:\n",
      " |          samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |          logp     - size B vector of logscale probabilities ln[p(s)]\n",
      " |          logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
      " |                     when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
      " |          sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
      " |      Returns:\n",
      " |          size B vector of energies Hloc(s)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Hamiltonian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b5e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "torch.Size([10, 16, 1])\n",
      "torch.Size([10, 16, 1])\n",
      "torch.Size([10, 16, 1]) torch.Size([10, 16])\n",
      "tensor(-11.1137, device='cuda:0', grad_fn=<SelectBackward0>) \n",
      " tensor([[-11.1170, -11.1226, -11.0983, -11.1238, -11.0995, -11.0992, -11.1254,\n",
      "         -11.1251, -11.1003, -11.1011, -11.1214, -11.1025, -11.1040, -11.1038,\n",
      "         -11.1017, -11.1234]], device='cuda:0')\n",
      "RNN(\n",
      "  (rnn): LSTM(1, 128, batch_first=True)\n",
      "  (lin): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "testrnn = RNN(rnntype=\"LSTM\")\n",
    "print(testrnn.rnntype)\n",
    "x=torch.zeros([10,4*4,1]).to(device)\n",
    "print(testrnn(x).shape)\n",
    "sample = testrnn.sample(10,4*4)\n",
    "print(sample.shape)\n",
    "\n",
    "sample,pflip = testrnn.sample_with_labels(10,4*4,grad=False)\n",
    "logp=testrnn.logprobability(sample)\n",
    "print(sample.shape,pflip.shape)\n",
    "print(logp[0],'\\n',pflip[::10])\n",
    "\n",
    "print(testrnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5e48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_rnn_with_optim(rnntype,Nh,lr=1e-3,beta1=0.9,beta2=0.999):\n",
    "    rnn = RNN(rnntype=rnntype)\n",
    "    optimizer = torch.optim.Adam(\n",
    "    rnn.parameters(), \n",
    "    lr=lr, \n",
    "    betas=(beta1,beta2)\n",
    "    )\n",
    "    return rnn,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525888b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Hamiltonian(Lx,Ly,V,Omega,delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3205a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize=512\n",
    "\n",
    "testrnn,optimizer=new_rnn_with_optim(\"GRU\",64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855d365",
   "metadata": {},
   "source": [
    "# Training with Memory Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c37ca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "USEQUEUE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada68df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "74.01512026786804\n"
     ]
    }
   ],
   "source": [
    "BlockNum=(Lx*Ly)//2\n",
    "BbyL=bsize//BlockNum\n",
    "print(BbyL*BlockNum)\n",
    "samplequeue = torch.zeros([bsize,Lx*Ly,1]).to(device)\n",
    "sump_queue=torch.zeros([bsize]).to(device)\n",
    "sqrtp_queue=torch.zeros([bsize]).to(device)\n",
    "\n",
    "for i in range(BlockNum):\n",
    "    sample,sump,sqrtp = testrnn.sample_with_labelsALT(BbyL,Lx*Ly,grad=False)\n",
    "    with torch.no_grad():\n",
    "        samplequeue[i*BbyL:(i+1)*BbyL]=sample\n",
    "        sump_queue[i*BbyL:(i+1)*BbyL]=sump\n",
    "        sqrtp_queue[i*BbyL:(i+1)*BbyL]=sqrtp\n",
    "\n",
    "if not USEQUEUE:\n",
    "    nqueue_updates = BlockNum\n",
    "else:\n",
    "    nqueue_updates=1\n",
    "\n",
    "\n",
    "i=0\n",
    "t=time.time()\n",
    "losses=[]\n",
    "for x in range(2000):\n",
    "    \n",
    "    for k in range(nqueue_updates):\n",
    "        sample,sump,sqrtp = testrnn.sample_with_labelsALT(BbyL,Lx*Ly,grad=False)\n",
    "        with torch.no_grad():\n",
    "            samplequeue[i*BbyL:(i+1)*BbyL]=sample\n",
    "            sump_queue[i*BbyL:(i+1)*BbyL]=sump\n",
    "            sqrtp_queue[i*BbyL:(i+1)*BbyL]=sqrtp\n",
    "        i=(i+1)%BlockNum\n",
    "    \n",
    "    logp=testrnn.logprobability(samplequeue)\n",
    "    \n",
    "    \n",
    "    #sample = testrnn.sample(bsize,Lx*Ly)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        E=h.localenergyALT(samplequeue,logp,sump_queue,sqrtp_queue)\n",
    "        #E = h.LE2(sample,testrnn)\n",
    "        Eo=E.mean()\n",
    "            \n",
    "    loss = (E*logp - Eo*logp).mean()\n",
    "    \n",
    "    ERR  = Eo/(Lx*Ly)\n",
    "    testrnn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(ERR.cpu().item())\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0707871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoQAAAEzCAYAAACokBIfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABJ0AAASdAHeZh94AAAxCElEQVR4nO3de1xVdb7/8fcGBEQEFUVFJTMrFRTExqxx0o5ZpI1g5aQzHi9NeU6Zk51fOVaWWnafSuOR03Qyu3jBsSZRM8vJSw+7aqgpOY2UF9QSBUEUQcDv7w8PWzdsYIMb9mW9no8HD93fvfbany9rX95811rfZTPGGAEAAMCyAjxdAAAAADyLQAgAAGBxBEIAAACLIxACAABYHIEQAADA4giEAAAAFkcgBAAAsDgCIQAAgMURCAEAACwuyNMF+IKCggJt2rRJXbp0UUhIiKfLAQAAqFFpaalycnI0aNAgtWrVyqXHEAhdsGnTJqWmpnq6DAAAAJetWLFCKSkpLi1LIHRBly5dJJ37xXbv3t3D1QAAANQsOztbqamp9vziCgKhCyp3E3fv3l1xcXEergYAAKBu9TnMjZNKAAAALI5ACAAAYHEEQgAAAIsjEAIAAFgcgRAAAMDiOMsYAADUizFGRUVFOnHihMrKymSM8XRJlhAQEKCQkBC1b99eAQHuHdMjEHqJA3nFmpKeqe8Pn1CvmAiljU5SbFSYp8sCAMBBeXm5Dh06pOLiYklSUFCQAgICZLPZPFyZfzPG6MyZMzp9+rRKS0sVGxvr1lBIIPQSU9IztSOnUJK0I6dQU9IzlTF5oIerAgDA0fHjx1VcXKzIyEhFR0crKIgo0VSMMcrNzVV+fr6OHDmijh07um3dHEPoJb4/fKLW2wAAeIOTJ08qMDBQHTt2JAw2MZvNpujoaAUGBqq0tNSt6yYQeoleMRG13gYAwBsYYxQUFMQuYg+x2WwKDAzU2bNn3bpeAqGXSBudpIQukWoWaFNCl0iljU7ydEkAAMALNUYYZ6zXS8RGhXHMIAAA8AhGCAEAACyOQOhFDuQVK+XVzbr80TVKeXWzDuQVe7okAAAs5a233pLNZrP/BAUFqWPHjho9erT27NnjsOzgwYNls9mUnJxcbT379u2TzWbTX/7yF3vbxo0b7ev98ssvqz1mwoQJCg8Pd3+nXEAg9CKVU8+UVRj71DMAAKDpLVy4UF9++aX++c9/6r777tPKlSs1cOBAHT9+vNqyH3/8sdavX1+v9U+bNs1dpboFgdCLMPUMAADeIT4+XgMGDNDgwYP16KOPavr06crNzdWKFSsclrviiivUrVs3TZs2zeUrtiQnJ2vz5s1atWpVI1TeMARCL8LUMwAAK/PmQ6euuuoqSdKRI0cc2ps1a6annnpK3377rZYtW+bSuiZMmKBevXrp4YcfVkVFhdtrbQgCoRdh6hkAgJV586FTe/fulXRuRLCqO+64Q/369dOMGTNUVlZW57oCAwP1zDPPKCsrS2+//bbba20IAqEXqZx6Zs9Tw5QxeSDXMgYAWIo3HTpVUVGh8vJynTx5Uh9//LHmzJmj6667TiNGjKi2rM1m03PPPacff/xRf/vb31xa/4gRIzRw4EDNnDlTJSUl7i6/3giEAADAK3jToVMDBgxQs2bN1LJlSyUnJ6t169bKyMio8XJ9Q4YM0Y033qgnnnhCRUVFLj3Hc889p4MHD2revHnuLL1BCIQAAMAreNOhU++88462bNmi9evX67/+67+0e/dujRkzptbHPPfcczp27JjDVDO1ufbaa5Wamqpnn33W6dnLTYkrlQAAAK/gTVft6tmzp/1Ekuuvv14VFRV644039N577+n22293+pjExESNGTNGL730koYNG+bS8zzzzDOKj4/X008/7bbaG4IRQgAAgDo8//zzat26tR5//HGdPXu2xuXmzJmjM2fOaPbs2S6tt0ePHrrzzjuVlpamAwcOuKvceiMQAgAA1KF169Z6+OGHtXv3bi1ZsqTG5S699FLdc889+uijj1xe96xZsxQYGKgNGza4o9QGIRACAAC4YMqUKYqNjdUTTzxR6/yBM2bMUESE6yfExMTEaOrUqW6osOFsxtVptS0sKytL8fHx2rVrl+Li4jxdDgAAHvPTTz9Jkrp16+bhSqyrrm3QkNzCCCEAAIDFEQgBAAAsjkAIAABgcQRCAAAAiyMQAgAA+JDGOB+YQAgAAFxms9lUXl7eKKEEdTPGqKKiQgEB7o1wBEIAAOCy8PBwVVRU6Oeff1Z5ebmny7EUY4xyc3NVUVGhkJAQt66baxkDAACXtW7dWsXFxSosLFRhYaGCgoIUEBAgm83m6dL8WuXIYEVFhZo3b6727du7df0EQgAA4LKgoCDFxsaqqKhIJ06cUFlZGbuPm4DNZlNwcLBCQkLUvn17t+8yJhACAIB6sdlsioiIqNfl2eDdOIYQAADA4giEAAAAFkcgBAAAsDgCIQAAgMURCAEAACyOQAgAAGBxBEIAAACL88tAuH79et15553q0aOHWrRooU6dOiklJUXffvutp0sDAADwOn4ZCP/6179q3759uv/++7VmzRrNmzdPubm5GjBggNavX+/p8gAAALyKX16p5NVXX1V0dLRDW3Jysrp3766nn35a//Ef/+GhygAAALyPX44QVg2DkhQeHq5evXopJyfHAxUBAAB4L78cIXSmsLBQmZmZdY4O5ubm6ujRow5t2dnZjVkaAACAR1kmEE6ePFmnTp3So48+Wuty8+fP1+zZs5uoKgAAAM+zRCB87LHHtHjxYqWlpalfv361Lnvvvfdq1KhRDm3Z2dlKTU1txAoBAAA8x+8D4ezZszVnzhw99dRTuu++++pcPjo62ukxiAAAAP7KL08qqTR79mzNmjVLs2bN0iOPPOLpcgAAALyS3wbCJ598UrNmzdKMGTM0c+ZMT5cDAADgtfxyl/GLL76oxx9/XMnJyRo+fLi++uorh/sHDBjgocoAAAC8j18GwlWrVkmS1q5dq7Vr11a73xjT1CUBAAB4Lb8MhBs3bvR0CQAAAD7Db48hBAAAgGsIhAAAABZHIAQAALA4vzyG0NcdyCvWlPRMfX/4hHrFRChtdJJio8I8XRYAAPBTjBB6oSnpmdqRU6iyCqMdOYUa9MIGpby6WQfyij1dGgAA8EMEQi/0/eETDreNpB05hZqSnumZggAAgF8jEHqhXjERTturBkUAAAB3IBB6obTRSUroElmtvaagCAAAcDEIhF4oNipMGZMH6rOHrldCl0g1C7QpoUuk0kYnebo0AADghzjL2ItVBkMAAIDGxAghAACAxREIAQAALI5ACAAAYHEEQgAAAIsjEAIAAFgcgRAAAMDiCIQAAAAWRyAEAACwOAIhAACAxREIAQAALI5ACAAAYHEEQgAAAIsjEAIAAFgcgRAAAMDigjxdABwdyCvWlPRMfX/4hHrFRChtdJJio8I8XRYAAPBjjBB6mSnpmdqRU6iyCqMdOYWakp7p6ZIAAICfIxB6me8Pn6j1NgAAgLsRCL1Mr5iIWm8DAAC4G4HQy6SNTlJCl0g1C7QpoUuk0kYnebokAADg5zipxMvERoUpY/JAT5cBAAAshBFCAAAAi2OE0AcwFQ0AAGhMjTpCWFpaqtWrV+uuu+5Shw4dlJiYqJkzZ2rbtm2N+bR+5UBesZLnfcZUNAAAoNG4fYQwPz9fq1atUkZGhj755BOdPn1akmSM0dGjR7Vz507NmTNHnTt31ogRI5SSkqLBgwcrKIjBSmempGeq+EyFQxtT0QAAAHdySwrbu3evVqxYoYyMDH3xxReqqDgXYIwx9mWuvPJKHT16VPn5+ZKknJwczZ8/X/Pnz1dERISGDRumlJQU3XzzzWrZsqU7yvILzsIfU9EAAAB3anAg/Pbbb+0hMCsry95eGQIDAgJ07bXXKiUlRampqerevbvOnj2rzz77TBkZGVq5cqX27t0rSSosLFR6errS09PVrFkzDR48WKmpqRoxYoRiYmIusou+rVdMhHbkFNpvhwUHMhUNAABwK5u5cBjPBZMnT9bKlSt1+PBhe1vlKpo3b64bbrhBKSkp+u1vf6t27drVuq6dO3cqIyNDK1asUGbm+ePibDab/f/9+vXT/fffrz/84Q/1KdOtsrKyFB8fr127dikuLq5Jn5sTSgAAQH00JLfUOxAGBATIZrPZQ2BUVJRuueUWpaSk6KabblLz5s3rX7mkQ4cOKSMjQxkZGdq4caPKysrOFWiz6bbbbtPf//73Bq3XHTwZCC9EOAQAAHVpSG5p0C7jSy+9VCkpKUpJSdHAgQMVEHDxJyt36tRJ9957r+69916dOHFCa9asUUZGhj766KOLXre/mJKead99XHm2MZNYAwCAi1XvQPjdd98pPj6+MWqxi4iI0OjRozV69GiVl5fbjzW0uqonmHC2MQAAcId6D+01dhisKigoSJdffnmTPqe3qnp2MWcbAwAAd+DSdT4kbXSSErpEqlmgTQldIjnbGAAAuEW9dxkfP35crVu3boxaUIfYqDCOGQQAAG5X70DYtm1bde7cWX379lViYqL930suuaQx6gMAAEAjq3cgNMbo4MGDOnjwoFatWmVvb9WqlRISEhyCYs+ePRUYGOjWggEAAOBeDZp2xtnUhcePH9emTZu0adMme1twcLDi4+MdRhITEhLUokWLhlcMAAAAt6p3IPziiy+0fft2bdu2Tdu2bdOuXbtUUlIiqXpQLC0t1bffflvtKiSXXXZZtV3O7du3v8iuAAAAoCHqHQgHDBigAQMG2G+fPXtWu3fv1rZt2+xBcfv27Tp+/Lj9iiYXBkVjjLKzs5Wdna3ly5fb29u3b28PiE899dRFdgsAAACuqvel61yVkJCgnTt3ymaz6eabb9b27dsdrn9crZD/C482m00VFRWNUVKDecul6wAAAOrSZJeuc8WFl7NbvXq1JOnYsWP2EcTK0cQ9e/aooqLC6XGJAAAAaHyNFgidadu2rYYOHaqhQ4fa20pKSvTdd985BEUAAAA0nSYNhM6Ehoaqf//+6t+/v6dLAQAAsCQuXQcAAGBxBEIAAACLIxACAABYnN8GwqKiIk2bNk033nij2rVrJ5vNplmzZnm6LAAAAK/jt4EwLy9Pr7/+ukpLS5WamurpcgAAALyWx88ybiyXXHKJ/Wopx44d0xtvvOHpkgAAALxSvQNhXFyc+vbt63At4jZt2jRGbRfFZrN5ugQAAACfUO9AuHv3bv3rX//S0qVL7W2dO3e2h8PKoAgAAADf0KBdxlUvM5eTk6ODBw/aL1FX1aJFi5SQkKBevXopMDCwIU/ZZHJzc3X06FGHtuzsbA9VAwAA0PjqHQiffvpp+yXmsrOzdfbsWUnVQ6LNZrPvth0/frwkKTg4WHFxcfbRxMTERCUkJCg8PPxi++E28+fP1+zZsz1dRq0O5BVrSnqmvj98Qr1iIpQ2OkmxUWGeLgsAAPioegfC6dOn2/9fXFysHTt22K9DvG3bNmVlZamkpKRaQJSk0tJSbdu2Tdu2bdPChQslnQuO3bp1U2Jiov1n2LBhF9Gli3Pvvfdq1KhRDm3Z2dledabylPRM7cgplCTtyCnUlPRMZUwe6OGqAACAr7qos4zDwsJ0zTXX6JprrrG3VVRUaPfu3faAWDmaePz4cUnVRxKNMfrxxx/1448/6v3335fNZlN5efnFlHVRoqOjFR0d7bHnd8X3h0/UehsAAKA+3D7tTGBgoOLj4xUfH6+xY8fa2w8cOOAwkrh9+3YdOHBAUvWQiNr1iomwjxBW3gYAAGioJpuHMDY2VrGxsUpJSbG3FRQU2HchVwbFH374wW3P+dFHH+nUqVMqKiqSJH3//fd67733JEnDhg1TWJhvHneXNjqp2jGEAAAADWUzXjY8V1paqpCQELesq2vXrtq/f7/T+/bu3auuXbu6tJ6srCzFx8dr165diouLc0ttAAAAjaEhucXrrlTirjAoSfv27XPbugAAAPyV317LGAAAAK6pdyDcuXNnY9RRo7KyMv373/9u0ucEAACwknoHwoSEBHXr1k0PPPCANm7caJ+Y2p0KCwu1ZMkS3XHHHWrbtq1mzJjh9ucAAADAOQ06hnD//v165ZVX9Morr6h169YaPny4UlJSlJyc3OAzd3NycpSRkaGMjAx99tln9rkIveycFwAAAL9T70A4adIkrV69WocPH5Yk5efna9GiRVq0aJFCQkI0ZMgQpaSkaMSIEXVO8Lxjxw5lZGRoxYoV2rFjh739whCYlJTkMFUNAAAA3KvB085s2bLFPqKXlZV1foX/d/1im82mq6++WikpKUpNTdUVV1yhiooKbdq0SRkZGVq5cqV9YmrpfAgMDg7W4MGD7aGyU6dOF9M/t2DaGQAA4CsaklvcMg/hTz/9ZA+Hn3/+uSoqKs6t/P/CoSRddtllysvLU0FBgSTHUcDIyEgNGzZMKSkpuvnmm9WyZcuLLcmtCIQAAMBXeGwewsqTTB544AHl5+dr9erVysjI0CeffKJTp05JkrKzsx0eExsbqxEjRiglJUWDBg1SUJDXTYkIAABgCW5PYW3atNG4ceM0btw4lZaW6p///KcyMjK0evVqdejQQSkpKUpJSVFiYqK7nxoAAAAN0KjDciEhIRo+fLiGDx/emE8DAACAi8CVSgAAACyOQAgAAGBxBEIAAACLIxACAABYHIEQAADA4giEAAAAFkcgBAAAsDguD+LjDuQVa0p6pr4/fEK9YiKUNjpJsVFhni4LAAD4EEYIfdyU9EztyClUWYXRjpxCTUnP9HRJAADAxxAIfdz3h0/UehsAAKAuBEIfdSCvWCmvblZZhXFo7xUT4aGKAACAr+IYQh9Vuau4kk1Sny6RShud5LmiAACATyIQ+qiqu4aDAm3KmDzQQ9UAAABfxi5jH1V11zC7igEAQEMRCH1U2ugkJXSJVLNAmxLYVQwAAC4Cu4x9TNV5Bz/9n8HMOwgAAC4KI4Q+hnkHAQCAuxEIfQzzDgIAAHcjEPoYTiYBAADuRiD0MZxMAgAA3I2TSnxMbFQY8w0CAAC3YoQQAADA4hgh9AMH8oo1adFW/evnItkk9ejYUn8bexXT0QAAAJcwQugHpqRn6l8/F0mSjKTdPxcxHQ0AAHAZgdAPOJt6huloAACAqwiEfsDZ1DNMRwMAAFxFIPQDaaOT1KNjS0mSTVLPji2ZjgYAALiMk0r8QGxUmNbef52nywAAAD6KEUIAAACLIxACAABYHIEQAADA4giEAAAAFkcgBAAAsDgCIQAAgMURCAEAACyOQAgAAGBxBEIAAACL40olfuRAXrGmpGfq+8Mn1CsmQmmjkxQbFebpsgAAgJdjhNCPTEnP1I6cQpVVGO3IKdSgFzYo5dXNOpBX7OnSAACAFyMQ+pHvD59wuG0k7cgp1JT0TM8UBAAAfAKB0I/0iolw2l41KAIAAFyIQOhH0kYnKaFLZLX2moIiAACARCD0K7FRYcqYPFCfPXS9enRsKUmySTpTfpbjCAEAQI0IhH4oNipMIUHnNq2RtPvnIo4jBAAANSIQ+qmqxw1yHCEAAKgJgdBPVT1ukOMIAQBATfw2EJ48eVJTp05VTEyMQkNDlZiYqPT0dE+X1WQqTzBpFmhTQpdIpY1O8nRJAADAS/ntlUpuvfVWbdmyRc8++6yuuOIKLVmyRGPGjNHZs2f1+9//3tPlNbrKE0wAAADq4peBcM2aNVq3bp09BErS9ddfr/379+uhhx7SHXfcocDAQA9XCQAA4B38cpfxBx98oPDwcI0aNcqhfeLEiTp8+LC+/vprD1XWtA7kFSvl1c26/NE1XMIOAADUyC9HCHft2qWePXsqKMixe3369LHff+211zp9bG5uro4ePerQlp2d3TiFNrLKaxtL5y9hx25kAABQlV8Gwry8PHXr1q1ae5s2bez312T+/PmaPXt2o9XWlKpONbMjp1Apr25W2ugkxUaFeagqAADgbfxyl7Ek2Wy2Bt137733ateuXQ4/K1asaIQKG5+zqWZ25BQqed5n7D4GAAB2fjlCGBUV5XQUMD8/X9L5kUJnoqOjFR0d3Wi1NaW00UkOu40rFZ+pYPcxAACw88sRwt69e2v37t0qLy93aN+5c6ckKT4+3hNlNbnKqWcSukRWu48rlwAAgEp+GQhHjhypkydP6v3333dof/vttxUTE6Orr77aQ5V5RtroJIUFO06zw5VLAABAJb/cZXzzzTdr6NChuueee3TixAl1795dS5cu1dq1a7Vo0SLLzUEYGxWmtfdfp0mLtupfPxfJJulM+VkdyCvm5BIAAOCfI4SS9I9//EP/+Z//qccff1zJycn6+uuvtXTpUv3hD3/wdGkeERsVppCgc5vbSNr9c5GmpGd6tigAAOAV/HKEUJLCw8M1b948zZs3z9OleA1n09AwSggAAPx2hBDVOTtucNALG7iKCQAAFkcgtJC00UmqOgOj0fmrmAAAAGsiEFpIbFSY+jiZgkY6fxUTRgoBALAeAqHFpI1OcjovocRIIQAAVkUgtJjKyao/e+j6anMTSudPNAEAANZBILSo2KgwlVWcdXofo4QAAFgLgdDCarpayY6cQl3+6BqOKQQAwCL8dh5C1C1tdJKmpGdqR05htfvKKox25BTquhc2KLRZgMorjOI6RShtdBLzFgIA4GcIhBZWeTzhgbziGoOhJJWUndu1TEAEAMA/EQhhD4aXP7pGZRWmzuWrBkRJCg6y6Uy542N7dmypv429isAIAICXIxDCrldMRI2jhHWpGgalc9dLrgyMlaoGx8rbNkk9CJAAAHgEgRB2lccUZh0qVFBggH0k0KZzVzRxh6rBsfK20fkA6Wy0saYgWdsyrraFNgtQWflZhTQL1Jnys7XuCq/cvX7h74gwCwDwdTZjjLu+6/1WVlaW4uPjtWvXLsXFxXm6nCZ3IK9YkxZt1b9+LpLk3oDoS5yFS1eWcUfArSm0SnLYNo1ZQ1Osq6GP4/AEADivIbmFQOgCqwfCqqoGRMm1sAT3s2o4d0XV1+Rl7VpIkn48espp24UjvdL5oM0IMABfQyBsJATC+nMWGiXXRoQAX9AUI6WVYXTmLXF6Zu1ufX/4hHrFnB8dnpKe6dBGYAUgEQgbDYGwcVUNj029e5NRNviThrxXLmvXQsFBAcrOPUm4BPxAQ3ILJ5XA42KjwrT2/us89vwXnigSHBSo0rIKNQs6f1JNJWdnREuqd5h11ubq41xhxWMIcV5NJ27V1nbhbvTappNydVv0vGBUkxOwAN/ACKELGCGEp1SG1crdgg8n99Ss1Vn2AGr1ScKdnfVdidDo3Xzpj42mrsHZSVIN2ZPiLf1p6LqcHffbs8ohFJdFh8smac+Rolr/oHe2LmczTtT1eerqMfS1tTXFH0bsMm4kBELAd1UN1c6Ov3s4uWeNo1kzb4lzCOGVmvqLFtbGoS2e0ZjvxYQukcqYPLBR1s0uYz/w1ltv6a233qp1mcTERM2dO9d+e/v27Zo6dWqd6964caPD7cGDB9f5mLlz5yoxMdF+e+rUqdq+fXutj5kwYYImTJhgv02fzqFP53miT50kFUoat/z88p0kXZqYqAGT59o/mC/s02lJ0z84t2zXKs/lrj4lJiY61FzpwpEIm6RudZwlXakxD0eA57C1PKMx3yffHz7RaOtuCAKhl9m3b582bdpUr8cUFBTU+zGSXHpMQUGBw+3t27fX+biqX4z06fzz0qdz6NM527dvV6tWrZwu2xTH1l44enpZdLjKys8SLoEm0ismwtMlOCAQepmuXbtq0KBBtS5z4QiDJLVq1arOxzjjymOqfllVfW5nunbtWu02faJPF6JP55+7as1NqfI65u5UdWTTUydg+fIxhK7wpf5462ES7ppxoqHHEFYevuItOIbQBRxDCABoCjXN4Wq1E8hqOva36mEUwUEB9hNKarr0qLN1XcwJI75wZSROKmkkBEIAAOArGpJbAhq5JgAAAHg5AiEAAIDFEQgBAAAsjkAIAABgcQRCAAAAiyMQAgAAWBwTU7ugtLRUkpSdne3hSgAAAGpXmVcq84srCIQuyMnJkSSlpqZ6thAAAAAX5eTkKCnJtSuiMDG1CyqvbdqlSxeFhIQ02vNkZ2crNTVVK1asUPfu3RvtebyRlfsuWbv/9N2afZes3X8r912ydv+bou+lpaXKycnRoEGDarxeelWMELqgVatWSklJabLn6969u2WviGLlvkvW7j99t2bfJWv338p9l6zd/8buu6sjg5U4qQQAAMDiCIQAAAAWRyAEAACwOAKhF2nXrp1mzpypdu3aebqUJmflvkvW7j99t2bfJWv338p9l6zdf2/tO2cZAwAAWBwjhAAAABZHIAQAALA4AiEAAIDFEQgBAAAsjkDoYSdPntTUqVMVExOj0NBQJSYmKj093dNlNdj69et15513qkePHmrRooU6deqklJQUffvttw7LTZgwQTabrdpPjx49nK43LS1NPXr0UEhIiC699FLNnj1bZWVlTdGletm4caPTftlsNn311VcOy2ZmZuqGG25QeHi4WrVqpVtvvVU//fST0/X6Qv9r2qZV++8P276oqEjTpk3TjTfeqHbt2slms2nWrFlOl22M7Zybm6sJEyaobdu2CgsL0zXXXKNPP/3UnV2skSt9r6io0EsvvaTk5GR17txZYWFh6tmzp6ZPn66CgoJq66zpNfPss89WW9aTfZdc3/aN9Tr39m0v1bw9nfXfV7a9q99tkg+/5w08aujQoaZVq1bmtddeM+vXrzd33XWXkWQWL17s6dIa5PbbbzfXX3+9mT9/vtm4caNZvny5GTBggAkKCjKffvqpfbnx48eb5s2bmy+//NLhZ/v27dXWOWfOHGOz2czDDz9sNmzYYJ5//nkTHBxs7r777qbsmks2bNhgJJmnn366Wt+Kiorsy+3evdu0bNnS/OY3vzEffvihef/9901cXJyJiYkxubm5Duv0lf5nZ2dX6/OXX35p2rZtazp16mTKy8uNMf6x7ffu3WsiIyPNddddZ3/Pzpw5s9pyjbGdS0pKTHx8vOncubNZtGiR+eSTT0xKSooJCgoyGzdubMxuG2Nc63tRUZFp2bKlmTRpklm+fLnZsGGDefHFF03r1q1Nr169THFxscPyksztt99e7TVx6NAhh+U83XdjXN/2jfE693T/Xe27s8+BuXPnGklm+vTpDsv6yrZ39bvNl9/zBEIP+vDDD40ks2TJEof2oUOHmpiYGPsXqC85cuRItbaioiLTvn17M2TIEHvb+PHjTYsWLepc37Fjx0xoaKiZNGmSQ/tTTz1lbDabycrKuvii3agyEC5fvrzW5UaNGmXatm1rCgsL7W379u0zzZo1M9OmTbO3+Vr/q9q4caORZGbMmGFv84dtf/bsWXP27FljjDFHjx6t8YuxMbbzq6++aiSZL774wt5WVlZmevXqZfr37++uLtbIlb6Xl5ebY8eOVXvs8uXLjSTz7rvvOrRLMpMnT67zuT3dd2Nc3/aN8Tr3dP9d7bszEyZMMDabzezZs8eh3Ve2vavfbb78nmeXsQd98MEHCg8P16hRoxzaJ06cqMOHD+vrr7/2UGUNFx0dXa0tPDxcvXr1Uk5OTr3Xt3btWpWUlGjixIkO7RMnTpQxRitWrGhoqR5TXl6u1atX67bbblNERIS9/ZJLLtH111+vDz74wN7m6/1fsGCBbDab7rzzzno/1pv7XrlbqzaNtZ0/+OADXXnllbrmmmvsbUFBQRo7dqy++eYbHTp06CJ7VztX+h4YGKioqKhq7f3795ekBn0WSJ7vu+Ra/+vD37a9M0VFRVq+fLkGDRqk7t27N+i5Pd13V77bfP09TyD0oF27dqlnz54KCgpyaO/Tp4/9fn9QWFiozMxMxcXFObSfPn1aHTp0UGBgoDp37qz77rtP+fn5DstU/g569+7t0N6xY0e1bdvWa39HkydPVlBQkCIiInTTTTdp8+bN9vt+/PFHnT592r6dL9SnTx9lZ2erpKREku/2Xzq33d977z0NGTJEl156qcN9/rztKzXWdt61a1eN65SkrKwst/XB3davXy9J1T4LJGnJkiVq3ry5QkJC1K9fPy1cuLDaMr7Wd3e/zn2t/5XS09N16tQp3XXXXU7v99VtX/W7zdff80F1L4LGkpeXp27dulVrb9Omjf1+fzB58mSdOnVKjz76qL0tISFBCQkJio+PlyRt2rRJL7/8sj799FNt2bJF4eHhks79DkJCQtSiRYtq623Tpo3X/Y4iIyN1//33a/DgwYqKilJ2drZeeOEFDR48WB9++KFuuukme82V2/lCbdq0kTFGx48fV8eOHX2u/xdaunSpTp8+rT/+8Y8O7f667atqrO2cl5dX4zovfF5vc+jQIU2fPl1XXXWVbrnlFof7fv/732v48OHq0qWLcnNztWDBAt1555366aef9OSTT9qX86W+N8br3Jf6f6EFCxaoVatWuu2226rd58vbvup3m6+/5wmEHlbb8Ls7d0t4ymOPPabFixcrLS1N/fr1s7c/8MADDssNHTpUffv21e23367//d//dbjfl35Hffv2Vd++fe23f/Ob32jkyJHq3bu3pk2bpptuusl+n6v98qX+X2jBggWKiorSyJEjHdr9ddvXpDG2s6/9XvLz8zVs2DAZY7Rs2TIFBDjunFq8eLHD7dtuu02//e1v9eyzz+pPf/qTwzVffaXvjfU695X+V8rKytLXX3+tyZMnKzQ0tNr9vrrta/puq6sWb37Ps8vYg6Kiopwm+8pdCs7+IvAls2fP1pw5c/TUU0/pvvvuq3P5kSNHqkWLFg7Ts0RFRamkpETFxcXVls/Pz/eJ31GrVq10yy236LvvvtPp06ftx1bVtO1tNptatWolyXf7/91332nr1q0aO3asQkJC6lzeH7d9Y21nX/vcOH78uIYOHapDhw5p3bp1TveKODN27FiVl5dr69at9jZf63tVF/s698X+L1iwQJJq3F3sjLdv+5q+23z9PU8g9KDevXtr9+7dKi8vd2jfuXOnJNl3Nfii2bNna9asWZo1a5YeeeQRlx9njHEYPag8vqLyd1Lpl19+0bFjx3zmd2SMkXTuL7nLLrtMzZs3r9Yn6Vw/u3fvbv9L2lf735AvAX/b9o21nXv37l3jOiXv+tw4fvy4brjhBu3du1fr1q1zehxUTSrfM1VfE77S95pczOvc1/p/5swZvfvuu+rXr58SExNdfpw3b/vavtt8/j3vtvOVUW9r1qwxkkx6erpDe3Jyss9OO2OMMU888US1qUZcsWzZMiPJzJ07196Wl5dnQkNDzX//9387LPvMM894fOoRV+Xn55tOnTqZxMREe9vvfvc7Ex0dbU6cOGFv279/vwkODjZ//vOf7W2+2P+SkhLTpk2bek2H4MvbvrbpNxpjO8+fP99IMl999ZW9rayszMTFxZmrr77ajT2rW219z8/PN0lJSaZVq1Zmy5Yt9V73sGHDTLNmzczRo0ftbd7Ud2PqP/XKxb7Ovan/rvS9cpqh+fPn12vd3rrtXflu8+X3PIHQw4YOHWpat25tXn/9dbN+/Xpz9913G0lm0aJFni6tQf7yl78YSSY5Odnp5KTGnJuT6dprrzWvvPKKWbNmjfnoo4/M9OnTTWhoqImLizMnT550WGfl5J2PPPKI2bhxo3nhhRdMSEiIxycndmbMmDHmz3/+s30y3tdff91ceeWVJigoyKxbt86+3O7du014eLi57rrrzJo1a8w//vEPEx8fX+vkpb7Qf2OMSU9PN5LM66+/Xu0+f9r2a9asMcuXLzdvvvmmkWRGjRplli9fbpYvX25OnTpljGmc7VxSUmLi4uJMly5dzOLFi826devMyJEjm3Ry5rr6XlxcbH71q18Zm81m5s2bV+1zIDs7276u559/3kyYMMG8++67ZsOGDWbZsmXmxhtvNJLMrFmzvK7vrvS/sV7n3tB/V173lZKTk03z5s1NQUGB03X50rZ35bvNGN9+zxMIPayoqMj86U9/Mh06dDDBwcGmT58+ZunSpZ4uq8EGDRpkJNX4Y8y5kYORI0earl27mubNm5vg4GBz+eWXm2nTptX4wTFv3jxzxRVXmODgYBMbG2tmzpxpzpw505Rdc8kzzzxjEhMTTWRkpAkMDDTt2rUzI0eONN988021Zbdu3WqGDBliwsLCTEREhElNTXX4oryQr/TfmHN/5LRo0cLhL+RK/rTtL7nkkhpf53v37rUv1xjb+ZdffjHjxo0zbdq0MaGhoWbAgAEOf3A0trr6vnfv3lo/B8aPH29f18qVK83AgQNNu3btTFBQkP0qDzV9Dnq678bU3f/GfJ17uv+uvu4PHDhgAgICzLhx42pcly9te1e+2yr56nveZsz/7awHAACAJXFSCQAAgMURCAEAACyOQAgAAGBxBEIAAACLIxACAABYHIEQAADA4giEAAAAFkcgBAAAsDgCIQAAgMURCAEAACwuyNMFAIC3Msbovffe05IlS5SZmanc3FwFBgaqffv26tixo/r376/f/OY3GjJkiCIiIuyPmzt3rgoKCpSamqrExETPdQAAXMS1jAHAicpAt2nTJntbUFCQIiIidOLECZWXl9vbFy5cqAkTJthvd+3aVfv376/WDgDeil3GAODEuHHjtGnTJgUGBur//b//p3//+98qLS1VXl6eTp8+rR07dui5555TQkKCp0sFgIvGLmMAqGLPnj1atWqVJGnOnDmaPn26w/1BQUHq06eP+vTpo2nTpun06dOeKBMA3IYRQgCoYvv27fb/p6Sk1Ll88+bNJUmzZs2SzWbT/v37JUkTJ06UzWZz+HFm48aNGjNmjGJjYxUaGqrIyEj1799fzz//vE6dOuX0MRMmTJDNZtOECRNkjNFrr72m/v37KzIyUhERERo4cKAWL15cz54DsCpGCAGgFgcPHlTPnj1dWjY8PFzt27fX0aNHdfbsWUVERNjDojPl5eW655579MYbbzis49SpU9qyZYu2bNmiN998Ux9//LEuueSSGtczZswYLVu2TAEBAYqMjFRBQYE+//xzff755/r000+1YMGCGsMoAEiMEAJANb/61a/sAary+EFXPPjgg/rll1/UpUsXSdK8efP0yy+/OPxUXf6NN95Q+/btNX/+fOXl5amoqEinT5/Whg0b1LdvX/3www+69dZbdfbsWafPuWLFCv3973/Xk08+qePHjys/P19HjhzRfffdJ+ncCS9paWkN/VUAsAgCIQBU0bVrV911112SpJ07d6pHjx5KSkrS5MmT9eabb2rXrl262Akadu3apVdeeUVhYWFat26d7rnnHrVp00aS1KxZMw0ePFibNm1S586dlZmZqZUrVzpdT2FhoWbMmKEZM2bYp75p166d0tLSNHbsWEnS7NmzVVJSclH1AvBvBEIAcGL+/Pl67LHH1KJFCxljtG3bNs2fP19//OMf1bt3b3Xo0EH/8z//oyNHjjRo/QsWLJAxRsOHD1fv3r2dLtOyZUulpqZKkj7++GOnyzRv3lwPPvig0/sef/xxSVJ+fr7WrVvXoDoBWAOBEACcCAoK0hNPPKFDhw7p3Xff1V133aWEhAQFBwdLknJzc/Xyyy8rPj5e33zzTb3Xv3nzZknSRx99pA4dOtT4s3DhQkmyn6hS1VVXXeUwKfaFLr/8cnXu3FmStHXr1nrXCMA6OKkEAGoRGRmpsWPH2ne/lpSUaPPmzXrllVe0atUqHTt2TLfddpv27Nmj0NBQl9d7+PBhSdLJkyd18uTJOpcvLi522t6pU6daH9epUycdPHhQubm5LtcGwHoYIQSAeggNDdUNN9yglStXavz48ZLOnYm8du3aeq2noqJCkvTss8/KGFPnz8aNG52uh7OHAbgDgRAAGmjSpEn2///www/1emyHDh0knTtp5WIcPHiw1vsPHTokSYqOjr6o5wHg3wiEANBA4eHh9v+HhITY/x8QcO6jtbYzkX/9619Lkj788EOXdhnXZOvWrSoqKnJ6X3Z2tj0wXnXVVQ1+DgD+j0AIAFXs3bvXpbkH3377bfv/k5KS7P+vPMmjoKCgxsfefffdstlsKigo0EMPPVTr85SVldUYGk+fPq0XX3zR6X1z5syRJLVp00ZDhw6t9TkAWBuBEACqyMrKUs+ePTV8+HC988472rdvn/2+srIybdu2TRMnTtRLL70kSerfv78GDhxoXyY+Pl6S9N577+n48eNOnyMxMVFTp06VJL322msaNWqUtm/fbh9VrKio0I4dO/Tkk0/qsssuc7ic3oUiIyP15JNP6plnnrGPFB47dkz333+/PbA+9thj9TrhBYD12MzFzq4KAH7m448/VnJyskNbcHCwwsPDdfz4cYddwUlJSVq1apViYmLsbZ999pkGDx4sY4wCAwMVHR1tn67mwnBZUVGhBx98UHPnzrW3hYaGqkWLFiosLFR5ebm9ffPmzfbdzNK5axm//fbbGj9+vEpKSrRs2TIFBgYqIiJCBQUF9hrHjRunhQsX2ndjA4AzfEIAQBU33XST9uzZo3nz5mnUqFHq2bOnQkJCVFBQoLCwMF1++eX63e9+p/T0dG3ZssUhDErSddddpw8//FA33HCDIiMjdeTIEe3fv7/aXIKBgYF6+eWXlZmZqUmTJunKK69UYGCgCgsL1bp1a/3617/WrFmztH37docwWNXSpUv117/+VX379lV5eblatGiha665Ru+8847efvttwiCAOjFCCAA+6MIRwrfeesvT5QDwcfzZCAAAYHEEQgAAAIsjEAIAAFgcgRAAAMDiOKkEAADA4hghBAAAsDgCIQAAgMURCAEAACyOQAgAAGBxBEIAAACLIxACAABYHIEQAADA4giEAAAAFkcgBAAAsDgCIQAAgMX9f18aH7ukJG4VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(1,figsize=(6,2.5), dpi=120, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(losses,marker='o',markersize=2,linewidth=0.0,markevery=5,label=\"RNN\")\n",
    "plt.plot([0,1000],[exact_energy,exact_energy],'k--')\n",
    "\n",
    "plt.xlabel(\"Step\",fontsize=15)\n",
    "plt.ylabel(\"$\\\\langle H \\\\rangle$\",fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(1,figsize=(6,2.5), dpi=120, facecolor='w', edgecolor='k')\n",
    "plt.plot(abs(np.array(losses)-exact_energy),marker='o',markersize=2,linewidth=0.0,markevery=5,label=\"RNN\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(5e-3,5)\n",
    "plt.xlabel(\"Step\",fontsize=15)\n",
    "plt.ylabel(\"$\\\\langle H \\\\rangle-H_{min}$\",fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "928251e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010886458606719995 -0.39433354139328003\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1]-exact_energy,losses[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
