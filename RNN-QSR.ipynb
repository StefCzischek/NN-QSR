{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45797e5b",
   "metadata": {},
   "source": [
    "# Exploring RNN architectures for Quantum state representation\n",
    "\n",
    "A couple types of rnn:\n",
    "    \n",
    "    Gru\n",
    "    \n",
    "    Lstm\n",
    "    \n",
    "    2D lstm (if we have a 2d input)\n",
    "    \n",
    "    Tranformer (masked) (masked makes it causal whereas unmasked would be non causal)\n",
    "    \n",
    "Extra networks:\n",
    "\n",
    "    Echo state networks  -> look into\n",
    "    \n",
    "    Resevoir computing cells -> look into\n",
    "        \n",
    "        Apparently you only train the output weights -> avoids a bunch of backprop\n",
    "        \n",
    "        but we use it in a recurrent fashion so you need to backprop still\n",
    "        \n",
    "        Look at BYOL for this (might not be possible)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4fa75",
   "metadata": {},
   "source": [
    "# Quantum state to represent\n",
    "\n",
    "- Start with Rydberg system\n",
    "\n",
    "Transverse and longitudinal view of ising model\n",
    "\n",
    "Excited state encourages nearby (within radius $R_b$)states to tend towards ground states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7f3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math,time\n",
    "import torch\n",
    "from torch import nn\n",
    "ngpu=1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89741030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.38052\n"
     ]
    }
   ],
   "source": [
    "# Hamiltonian parameters\n",
    "Lx = 16      # Linear size in x direction\n",
    "Ly = 16      # Linear size in y direction\n",
    "N = Lx*Ly   # Total number of atoms\n",
    "V = 7.0     # Strength of Van der Waals interaction\n",
    "Omega = 1.0 # Rabi frequency\n",
    "delta = 1.0 # Detuning \n",
    "exact_energy = {16:-0.45776822,64:-0.40522,144:-0.38852,256:-0.38052}[Lx*Ly]\n",
    "print(exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d13afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(nn.Module):\n",
    "    def __init__(self,device=device):\n",
    "        self.device=device\n",
    "        super(Sampler, self).__init__()\n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def sample_with_labels(self,B,L,grad=False):\n",
    "        \"\"\"Inputs:\n",
    "            B (int) - The number of states to generate in parallel\n",
    "            L (int) - The length of generated vectors\n",
    "            grad (boolean) - Whether or not to use gradients\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logppl - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i+B*j] had one spin flipped at position j\n",
    "                    relative to s[i]\n",
    "        \"\"\"\n",
    "        sample=self.sample(B,L)\n",
    "        sflip = torch.zeros([B,L,L,1],device=self.device)\n",
    "        for j in range(L):\n",
    "            #get all of the states with one spin flipped\n",
    "            sflip[:,j] = sample*1.0\n",
    "            sflip[:,j,j] = 1-sflip[:,j,j]\n",
    "        #compute all of their logscale probabilities\n",
    "        if not grad:\n",
    "            with torch.no_grad():\n",
    "                probs = self.logprobability(sflip.view([B*L,L,1]))\n",
    "        else:\n",
    "            probs = self.logprobability(sflip.view([B*L,L,1]))\n",
    "            \n",
    "        #might make sflip shape [B,L] in the future\n",
    "        return sample,probs.reshape([B,L])\n",
    "    \n",
    "    def sample_with_labelsALT(self,B,L,grad=False):\n",
    "        \"\"\"Returns:\n",
    "            samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
    "                       when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
    "            sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
    "        \"\"\"\n",
    "        sample,probs = self.sample_with_labels(B,L,grad)\n",
    "        #get the average of our logprobabilities and divide by 2\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        #compute the sum with a constant multiplied to keep the sum closeish to 1\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5dec33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sampler in module __main__:\n",
      "\n",
      "class Sampler(torch.nn.modules.module.Module)\n",
      " |  Sampler(device=device(type='cuda', index=0))\n",
      " |  \n",
      " |  Base class for all neural network modules.\n",
      " |  \n",
      " |  Your models should also subclass this class.\n",
      " |  \n",
      " |  Modules can also contain other Modules, allowing to nest them in\n",
      " |  a tree structure. You can assign the submodules as regular attributes::\n",
      " |  \n",
      " |      import torch.nn as nn\n",
      " |      import torch.nn.functional as F\n",
      " |  \n",
      " |      class Model(nn.Module):\n",
      " |          def __init__(self):\n",
      " |              super(Model, self).__init__()\n",
      " |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      " |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      " |  \n",
      " |          def forward(self, x):\n",
      " |              x = F.relu(self.conv1(x))\n",
      " |              return F.relu(self.conv2(x))\n",
      " |  \n",
      " |  Submodules assigned in this way will be registered, and will have their\n",
      " |  parameters converted too when you call :meth:`to`, etc.\n",
      " |  \n",
      " |  :ivar training: Boolean represents whether this module is in training or\n",
      " |                  evaluation mode.\n",
      " |  :vartype training: bool\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sampler\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, device=device(type='cuda', index=0))\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  logprobability(self, input)\n",
      " |      Compute the logscale probability of a given state\n",
      " |      Inputs:\n",
      " |          input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |      Returns:\n",
      " |          logp - [B] size vector of logscale probability labels\n",
      " |  \n",
      " |  sample(self, B, L)\n",
      " |      Generates a set states\n",
      " |      Inputs:\n",
      " |          B (int)            - The number of states to generate in parallel\n",
      " |          L (int)            - The length of generated vectors\n",
      " |      Returns:\n",
      " |          samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |  \n",
      " |  sample_with_labels(self, B, L, grad=False)\n",
      " |      Inputs:\n",
      " |          B (int) - The number of states to generate in parallel\n",
      " |          L (int) - The length of generated vectors\n",
      " |          grad (boolean) - Whether or not to use gradients\n",
      " |      Returns:\n",
      " |          samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |          logppl - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i+B*j] had one spin flipped at position j\n",
      " |                  relative to s[i]\n",
      " |  \n",
      " |  sample_with_labelsALT(self, B, L, grad=False)\n",
      " |      Returns:\n",
      " |      samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |      logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
      " |                 when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
      " |      sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |          or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb026d6",
   "metadata": {},
   "source": [
    "# Simple RNN to start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e73a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Sampler):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    def __init__(self,rnntype=\"GRU\",Nh=128,device=device, **kwargs):\n",
    "        super(RNN, self).__init__(device=device)\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=1,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(128,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.rnntype=rnntype\n",
    "        self.to(device)\n",
    "    def forward(self, input):\n",
    "        # h0 is shape [d*numlayers,B,H] but D=numlayers=1 so\n",
    "        # h0 has shape [1,B,H]\n",
    "        \n",
    "        if self.rnntype==\"LSTM\":\n",
    "            h0=[torch.zeros([1,input.shape[0],128],device=self.device),\n",
    "               torch.zeros([1,input.shape[0],128],device=self.device)]\n",
    "            #h0 and c0\n",
    "        else:\n",
    "            h0=torch.zeros([1,input.shape[0],128],device=self.device)\n",
    "        out,h=self.rnn(input,h0)\n",
    "        return self.lin(out)\n",
    "    \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        #Input should have shape [B,L,1]\n",
    "        B,L,one=input.shape\n",
    "        \n",
    "        #first prediction is with the zero input vector\n",
    "        data=torch.zeros([B,L,one],device=self.device)\n",
    "        #data is the input vector shifted one to the right, with the very first entry set to zero instead of using pbc\n",
    "        data[:,1:,:]=input[:,:-1,:]\n",
    "        \n",
    "        #real is going to be a set of actual values\n",
    "        real=input\n",
    "        #and pred is going to be a set of probabilities\n",
    "        #if real[i]=1 than you muptiply your conditional probability by pred[i]\n",
    "        #if real[i]=0 than you muliply by 1-pred[i]\n",
    "        \n",
    "        #probability predictions should be done WITH gradients\n",
    "        #with torch.no_grad():\n",
    "        \n",
    "        pred = self.forward(data)\n",
    "        ones = real*pred\n",
    "        zeros=(1-real)*(1-pred)\n",
    "        total = ones+zeros\n",
    "        #this is the sum you see in the cell above\n",
    "        #add 1e-10 to the prediction to avoid nans when total=0\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp\n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        if self.rnntype==\"LSTM\":\n",
    "            h=[torch.zeros([1,B,128],device=self.device),\n",
    "               torch.zeros([1,B,128],device=self.device)]\n",
    "            #h is h0 and c0\n",
    "        else:\n",
    "            h=torch.zeros([1,B,128],device=self.device)\n",
    "        #Sample set will have shape [N,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [N,L+1,1] \n",
    "        input = torch.zeros([B,L+1,1],device=self.device)\n",
    "        #sampling can be done without gradients\n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #run the rnn on shape [B,1,1]\n",
    "            \n",
    "            out,h=self.rnn(input[:,idx-1:idx,:],h)\n",
    "            out=out[:,0,:]\n",
    "            #if probs[i]=1 then there should be a 100% chance that sample[i]=1\n",
    "            #if probs[i]=0 then there should be a 0% chance that sample[i]=1\n",
    "            #stands that we generate a random uniform u and take int(u<probs) as our sample\n",
    "            probs=self.lin(out)\n",
    "            sample = (torch.rand([B,1],device=device)<probs).to(torch.float32)\n",
    "            input[:,idx,:]=sample\n",
    "        #input's first entry is zero to get a predction for the first atom\n",
    "        return input[:,1:,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e73636",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def Vij(Ly,Lx,Rcutoff,V,matrix):\n",
    "    #matrix will be size [Lx*Ly,Lx*Ly]\n",
    "    \n",
    "    i,j=cuda.grid(2)\n",
    "    if i>Ly or j>Lx:\n",
    "        return\n",
    "    R=Rcutoff**6\n",
    "    #flatten two indices into one\n",
    "    idx = Ly*j+i\n",
    "    # only fill in the upper diagonal\n",
    "    for k in range(idx+1,Lx*Ly):\n",
    "        #expand one index into two\n",
    "        i2 = k%Ly\n",
    "        j2=k//Ly\n",
    "        div = ((i2-i)**2+(j2-j)**2)**3\n",
    "        if div<=R:\n",
    "            matrix[idx][k]=V/div\n",
    "    \n",
    "\n",
    "class Hamiltonian():\n",
    "    def __init__(self,Lx,Ly,V,Omega,delta,R=2.01,device=device):\n",
    "        self.Lx       = Lx              # Size along x\n",
    "        self.Ly       = Ly              # Size along y\n",
    "        self.V        = V               # Van der Waals potential\n",
    "        self.Omega    = Omega           # Rabi frequency\n",
    "        self.delta    = delta           # Detuning\n",
    "        self.L        = Lx * Ly         # Number of spins\n",
    "        self.device   = device\n",
    "        self.R=R\n",
    "        self.buildlattice()\n",
    "        \n",
    "    def buildlattice(self):\n",
    "        Lx,Ly=self.Lx,self.Ly\n",
    "        \n",
    "        #diagonal hamiltonian portion can be written as a matrix multiplication then a dot product\n",
    "        self.Vij=nn.Linear(self.L,self.L).to(device)\n",
    "        \n",
    "        mat=np.zeros([self.L,self.L])\n",
    "        \n",
    "        Vij[(1,1),(Lx,Ly)](Lx,Ly,self.R,self.V,mat)\n",
    "        with torch.no_grad():\n",
    "            self.Vij.weight[:,:]=torch.Tensor(mat)\n",
    "            self.Vij.bias.fill_(-self.delta)\n",
    "\n",
    "\n",
    "    def localenergy(self,samples,logp,logppj):\n",
    "        \"\"\"\n",
    "        Takes in s, ln[p(s)] and ln[p(s')] (for all s'), then computes Hloc(s) for N samples s.\n",
    "        \n",
    "        Inputs:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logp - size B vector of logscale probabilities ln[p(s)]\n",
    "            logppj - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i][j] had one spin flipped at position j\n",
    "                    relative to s[i]\n",
    "        Returns:\n",
    "            size B vector of energies Hloc(s)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Going to calculate Eloc for each sample in a separate spot\n",
    "        # so eloc will have shape [B]\n",
    "        # recall samples has shape [B,L,1]\n",
    "        B=samples.shape[0]\n",
    "        eloc = torch.zeros(B,device=self.device)\n",
    "        # Chemical potential\n",
    "        with torch.no_grad():\n",
    "            tmp=self.Vij(samples.squeeze(2))\n",
    "            eloc += torch.sum(tmp*samples.squeeze(2),axis=1)\n",
    "        # Off-diagonal part\n",
    "        #flip ONE spin here and get sqrt(p(s)/p(s'))\n",
    "        #then sum over all spin flips\n",
    "        #think of ways to cheat\n",
    "        \n",
    "        #logppj is shape [B*L]\n",
    "        # the first N labels in logppj are the log probabilities for\n",
    "        # the N states but with the first state flipped (ground-> excited and excited-> ground)\n",
    "        # the second N all were calculated with only the second state flipped \n",
    "        # etc...\n",
    "        for j in range(self.L):\n",
    "            #logpflip is log(p(1-s))\n",
    "            #logp is log(p(s))?\n",
    "            #s' has one spin flipped at j\n",
    "            #with psi(s)=sqrt(p(s)), sigma_i^x = psi(s')/psi(s)?\n",
    "            \n",
    "            #make sure torch.exp is a thing\n",
    "            eloc += -0.5*self.Omega * torch.exp((logppj[:,j]-logp)/2)\n",
    "\n",
    "        return eloc\n",
    "    def localenergyALT(self,samples,logp,sumsqrtp,logsqrtp):\n",
    "        \"\"\"\n",
    "        Takes in s, ln[p(s)] and exp(-logsqrtp)*sum(sqrt[p(s')]), then computes Hloc(s) for N samples s.\n",
    "        \n",
    "        Inputs:\n",
    "            samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            logp     - size B vector of logscale probabilities ln[p(s)]\n",
    "            logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
    "                       when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
    "            sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
    "        Returns:\n",
    "            size B vector of energies Hloc(s)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Going to calculate Eloc for each sample in a separate spot\n",
    "        # so eloc will have shape [B]\n",
    "        # recall samples has shape [B,L,1]\n",
    "        B=samples.shape[0]\n",
    "        eloc = torch.zeros(B,device=self.device)\n",
    "        # Chemical potential\n",
    "        with torch.no_grad():\n",
    "            tmp=self.Vij(samples.squeeze(2))\n",
    "            eloc += torch.sum(tmp*samples.squeeze(2),axis=1)\n",
    "        # Off-diagonal part\n",
    "        #flip ONE spin here and get sqrt(p(s)/p(s'))\n",
    "        #then sum over all spin flips\n",
    "        #think of ways to cheat\n",
    "        \n",
    "        eloc += -0.5*self.Omega *sumsqrtp* torch.exp(logsqrtp-logp/2)\n",
    "\n",
    "        return eloc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00252ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Hamiltonian in module __main__:\n",
      "\n",
      "class Hamiltonian(builtins.object)\n",
      " |  Hamiltonian(Lx, Ly, V, Omega, delta, R=2.01, device=device(type='cuda', index=0))\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, Lx, Ly, V, Omega, delta, R=2.01, device=device(type='cuda', index=0))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  buildlattice(self)\n",
      " |  \n",
      " |  localenergy(self, samples, logp, logppj)\n",
      " |      Takes in s, ln[p(s)] and ln[p(s')] (for all s'), then computes Hloc(s) for N samples s.\n",
      " |      \n",
      " |      Inputs:\n",
      " |          samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |          logp - size B vector of logscale probabilities ln[p(s)]\n",
      " |          logppj - [B,L] matrix of logscale probabilities ln[p(s')] where s'[i][j] had one spin flipped at position j\n",
      " |                  relative to s[i]\n",
      " |      Returns:\n",
      " |          size B vector of energies Hloc(s)\n",
      " |  \n",
      " |  localenergyALT(self, samples, logp, sumsqrtp, logsqrtp)\n",
      " |      Takes in s, ln[p(s)] and exp(-logsqrtp)*sum(sqrt[p(s')]), then computes Hloc(s) for N samples s.\n",
      " |      \n",
      " |      Inputs:\n",
      " |          samples  - [B,L,1] matrix of zeros and ones for ground/excited states\n",
      " |          logp     - size B vector of logscale probabilities ln[p(s)]\n",
      " |          logsqrtp - size B vector of average (log p)/2 values used for numerical stability \n",
      " |                     when calculating sum_s'(sqrt[p(s')/p(s)]) \n",
      " |          sumsqrtp - size B vector of exp(-logsqrtp)*sum(sqrt[p(s')]).\n",
      " |      Returns:\n",
      " |          size B vector of energies Hloc(s)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Hamiltonian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b5e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "torch.Size([10, 16, 1])\n",
      "torch.Size([10, 16, 1])\n",
      "torch.Size([10, 16, 1]) torch.Size([10, 16])\n",
      "tensor(-11.0908, device='cuda:0', grad_fn=<SelectBackward>) \n",
      " tensor([[-11.0832, -11.0940, -11.0814, -11.0950, -11.0796, -11.0977, -11.0987,\n",
      "         -11.0965, -11.0827, -11.0838, -11.0935, -11.0803, -11.0974, -11.0988,\n",
      "         -11.0970, -11.0811]], device='cuda:0')\n",
      "RNN(\n",
      "  (rnn): LSTM(1, 128, batch_first=True)\n",
      "  (lin): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "testrnn = RNN(rnntype=\"LSTM\")\n",
    "print(testrnn.rnntype)\n",
    "x=torch.zeros([10,4*4,1]).to(device)\n",
    "print(testrnn(x).shape)\n",
    "sample = testrnn.sample(10,4*4)\n",
    "print(sample.shape)\n",
    "\n",
    "sample,pflip = testrnn.sample_with_labels(10,4*4,grad=False)\n",
    "logp=testrnn.logprobability(sample)\n",
    "print(sample.shape,pflip.shape)\n",
    "print(logp[0],'\\n',pflip[::10])\n",
    "\n",
    "print(testrnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5e48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_rnn_with_optim(rnntype,Nh,lr=1e-3,beta1=0.9,beta2=0.999):\n",
    "    rnn = RNN(rnntype=rnntype,Nh=Nh)\n",
    "    optimizer = torch.optim.Adam(\n",
    "    rnn.parameters(), \n",
    "    lr=lr, \n",
    "    betas=(beta1,beta2)\n",
    "    )\n",
    "    return rnn,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525888b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Hamiltonian(Lx,Ly,V,Omega,delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3205a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4958, 0.4956, 0.4954, 0.4951], device='cuda:0')\n",
      "tensor([0.4999, 0.4993, 0.4989, 0.4987], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "bsize=512\n",
    "\n",
    "testrnn,optimizer=new_rnn_with_optim(\"GRU\",128,lr=1e-3)\n",
    "samplernn = RNN(rnntype=\"GRU\",Nh=128)\n",
    "with torch.no_grad():\n",
    "    print(testrnn(torch.zeros([1,4,1],device=device))[0,:,0])\n",
    "    print(samplernn(torch.zeros([1,4,1],device=device))[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dd30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(m, target_network, network):\n",
    "    for target_param, param in zip(target_network.parameters(), network.parameters()):\n",
    "        target_param.data.copy_(target_param.data*m + param.data*(1-m))\n",
    "with torch.no_grad():\n",
    "    momentum_update(0.0,samplernn,testrnn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79f6d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4958, 0.4956, 0.4954, 0.4951], device='cuda:0')\n",
      "tensor([0.4958, 0.4956, 0.4954, 0.4951], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(testrnn(torch.zeros([1,4,1],device=device))[0,:,0])\n",
    "    print(samplernn(torch.zeros([1,4,1],device=device))[0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855d365",
   "metadata": {},
   "source": [
    "# Training with Memory Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c37ca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "USEQUEUE=True\n",
    "losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada68df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "0,2.65|68,-0.24|136,-0.30|204,-0.30|272,-0.31|340,-0.31|408,-0.32|475,-0.33|543,-0.33|611,-0.34|679,-0.34|746,-0.35|814,-0.35|882,-0.36|950,-0.36|1018,-0.36|1086,-0.36|1154,-0.37|1222,-0.37|1290,-0.37|1358,-0.37|1425,-0.37|1493,-0.37|1561,-0.31|1629.5235583782196\n"
     ]
    }
   ],
   "source": [
    "M=0.9\n",
    "BlockNum=(Lx*Ly)//8\n",
    "BbyL=bsize//BlockNum\n",
    "\n",
    "print(BbyL*BlockNum)\n",
    "samplequeue = torch.zeros([bsize,Lx*Ly,1]).to(device)\n",
    "sump_queue=torch.zeros([bsize]).to(device)\n",
    "sqrtp_queue=torch.zeros([bsize]).to(device)\n",
    "\n",
    "for i in range(BlockNum):\n",
    "    sample,sump,sqrtp = samplernn.sample_with_labelsALT(BbyL,Lx*Ly,grad=False)\n",
    "    with torch.no_grad():\n",
    "        samplequeue[i*BbyL:(i+1)*BbyL]=sample\n",
    "        sump_queue[i*BbyL:(i+1)*BbyL]=sump\n",
    "        sqrtp_queue[i*BbyL:(i+1)*BbyL]=sqrtp\n",
    "\n",
    "if not USEQUEUE:\n",
    "    nqueue_updates = BlockNum\n",
    "else:\n",
    "    nqueue_updates=1\n",
    "\n",
    "\n",
    "i=0\n",
    "t=time.time()\n",
    "for x in range(12000):\n",
    "    \n",
    "    for k in range(nqueue_updates):\n",
    "        sample,sump,sqrtp = samplernn.sample_with_labelsALT(BbyL,Lx*Ly,grad=False)\n",
    "        with torch.no_grad():\n",
    "            samplequeue[i*BbyL:(i+1)*BbyL]=sample\n",
    "            sump_queue[i*BbyL:(i+1)*BbyL]=sump\n",
    "            sqrtp_queue[i*BbyL:(i+1)*BbyL]=sqrtp\n",
    "        i=(i+1)%BlockNum\n",
    "    \n",
    "    logp=testrnn.logprobability(samplequeue)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        E=h.localenergyALT(samplequeue,logp,sump_queue,sqrtp_queue)\n",
    "        Eo=E.mean()\n",
    "            \n",
    "    loss = (E*logp - Eo*logp).mean()\n",
    "    \n",
    "    ERR  = Eo/(Lx*Ly)\n",
    "    testrnn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(ERR.cpu().item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        momentum_update(M,samplernn,testrnn)\n",
    "    \n",
    "    \n",
    "    if x%500==0:\n",
    "        print(int(time.time()-t),end=\",%.2f|\"%(losses[-1]))\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0707871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(1,figsize=(6,2.5), dpi=120, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(losses,marker='o',markersize=2,linewidth=0.0,markevery=5,label=\"RNN\")\n",
    "plt.plot([0,len(losses)],[exact_energy,exact_energy],'k--')\n",
    "\n",
    "plt.xlabel(\"Step\",fontsize=15)\n",
    "plt.ylabel(\"$\\\\langle H \\\\rangle$\",fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(1,figsize=(6,2.5), dpi=120, facecolor='w', edgecolor='k')\n",
    "plt.plot(abs(np.array(losses)-exact_energy),marker='o',markersize=2,linewidth=0.0,markevery=5,label=\"RNN\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(5e-3,5)\n",
    "plt.xlabel(\"Step\",fontsize=15)\n",
    "plt.ylabel(\"$\\\\langle H \\\\rangle-H_{min}$\",fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928251e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03829325630187991 -0.3422267436981201\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1]-exact_energy,losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faecec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3394317328929901\n"
     ]
    }
   ],
   "source": [
    "for k in range(BlockNum):\n",
    "    sample,sump,sqrtp = testrnn.sample_with_labelsALT(BbyL,Lx*Ly,grad=False)\n",
    "    with torch.no_grad():\n",
    "        samplequeue[i*BbyL:(i+1)*BbyL]=sample\n",
    "        sump_queue[i*BbyL:(i+1)*BbyL]=sump\n",
    "        sqrtp_queue[i*BbyL:(i+1)*BbyL]=sqrtp\n",
    "    i=(i+1)%BlockNum\n",
    "logp=testrnn.logprobability(samplequeue)\n",
    "with torch.no_grad():\n",
    "    E=h.localenergyALT(samplequeue,logp,sump_queue,sqrtp_queue)\n",
    "    Eo=E.mean()\n",
    "\n",
    "ERR  = Eo/(Lx*Ly)\n",
    "\n",
    "print(ERR.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc9447f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41891a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"git commit -a -m \\\"Auto Commit\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85eafe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
