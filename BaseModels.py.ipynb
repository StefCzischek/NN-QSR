{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf7b69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Ti', major=8, minor=6, total_memory=12287MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa56499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMaskedTransformerEncoder(nn.Module):#(torch.jit.ScriptModule):\n",
    "    \"\"\"\n",
    "    Base class for a fast, masked transformer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Nh=128,dropout=0.0,num_layers=2,nhead=8,device=device):\n",
    "        super(FastMaskedTransformerEncoder, self).__init__()\n",
    "        #print(nhead)\n",
    "        #Encoder only transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.device=device\n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        \"\"\"\n",
    "        Set the transformer mask for a sequence of length L\n",
    "        Inputs: \n",
    "            L (int) - the desired sequence length\n",
    "        \"\"\"\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.mask = torch.log(torch.tril(torch.ones([L,L],device=self.device)))        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # type: (Tensor)->Tensor\n",
    "        \"\"\"Run the transformer on a sequence of length L\n",
    "            Inputs:\n",
    "                input -  Tensor of shape [L,B,Nh]\n",
    "            Outputs:    \n",
    "                Tensor of shape [L,B,Nh]\n",
    "        \"\"\"\n",
    "        return self.transformer(input,self.mask)\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        # type: (Tensor,Optional[Tensor],int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,Nh]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,Nh]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        #HMMM\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i,layer in enumerate(self.transformer.layers):\n",
    "            \n",
    "            tgt=output\n",
    "            #have to merge the functions into one\n",
    "            src = tgt[idx:, :, :]\n",
    "            mask = None if idx==-1 else self.mask[idx:]\n",
    "\n",
    "            # self attention part\n",
    "            src2 = layer.self_attn(\n",
    "                src,#only do attention with the last elem of the sequence\n",
    "                tgt,\n",
    "                tgt,\n",
    "                attn_mask=mask,  \n",
    "                key_padding_mask=None,\n",
    "            )[0]\n",
    "            #straight from torch transformer encoder code\n",
    "            src = src + layer.dropout1(src2)\n",
    "            src = layer.norm1(src)\n",
    "            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "            src = src + layer.dropout2(src2)\n",
    "            src = layer.norm2(src)\n",
    "            #return src\n",
    "            \n",
    "            output = src#self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        # type: (Tensor) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"\n",
    "        Equivalent to forward, but the intermediate outputs are also returned\n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,Nh]\n",
    "        Outputs:\n",
    "            output - Tensor of shape [L,B,Nh]\n",
    "            new_cache - Tensor of shape [?,L,B,Nh]\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc686f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PE2D(nn.Module):\n",
    "    \"\"\"Sequence-First 2D Positional Encoder\"\"\"\n",
    "    #TODO: Positional encoding is wrong because the spins are at index i+1 when we sample and get probabilities\n",
    "    def __init__(self, d_model, Lx,Ly,device,n_encode=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        assert (d_model%4==0)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(Lx*Ly, d_model)\n",
    "        \n",
    "        if type(n_encode)==type(None):\n",
    "            n_encode=3*d_model//4\n",
    "        for pos in range(Lx*Ly):\n",
    "            x=pos//Ly\n",
    "            y=pos%Ly\n",
    "            # Only going to fill 3/4 of the matrix so the\n",
    "            # occupation values are preserved\n",
    "            for i in range(0, n_encode, 4):\n",
    "                \n",
    "                #x direction encoding\n",
    "                pe[pos, i] =                 math.sin(x / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 1] =                 math.cos(x / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                #y direction encoding\n",
    "                pe[pos, i+2] =                 math.sin(y / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 3] =                 math.cos(y / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(1).to(device)\n",
    "        self.L=Lx*Ly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds a 2D positional encoding of size d_model to x\n",
    "        Inputs:\n",
    "            Tensor of shape [L,B,?]\n",
    "        Outputs:\n",
    "            Tensor of shape [L,B,d_model]\n",
    "        \"\"\"\n",
    "        if self.d_model%x.shape[-1]!=0:\n",
    "            return x.repeat(1,1,self.d_model//x.shape[-1]+1)[:,:,:self.d_model] + self.pe[:x.shape[0]]\n",
    "        return x.repeat(1,1,self.d_model//x.shape[-1]) + self.pe[:x.shape[0]]\n",
    "    \n",
    "class PE1D(nn.Module):\n",
    "    \"\"\"Sequence-First 1D Positional Encoder\"\"\"\n",
    "    def __init__(self, d_model, L,device,n_encode=None):\n",
    "        super().__init__()\n",
    "        assert (d_model%4==0)\n",
    "        self.d_model = d_model\n",
    "        # create constant 'pe' matrix with values dependent on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(L, d_model)\n",
    "        if type(n_encode)==type(None):\n",
    "            n_encode=3*d_model//4\n",
    "        for pos in range(L):\n",
    "            # Only going to fill 3/4 of the matrix so the\n",
    "            # occupation values are preserved\n",
    "            for i in range(0, n_encode, 2):\n",
    "                #position encoding\n",
    "                pe[pos, i] =                 math.sin(pos / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 1] =             math.cos(pos / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "        self.pe = pe.unsqueeze(1).to(device)\n",
    "        self.L=L\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds a 1D positional encoding of size d_model to x\n",
    "        Inputs:\n",
    "            Tensor of shape [L,B,?]\n",
    "        Outputs:\n",
    "            Tensor of shape [L,B,d_model]\n",
    "        \"\"\"\n",
    "        if self.d_model%x.shape[-1]!=0:\n",
    "            return x.repeat(1,1,self.d_model//x.shape[-1]+1)[:,:,:self.d_model] + self.pe[:x.shape[0]]\n",
    "        return x.repeat(1,1,self.d_model//x.shape[-1]) + self.pe[:x.shape[0]]\n",
    "\n",
    "def pe2Dtest(Lx,Ly):\n",
    "    pe = torch.zeros(Lx*Ly, 2)\n",
    "    for pos in range(Lx*Ly):\n",
    "        x=pos//Ly\n",
    "        y=pos%Ly\n",
    "        # Only going to fill 3/4 of the matrix so the\n",
    "        # occupation values are preserved\n",
    "        #x direction encoding\n",
    "        pe[pos, 0] =                 x\n",
    "        #y direction encoding\n",
    "        pe[pos, 1] =                 y\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c68639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch2D(nn.Module):\n",
    "    def __init__(self,n,Lx):\n",
    "        super().__init__()\n",
    "        self.n=n\n",
    "        self.Lx=Lx\n",
    "    def forward(self,x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        n,Lx=self.n,self.Lx\n",
    "        \"\"\"Unflatten a tensor back to 2D, break it into nxn chunks, then flatten the sequence and the chunks\n",
    "            Input:\n",
    "                Tensor of shape [B,L]\n",
    "            Output:\n",
    "                Tensor of shape [B,L//n^2,n^2]\n",
    "        \"\"\"\n",
    "        #make the input 2D then break it into 2x2 chunks \n",
    "        #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "        return x.view([x.shape[0],Lx,Lx]).unfold(-2,n,n).unfold(-2,n,n).reshape([x.shape[0],int(Lx*Lx//n**2),int(n**2)])\n",
    "\n",
    "    def reverse(self,x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Inverse function of forward\n",
    "            Input:\n",
    "                Tensor of shape [B,L//n^2,n^2]\n",
    "            Output:\n",
    "                Tensor of shape [B,L]\n",
    "        \"\"\"\n",
    "        n,Lx=self.n,self.Lx\n",
    "        # original sequence order can be retrieved by chunking twice more\n",
    "        #in the x-direction you should have chunks of size 2, but in y it should\n",
    "        #be chunks of size Ly//2\n",
    "        return x.unfold(-2,Lx//n,Lx//n).unfold(-2,n,n).reshape([x.shape[0],Lx*Lx])\n",
    "    \n",
    "class Patch1D(nn.Module):\n",
    "    def __init__(self,n,L):\n",
    "        super().__init__()\n",
    "        self.n=n\n",
    "        self.L = L\n",
    "    def forward(self,x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Break a tensor into chunks, essentially a wrapper of reshape\n",
    "            Input:\n",
    "                Tensor of shape [B,L]\n",
    "            Output:\n",
    "                Tensor of shape [B,L/n,n]\n",
    "        \"\"\"\n",
    "        #make the input 2D then break it into 2x2 chunks \n",
    "        #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "        return x.reshape([x.shape[0],self.L//self.n,self.n])\n",
    "\n",
    "    def reverse(self,x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Inverse function of forward\n",
    "            Input:\n",
    "                Tensor of shape [B,L/n,n]\n",
    "            Output:\n",
    "                Tensor of shape [B,L]\n",
    "        \"\"\"\n",
    "        # original sequence order can be retrieved by chunking twice more\n",
    "        #in the x-direction you should have chunks of size 2, but in y it should\n",
    "        #be chunks of size Ly//2\n",
    "        return x.reshape([x.shape[0],self.L])\n",
    "    \n",
    "    \n",
    "@torch.jit.script\n",
    "def patch2onehot(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f3bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "         [ 3,  4,  5,  9, 10, 11, 15, 16, 17],\n",
      "         [18, 19, 20, 24, 25, 26, 30, 31, 32],\n",
      "         [21, 22, 23, 27, 28, 29, 33, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17],\n",
      "         [18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26],\n",
      "         [27, 28, 29],\n",
      "         [30, 31, 32],\n",
      "         [33, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [2., 0.],\n",
       "        [2., 1.],\n",
       "        [3., 0.],\n",
       "        [3., 1.],\n",
       "        [4., 0.],\n",
       "        [4., 1.],\n",
       "        [5., 0.],\n",
       "        [5., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(36).unsqueeze(0)\n",
    "\n",
    "patch2d = Patch2D(3,6)\n",
    "\n",
    "patch1d = Patch1D(3,36)\n",
    "\n",
    "y2 = patch2d(x)\n",
    "\n",
    "y1 = patch1d(x)\n",
    "\n",
    "print(x.view(6,6))\n",
    "\n",
    "print(y2)\n",
    "print(patch2d.reverse(y2).view(6,6))\n",
    "print(y1)\n",
    "print(patch1d.reverse(y1).view(6,6))\n",
    "\n",
    "pe2Dtest(6,6//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3aa590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF(Sampler):\n",
    "    \"\"\"Same architecture as SlowTransformer (weights can be shared) but with improvements to sampling and labelling which\n",
    "    can give a 2x performance boost\"\"\"\n",
    "    \n",
    "    def __init__(self,Lx,Ly,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            Lx,Ly (int) -- Sequence dimensions\n",
    "            Nh (int) -- size of the input vector at each sequence element (same as d_model)\n",
    "            decoder (bool) -- whether to use a TF decoder or encoder. Using a decoder isn't currently implemented\n",
    "        \"\"\"\n",
    "        super(TF, self).__init__(device=device)\n",
    "        \n",
    "        self.pe = PE2D(Nh, Lx,Ly,device)\n",
    "        \n",
    "        self.transformer = FastMaskedTransformerEncoder(Nh=Nh,dropout=dropout,num_layers=num_layers,nhead=nhead)        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.set_mask(Lx*Ly)\n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        \"\"\"Initialize the self-attention mask\"\"\"\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.transformer.set_mask(L)\n",
    "        self.pe.L=L\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]!=self.L:\n",
    "            self.set_mask(input.shape[1])\n",
    "        \n",
    "        input=self.pe(input.transpose(1,0))\n",
    "        output = self.transformer(input)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        #Input should have shape [B,L,1]\n",
    "        B,L,one=input.shape\n",
    "        \n",
    "        #first prediction is with the zero input vector\n",
    "        data=torch.zeros([B,L,one],device=self.device)\n",
    "        #data is the input vector shifted one to the right, with the very first entry set to zero instead of using pbc\n",
    "        data[:,1:,:]=input[:,:-1,:]\n",
    "        \n",
    "        #real is going to be a set of actual values\n",
    "        real=input\n",
    "        #and pred is going to be a set of probabilities\n",
    "        #if real[i]=1 than you multiply your conditional probability by pred[i]\n",
    "        #if real[i]=0 than you multiply by 1-pred[i]\n",
    "        pred = self.forward(data)\n",
    "        ones = real*pred\n",
    "        zeros=(1-real)*(1-pred)\n",
    "        total = ones+zeros\n",
    "        #this is the sum you see in the cell above\n",
    "        #add 1e-10 to the prediction to avoid nans when total=0\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        sflip = torch.zeros([B,L,L,1],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L):\n",
    "            #get all of the states with one spin flipped\n",
    "            sflip[:,j] = sample*1.0\n",
    "            sflip[:,j,j] = 1-sflip[:,j,j]\n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            #prepare sample to be used as cache\n",
    "            B,L,one=sample.shape\n",
    "            dsample=torch.zeros([B,L,one],device=self.device)\n",
    "            dsample[:,1:,:]=sample[:,:-1,:]\n",
    "\n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.transformer.make_cache(self.pe(dsample.transpose(1,0)))\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L,B*L//D,cache.shape[-1])\n",
    "\n",
    "            pred0 = self.lin(out.transpose(1,0))\n",
    "            ones = sample*pred0\n",
    "            zeros=(1-sample)*(1-pred0)\n",
    "            total0 = ones+zeros\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out \n",
    "                tmp = real.reshape([B*L//D,L,1])\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[:,1:,:]=tmp[:,:-1,:]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample.transpose(1,0))\n",
    "                #grab your transformer output\n",
    "                out,_=self.transformer.next_with_cache(tgt,cache[:,:N],N)\n",
    "\n",
    "                # self.lin actually does some repeated work but it's probably\n",
    "                # negligable compared to the time attention takes\n",
    "                output = self.lin(out[N:].transpose(1,0))\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,L-N,1])\n",
    "                real=real[:,:,N:]\n",
    "                ones = real*pred\n",
    "                zeros=(1-real)*(1-pred)\n",
    "                total = ones+zeros\n",
    "                #sum across the sequence for probabilities\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=2).squeeze(2)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N]+1e-10),dim=1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample,probs\n",
    "        \n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        #transformers don't do batch first so to save a bunch of transpose calls \n",
    "        input = torch.zeros([L+1,B,1],device=self.device)\n",
    "        #self.set_mask(L)\n",
    "        \n",
    "        cache=None\n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #run the rnn on shape [B,1,1]   \n",
    "            #encode the input to the proper shape\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.transformer.next_with_cache(encoded_input,cache)\n",
    "            #if probs[i]=1 then there should be a 100% chance that sample[i]=1\n",
    "            #if probs[i]=0 then there should be a 0% chance that sample[i]=1\n",
    "            #stands that we generate a random uniform u and take int(u<probs) as our sample\n",
    "            probs=self.lin(output[-1,:,:])\n",
    "            sample = (torch.rand([B,1],device=device)<probs).to(torch.float32)\n",
    "            input[idx,:,:]=sample\n",
    "        #input's first entry is zero to get a predction for the first atom\n",
    "        #print(\".\",end=\"\")\n",
    "        return input.transpose(1,0)[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0855808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTFBase(Sampler):#(torch.jit.ScriptModule):\n",
    "    \"\"\"\n",
    "    Base class for the two patch transformer architectures \n",
    "    \n",
    "    Architexture wise this is how a patched transformer works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each patch given it and all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    Outputs should either be size 1 (the probability of the current patch which is input) or size 16 (for 2x2 patches where \n",
    "    the probability represented is of each potential patch)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,_2D=False,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8,outsize=1, **kwargs):\n",
    "        super(PTFBase, self).__init__()\n",
    "        #print(nhead)\n",
    "        if _2D:\n",
    "            self.pe = PE2D(Nh, Lx//2,Lx//2,device)\n",
    "            self.patch=Patch2D(2,Lx)\n",
    "        else:\n",
    "            self.pe = PE1D(Nh,Lx//4,device)\n",
    "            self.patch=Patch1D(4,Lx)\n",
    "        self.device=device\n",
    "        #Encoder only transformer\n",
    "        #misinterperetation on encoder made it so this code does not work\n",
    "        self.transformer = FastMaskedTransformerEncoder(Nh=Nh,dropout=dropout,num_layers=num_layers,nhead=nhead)       \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,outsize),\n",
    "                nn.Sigmoid() if outsize==1 else nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "        self.Lx=Lx\n",
    "        if _2D:\n",
    "            self.set_mask(Lx**2//4)\n",
    "        else:\n",
    "            self.set_mask(Lx//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "        \n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        \"\"\"Initialize the self-attention mask\"\"\"\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.transformer.set_mask(L)\n",
    "        self.pe.L=L\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        input=self.pe(self.patch(input.squeeze(-1)).transpose(1,0))\n",
    "        output = self.transformer(input)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "# In[7]:\n",
    "\n",
    "\n",
    "class PTF(PTFBase):\n",
    "    \"\"\"Note: logprobability IS normalized \n",
    "    \n",
    "    Architexture wise this is how it works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each potential patch given all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    \n",
    "    This model has 16 outputs, which describes the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        #only important bit is that outsize = 16\n",
    "        super(PTF, self).__init__(Lx,outsize=16,**kwargs)\n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        \n",
    "        #shape is modified to [L//4,B,4]\n",
    "        input = self.patch(input.squeeze(-1)).transpose(1,0)\n",
    "        \n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        data[1:]=input[:-1]\n",
    "        \n",
    "        #[L//4,B,4] -> [L//4,B,Nh]\n",
    "        encoded=self.pe(data)\n",
    "        #shape is preserved\n",
    "        output = self.transformer(encoded)\n",
    "        # [L//4,B,Nh] -> [L//4,B,16]\n",
    "        output = self.lin(output)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [L//4,B,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[L//4,B,16] -> [L//4,B]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[L//4,B] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=0)\n",
    "        return logp   \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([L+1,B,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.transformer.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample\n",
    "            \n",
    "        #remove the leading zero in the input    \n",
    "        input=input[1:]\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return self.patch.reverse(input.transpose(1,0)).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = self.patch(sample.squeeze(-1))\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #switch sample into sequence-first\n",
    "        sample = sample.transpose(1,0)\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            data[1:]=sample[:-1]\n",
    "            \n",
    "            #[L//4,B,4] -> [L//4,B,Nh]\n",
    "            encoded=self.pe(data)\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.transformer.make_cache(encoded)\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L//4,B*L//D,cache.shape[-1])\n",
    "\n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [L//4,B,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[L//4,B,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1).transpose(1,0)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4]).transpose(1,0)\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[1:]=tmp[:-1]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample)\n",
    "                #grab your transformer output\n",
    "                out,_=self.transformer.next_with_cache(tgt,cache[:,:N//4],N//4)\n",
    "\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out[N//4:].transpose(1,0))\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                \n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs\n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9ed665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t40\n",
      "Q                             \t\t\t1\n",
      "K                             \t\t\t1024\n",
      "B                             \t\t\t1024\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.96875\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t10\n",
      "hamiltonian                   \t\t\tTFIM\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPTF\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "ffq                           \t\t\tFalse\n",
      "h                             \t\t\t-1.0\n",
      "J                             \t\t\t1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=40\n",
    "op.L=Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.Q=1\n",
    "op.K=1024\n",
    "op.USEQUEUE=0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "op.hamiltonian=\"TFIM\"\n",
    "#op.steps=4000\n",
    "op.dir=\"PTF\"\n",
    "op.steps=12000\n",
    "op.NLOOPS=10\n",
    "\n",
    "op.h=-1.0\n",
    "op.J=1.0\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817c5a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "myptf=PTF(Lx,Nh=op.Nh,num_layers=2,_2D=False)\n",
    "\n",
    "trainsformer = torch.jit.script(myptf)\n",
    "#trainsformer = torch.jit.script(PatchedRNN(Lx,Nh=op.Nh))\n",
    "#trainsformer = RNN(Nh=op.Nh)\n",
    "#sampleformer= PatchedRNN(Lx,Nh=op.Nh)\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f5585ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-1.2642 40\n",
      "0,-0.998|\n",
      "52,-1.264|102,-1.264|151,-1.264|200,-1.264|250,-1.264|299,-1.264|348,-1.264|397,-1.264|\n",
      "447,-1.264|496,-1.264|545,-1.264|595,-1.264|644,-1.264|693,-1.264|743,-1.264|792,-1.264|\n",
      "841,-1.264|891,-1.264|940,-1.264|989,-1.264|1039,-1.264|1088,-1.264|1137,-1.264|1186.8282067775726 12000\n"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1804631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t500\n",
      "Q                             \t\t\t1\n",
      "K                             \t\t\t512\n",
      "B                             \t\t\t512\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.96875\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t125\n",
      "hamiltonian                   \t\t\tTFIM\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPTF\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "ffq                           \t\t\tFalse\n",
      "h                             \t\t\t-1.0\n",
      "J                             \t\t\t1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=500\n",
    "op.L=Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.Q=1\n",
    "op.K=512\n",
    "op.USEQUEUE=0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "op.hamiltonian=\"TFIM\"\n",
    "#op.steps=4000\n",
    "op.dir=\"PTF\"\n",
    "op.steps=12000\n",
    "op.NLOOPS=125\n",
    "\n",
    "op.h=-1.0\n",
    "op.J=1.0\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ce40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "myptf=PTF(Lx,Nh=op.Nh,num_layers=2,_2D=False)\n",
    "\n",
    "trainsformer = torch.jit.script(myptf)\n",
    "#trainsformer = torch.jit.script(PatchedRNN(Lx,Nh=op.Nh))\n",
    "#trainsformer = RNN(Nh=op.Nh)\n",
    "#sampleformer= PatchedRNN(Lx,Nh=op.Nh)\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b70659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-1.2725 500\n",
      "6,-0.967|\n"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff851dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testRNN(RNN):\n",
    "    \"\"\"Just adds some functions which make sure all probability labels are consistent\"\"\"\n",
    "    def __init__(self,L,**kwargs):\n",
    "        super(testRNN,self).__init__(**kwargs)\n",
    "        self.L=L\n",
    "        self.reset(1)\n",
    "#functions below aren't really necessary anymore since there was no issue with masking (they serve to avoid using a mask)\n",
    "    def reset(self,B):\n",
    "        # type: (int) -> Tensor\n",
    "        \"\"\"Setup for an autoregressive transformer\"\"\"\n",
    "        self._input = torch.zeros([B,self.L+1,1],device=self.device)\n",
    "        self._i=1\n",
    "        h0=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "        out,self._cache=self.rnn(torch.zeros([B,1,1],device=self.device),h0)\n",
    "        out=out[:,0,:]\n",
    "        probs=self.lin(out)\n",
    "        return probs\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    def getnext(self,vect):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Get probability for the next output in an autoregressive transformer\"\"\"\n",
    "        self._input[:,self._i]=vect\n",
    "        self._i+=1\n",
    "        out,self._cache=self.rnn(vect.unsqueeze(1),self._cache)\n",
    "        out=out[:,0,:]\n",
    "        probs=self.lin(out)\n",
    "        return probs\n",
    "    @torch.jit.export\n",
    "    def testsample(self,B):\n",
    "        # type: (int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Generate states with their probabilities in logscale\"\"\"\n",
    "        #set up variables\n",
    "        \n",
    "        L=self.L\n",
    "        probs=self.reset(B).squeeze(0)\n",
    "        sprobs=torch.zeros([B],device=self.device)\n",
    "        samples = torch.zeros([B,L,1],device=self.device)\n",
    "        #with torch.no_grad():\n",
    "        for idx in range(L):\n",
    "            #loop through L sequence elements and generate next in sequence based off of probabilities\n",
    "            sample = (torch.rand([B,1],device=self.device)<probs).to(torch.float32)\n",
    "            #print(sample.shape,samples.shape)\n",
    "            samples[:,idx] = sample\n",
    "            \n",
    "            ones = sample*probs\n",
    "            zeros=(1-sample)*(1-probs)\n",
    "            total = ones+zeros\n",
    "            sprobs+=torch.log(total).squeeze(-1)\n",
    "            if idx!=L-1: probs = self.getnext(sample)\n",
    "                \n",
    "        return samples,sprobs\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def testlabels(self,samples,B):\n",
    "        # type: (Tensor,int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Get logscale probabilities of all states with one spin flipped at position j\"\"\"\n",
    "        L=self.L\n",
    "        with torch.no_grad():\n",
    "            #print(\"|\",end=\"\")\n",
    "            orig=samples\n",
    "            samples=samples*1\n",
    "            \n",
    "            logprobs=torch.zeros([B,L],device=self.device)\n",
    "            for k in range(L):\n",
    "                #loop cross L flipped states (batched)\n",
    "                probs=self.reset(B).squeeze(0)\n",
    "                sprobs = torch.zeros([B],device=self.device)\n",
    "                #loop across sequence\n",
    "                for idx in range(L):\n",
    "                    \n",
    "                    sample = samples[:,idx] \n",
    "                    #kth state is flipped\n",
    "                    if idx==k:\n",
    "                        #multiply by 1 as a way to copy the tensor to new memory\n",
    "                        sample=sample*1.0\n",
    "                        #print(sample.shape)\n",
    "                        sample[:,0] = 1-sample[:,0]\n",
    "                    \n",
    "                    ones = sample*probs\n",
    "                    zeros=(1-sample)*(1-probs)\n",
    "                    total = ones+zeros\n",
    "                    sprobs+=torch.log(total).squeeze(-1)\n",
    "                    if idx!=L-1: probs = self.getnext(sample)\n",
    "                logprobs[:,k]=sprobs\n",
    "        return orig,logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a727cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = \"..\\\\NN-QSR\\\\RNN\\\\Rydberg\\\\64-NoQ-B=512-K=512-Nh=256-kl=0.00\\\\0\"\n",
    "op=Opt()\n",
    "op.L=64\n",
    "op.Nh=256\n",
    "tst=torch.load(mydir+\"/T\")\n",
    "\n",
    "tesRNN = torch.jit.script(testRNN(64,Nh=op.Nh))\n",
    "momentum_update(0,tesRNN,tst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ac9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||9.748275756835938\n"
     ]
    }
   ],
   "source": [
    "op.K=512\n",
    "op.Q=8\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "\n",
    "\n",
    "# Hamiltonian parameters\n",
    "N = op.L   # Total number of atoms\n",
    "V = 7.0     # Strength of Van der Waals interaction\n",
    "Omega = 1.0 # Rabi frequency\n",
    "delta = 1.0 # Detuning \n",
    "\n",
    "if op.hamiltonian==\"Rydberg\":\n",
    "    Lx=Ly=int(op.L**0.5)\n",
    "    op.L=Lx*Ly\n",
    "    h = Rydberg(Lx,Ly,V,Omega,delta)\n",
    "else:\n",
    "    #hope for the best here since there aren't defaults\n",
    "    h = TFIM(op.L,op.h,op.J)\n",
    "\n",
    "\n",
    "E_queue = torch.zeros([op.B],device=device)\n",
    "def fill_queue(net):\n",
    "    for i in range(op.Q):\n",
    "        print(\"|\",end=\"\")\n",
    "        if True:\n",
    "            with torch.no_grad():\n",
    "                sample,lp = net.testsample(op.K)\n",
    "                _,probs= net.testlabels(sample,op.K)\n",
    "            \n",
    "                #print(sample.shape)\n",
    "                E_i=h.localenergy(sample,lp,probs)\n",
    "                E_queue[i*op.K:(i+1)*op.K]=E_i\n",
    "        else:\n",
    "            sample,sump,sqrtp = net.sample_with_labelsALT(op.K,op.L,grad=False,nloops=144)\n",
    "            with torch.no_grad():\n",
    "                lp=net.logprobability(sample)\n",
    "                E_i=h.localenergyALT(sample,lp,sump,sqrtp)\n",
    "                E_queue[i*op.K:(i+1)*op.K]=E_i\n",
    "        \n",
    "            \n",
    "t=time.time()\n",
    "fill_queue(tesRNN)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00aaa411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.40522 4096\n",
      "-0.40542 +/- 7e-05\n"
     ]
    }
   ],
   "source": [
    "def errformat(m,s):\n",
    "    exp = -int(np.floor(np.log(s)/np.log(10)))\n",
    "    print( str(round(m,exp))+\" +/- \"+str(round(s,exp)))\n",
    "\n",
    "var,mean = torch.var_mean(E_queue/op.L)\n",
    "\n",
    "print(h.ground(),op.B)\n",
    "stdv=((var/op.B)**0.5).item()\n",
    "errformat(mean.item(),stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6635f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample,lp = tesRNN.testsample(op.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7648ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp2=tesRNN.logprobability(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a8654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=(lp-lp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1e29f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13fa96e8e48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(diff.reshape(16,16).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38a58c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(diff).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51edacec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.0272e-04, -1.0681e-04,  3.0518e-05,  6.2943e-04, -8.0109e-05,\n",
      "        -9.3842e-04, -4.4250e-04, -8.8882e-04, -3.8719e-04, -3.7766e-04],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(diff[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd4d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_train2(op,to=None):\n",
    "  try:\n",
    "\n",
    "    mydir = setup_dir(op)\n",
    "    \n",
    "    if type(to)==type(None):\n",
    "        trainrnn,optimizer=new_rnn_with_optim(\"GRU\",op.Nh,lr=op.lr)\n",
    "    else:\n",
    "        trainrnn,optimizer=to\n",
    "    # Hamiltonian parameters\n",
    "    N = op.L   # Total number of atoms\n",
    "    V = 7.0     # Strength of Van der Waals interaction\n",
    "    Omega = 1.0 # Rabi frequency\n",
    "    delta = 1.0 # Detuning \n",
    "\n",
    "    if op.hamiltonian==\"Rydberg\":\n",
    "        Lx=Ly=int(op.L**0.5)\n",
    "        op.L=Lx*Ly\n",
    "        h = Rydberg(Lx,Ly,V,Omega,delta)\n",
    "    else:\n",
    "        #hope for the best here since there aren't defaults\n",
    "        h = TFIM(op.L,op.h,op.J)\n",
    "    exact_energy = h.ground()\n",
    "    print(exact_energy,op.L)\n",
    "\n",
    "    debug=[]\n",
    "    losses=[]\n",
    "    true_energies=[]\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "    i=0\n",
    "    t=time.time()\n",
    "    for x in range(op.steps):\n",
    "        \n",
    "        #gather samples\n",
    "        sample,lp = trainrnn.testsample(op.K)\n",
    "        \n",
    "        logp=lp\n",
    "        #obtain energy\n",
    "        with torch.no_grad():\n",
    "            _,probs= trainrnn.testlabels(sample,op.K)\n",
    "            E=h.localenergy(sample,lp,probs)\n",
    "            #energy mean and variance\n",
    "            Ev,Eo=torch.var_mean(E)\n",
    "\n",
    "        ERR  = Eo/(op.L)\n",
    "        \n",
    "        \n",
    "        if op.B==1:\n",
    "            loss = ((E-op.kl)*logp).mean()\n",
    "        else:\n",
    "            #loss = (E*logp - Eo*logp).mean()\n",
    "            loss = (E*logp - (Eo+op.kl)*logp).mean()\n",
    "\n",
    "        #Main loss curve to follow\n",
    "        losses.append(ERR.cpu().item())\n",
    "\n",
    "        #update weights\n",
    "        trainrnn.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # many repeat values but it keeps the same format as no queue\n",
    "        debug+=[[Ev.item()**0.5,Eo.item(),Ev.item()**0.5,Eo.item(),loss.item(),Eo.item(),0,time.time()-t]]\n",
    "\n",
    "        if x%500==0:\n",
    "            print(int(time.time()-t),end=\",%.3f|\"%(losses[-1]))\n",
    "            if x%4000==0:print()\n",
    "    print(time.time()-t,x+1)\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "    import os\n",
    "    DEBUG = np.array(debug)\n",
    "    \n",
    "\n",
    "    if op.dir!=\"<NONE>\":\n",
    "        np.save(mydir+\"/DEBUG\",DEBUG)\n",
    "        #print(DEBUG[-1][3]/Lx/Ly-exact_energy,DEBUG[-1][3]/Lx/Ly,DEBUG[-1][1]/Lx/Ly,exact_energy)\n",
    "        trainrnn.save(mydir+\"/T\")\n",
    "        #torch.save(samplernn,mydir+\"/S\")\n",
    "        \n",
    "  except KeyboardInterrupt:\n",
    "    if op.dir!=\"<NONE>\":\n",
    "        import os\n",
    "        DEBUG = np.array(debug)\n",
    "        np.save(mydir+\"/DEBUG\",DEBUG)\n",
    "        #print(DEBUG[-1][3]/Lx/Ly-exact_energy,DEBUG[-1][3]/Lx/Ly,DEBUG[-1][1]/Lx/Ly,exact_energy)\n",
    "        trainrnn.save(mydir+\"/T\")\n",
    "        #torch.save(samplernn,mydir+\"/S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31bf85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "op.hamiltonian=\"Rydberg\"\n",
    "#op.steps=4000\n",
    "op.L=64\n",
    "op.B=op.K=512\n",
    "op.dir=\"TESTRNN\"\n",
    "op.steps=12000\n",
    "op.NLOOPS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ee6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesRNN = torch.jit.script(testRNN(64,Nh=op.Nh))\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "tesRNN.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a47222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder path established\n",
      "-0.40522 64\n",
      "1,2.577|\n",
      "545,-0.312|1090,-0.385|1636,-0.390|2183,-0.391|2727,-0.394|3269,-0.395|3811,-0.396|4355,-0.401|\n",
      "4898,-0.402|5444,-0.388|5993,-0.397|6539,-0.402|7085,-0.404|7629,-0.396|8174,-0.405|8719,-0.404|\n",
      "9264,-0.405|"
     ]
    }
   ],
   "source": [
    "reg_train2(op,(tesRNN,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816db9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def genpatch2onehot(patch,p):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(p):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=2**p)\n",
    "\n",
    "\n",
    "\n",
    "class PatchedRNN(Sampler):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    def __init__(self,L,p,_2D=False,rnntype=\"GRU\",Nh=128,device=device, **kwargs):\n",
    "        super(PatchedRNN, self).__init__(device=device)\n",
    "        \n",
    "        if _2D:\n",
    "            self.patch=Patch2D(p,L)\n",
    "            self.L = (L**2//p**2)\n",
    "            self.p=p**2\n",
    "        else:\n",
    "            self.patch=Patch1D(p,L)\n",
    "            self.L = (L//p)\n",
    "            self.p = p\n",
    "        \n",
    "        assert rnntype!=\"LSTM\"\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=self.p,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,2**self.p),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "        self.Nh=Nh\n",
    "        self.rnntype=rnntype\n",
    "        \n",
    "        \n",
    "        self.options=torch.zeros([2**self.p,self.p],device=self.device)\n",
    "        tmp=torch.arange(2**self.p,device=self.device)\n",
    "        for i in range(self.p):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "            \n",
    "        \n",
    "        self.to(device)\n",
    "    def forward(self, input):\n",
    "        # h0 is shape [d*numlayers,B,H] but D=numlayers=1 so\n",
    "        # h0 has shape [1,B,H]\n",
    "        \n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h0=[torch.zeros([1,input.shape[0],self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,input.shape[0],self.Nh],device=self.device)]\n",
    "            #h0 and c0\n",
    "        #else:\n",
    "        h0=torch.zeros([1,input.shape[0],self.Nh],device=self.device)\n",
    "        out,h=self.rnn(input,h0)\n",
    "        return self.lin(out)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "                \n",
    "        #shape is modified to [B,L//4,4]\n",
    "        input = self.patch(input.squeeze(-1))\n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        #batch first\n",
    "        data[:,1:]=input[:,:-1]\n",
    "        # [B,L//4,Nh] -> [B,L//4,16]\n",
    "        output = self.forward(data)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [B,L//4,16]\n",
    "        real=genpatch2onehot(input,self.p)\n",
    "        \n",
    "        #[B,L//4,16] -> [B,L//4]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[B,L//4] -> [B]\n",
    "        logp=torch.sum(torch.log(total),dim=1)\n",
    "        return logp\n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//self.p\n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h=[torch.zeros([1,B,self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,B,self.Nh],device=self.device)]\n",
    "            #h is h0 and c0\n",
    "        #else:\n",
    "        h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "        #Sample set will have shape [B,L,p]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([B,L+1,self.p],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #out should be batch first [B,L,Nh]\n",
    "            out,h=self.rnn(input[:,idx-1:idx,:],h)\n",
    "            #check out the probability of all 2**p vectors\n",
    "            probs=self.lin(out[:,0,:]).view([B,2**self.p])\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            input[:,idx] = sample\n",
    "        #remove the leading zero in the input    \n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return self.patch.reverse(input[:,1:]).unsqueeze(-1)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = self.patch(sample.squeeze(-1))\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//self.p,self.p],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//self.p):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(self.p):\n",
    "                sflip[:,j*self.p+j2] = sample*1.0\n",
    "                sflip[:,j*self.p+j2,j,j2] = 1-sflip[:,j*self.p+j2,j,j2]\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            \n",
    "            data[:,1:]=sample[:,:-1]\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            \n",
    "            h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "            \n",
    "            out,_=self.rnn(data,h)\n",
    "            \n",
    "            #cache for the rnn is the output in this sense\n",
    "            #shape [B,L//4,Nh]\n",
    "            cache=out\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(1)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,L//D,1,1).reshape(B*L//D,L//self.p,cache.shape[-1])\n",
    "                        \n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [B,L//4,16]\n",
    "            real=genpatch2onehot(sample,self.p)\n",
    "            #[B,L//4,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//self.p,self.p])\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[:,1:]=tmp[:,:-1]\n",
    "                #grab your rnn output\n",
    "                if k==0:\n",
    "                    out,_=self.rnn(fsample,cache[:,0].unsqueeze(0)*0.0)\n",
    "                else:\n",
    "                    out,_=self.rnn(fsample[:,N//self.p:],cache[:,N//self.p-1].unsqueeze(0)*1.0)\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out)\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//self.p,2**self.p])\n",
    "                real = genpatch2onehot(real[:,:,N//self.p:],self.p)\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//self.p]),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
