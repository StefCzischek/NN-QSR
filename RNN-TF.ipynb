{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a3fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69aa2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTransformer(Sampler):\n",
    "    def __init__(self,Lx,Ly,device=device,Nh=128,decoder=False,dropout=0.0,num_layers=3, **kwargs):\n",
    "        super(RNNTransformer, self).__init__(device=device)\n",
    "                \n",
    "        if decoder:\n",
    "            #Decoder only transformer\n",
    "            self.decoder_layer = nn.TransformerDecoderLayer(d_model=Nh, nhead=8, dropout=dropout)\n",
    "            self.transformer = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        else:\n",
    "            #Encoder only transformer\n",
    "            #misinterperetation on encoder made it so this code does not work\n",
    "            self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=8, dropout=dropout)\n",
    "            self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        self.Nh=Nh\n",
    "        #rnn to encode the input for the transformer\n",
    "        self.rnn = nn.GRU(input_size=1,hidden_size=Nh,batch_first=False)\n",
    "        \n",
    "        self.set_mask(Lx*Ly)\n",
    "        self.to(device)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.num_weights = sum (p.numel () for p in self.parameters ())\n",
    "        print(\"Initializing. . .\",self.num_weights)\n",
    "        stdv=10/np.sqrt(self.num_weights)\n",
    "        for weight in self.parameters():\n",
    "            nn.init.normal_(weight, mean=0.0,std= stdv)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]!=self.L:\n",
    "            self.set_mask(input.shape[1])\n",
    "        h0=torch.zeros([1,input.shape[0],self.Nh],device=self.device)\n",
    "        #using an rnn to (positionally) encode the input sequence\n",
    "        encoded,h=self.rnn(input.transpose(1,0),h0)\n",
    "        #apply self attention\n",
    "        output = self.transformer(encoded,self.mask)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        #Input should have shape [B,L,1]\n",
    "        B,L,one=input.shape\n",
    "        \n",
    "        #first prediction is with the zero input vector\n",
    "        data=torch.zeros([B,L,one],device=self.device)\n",
    "        #data is the input vector shifted one to the right, with the very first entry set to zero instead of using pbc\n",
    "        data[:,1:,:]=input[:,:-1,:]\n",
    "        #real is going to be a set of actual values\n",
    "        real=input\n",
    "        #and pred is going to be a set of probabilities\n",
    "        #if real[i]=1 than you multiply your conditional probability by pred[i]\n",
    "        #if real[i]=0 than you multiply by 1-pred[i]\n",
    "        pred = self.forward(data)\n",
    "        ones = real*pred\n",
    "        zeros=(1-real)*(1-pred)\n",
    "        total = ones+zeros\n",
    "        #this is the sum you see in the cell above\n",
    "        #add 1e-10 to the prediction to avoid nans when total=0\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp\n",
    "\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1): #_off_diag_labels\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\"\"\"\n",
    "        sflip = torch.zeros([B,L,L,1],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L):\n",
    "            #get all of the states with one spin flipped\n",
    "            sflip[:,j] = sample*1.0\n",
    "            sflip[:,j,j] = 1-sflip[:,j,j]\n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            #prepare sample to be used as cache\n",
    "            B,L,one=sample.shape\n",
    "            dsample=torch.zeros([B,L,one],device=self.device)\n",
    "            dsample[:,1:,:]=sample[:,:-1,:]\n",
    "\n",
    "            h_00 = torch.zeros([1,B,self.Nh],device=self.device)\n",
    "            encoded,_ = self.rnn(dsample.transpose(1,0),h_00) \n",
    "            \n",
    "            hstates=torch.zeros([D,1,B,self.Nh],device=self.device)\n",
    "            #get the hidden states at each batched point of the sequence\n",
    "            hstates[1:,0]=encoded[L//D-1:-L//D:L//D]\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.make_cache(encoded)\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L,B*L//D,cache.shape[-1])\n",
    "            #repeat the same way for hstates\n",
    "            hstates = hstates.repeat(1,L//D,1,1).transpose(1,2).reshape(D,B*L//D,self.Nh)\n",
    "            \n",
    "            encoded=encoded.unsqueeze(1).repeat(1,L//D,1,1).transpose(2,1).reshape(L,B*L//D,self.Nh)\n",
    "            \n",
    "            pred0 = self.lin(out.transpose(1,0))\n",
    "            ones = sample*pred0\n",
    "            zeros=(1-sample)*(1-pred0)\n",
    "            total0 = ones+zeros\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #this is slightly wasteful because you can just look at the tensors from N forward\n",
    "                #flatten it out \n",
    "                tmp = real.reshape([B*L//D,L,1])\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[:,1:,:]=tmp[:,:-1,:]\n",
    "                \n",
    "                new_encode,_=self.rnn(fsample[:,N:].transpose(1,0),hstates[k:k+1])\n",
    "                tgt = torch.cat([encoded[:N], new_encode], dim=0)\n",
    "                #tgt,_ = self.rnn(fsample.transpose(1,0),hstates[0:1])\n",
    "                                \n",
    "                #grab your transformer output\n",
    "                out,_=self.next_with_cache(tgt,cache[:,:N],N)\n",
    "\n",
    "                # self.lin actually does some repeated work but it's probably\n",
    "                # negligable compared to the time attention takes\n",
    "                output = self.lin(out[N:].transpose(1,0))\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,L-N,1])\n",
    "                real=real[:,:,N:]\n",
    "                ones = real*pred\n",
    "                zeros=(1-real)*(1-pred)\n",
    "                total = ones+zeros\n",
    "                #sum across the sequence for probabilities\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=2).squeeze(2)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N]+1e-10),dim=1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample,probs\n",
    "    def next_attn(_,tgt,layer,i=-1):\n",
    "        \"\"\"Calculates self attention with tgt and the last elem of tgt\n",
    "        Inputs: \n",
    "            tgt - Tensor of shape [L+1,B,1]\n",
    "            layer - TransformerDecoderLayer\n",
    "            i - index of the first bit we want self-attention from\n",
    "        Outputs:\n",
    "            Tensor of shape [1,B,1]\n",
    "        \"\"\"\n",
    "        src = tgt[i:, :, :]\n",
    "        mask = None if i==-1 else _.mask[i:]\n",
    "        # self attention part\n",
    "        src2 = layer.self_attn(\n",
    "            src,#only do attention with the last elem of the sequence\n",
    "            tgt,\n",
    "            tgt,\n",
    "            attn_mask=mask,  # not needed because we only care about the last token\n",
    "            key_padding_mask=None,\n",
    "        )[0]\n",
    "        #straight from torch transformer encoder code\n",
    "        src = src + layer.dropout1(src2)\n",
    "        src = layer.norm1(src)\n",
    "        src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "        src = src + layer.dropout2(src2)\n",
    "        src = layer.norm2(src)\n",
    "        return src\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,1]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,1]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache\n",
    "    \n",
    "    def set_mask(self, L):\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones(L,L))).to(device)\n",
    "    \n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        #transformers don't do batch first so to save a bunch of transpose calls \n",
    "        input = torch.zeros([L+1,B,1],device=self.device)\n",
    "        #self.set_mask(L)\n",
    "        h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "        #using an rnn to (positionally) encode the input sequence\n",
    "        encoded_input=torch.zeros([L,B,self.Nh],device=self.device)\n",
    "        \n",
    "        cache=None\n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #run the rnn on shape [B,1,1]   \n",
    "            #append new encoding from the rnn\n",
    "            encoded_input[idx-1:idx],h = self.rnn(input[idx-1:idx],h)\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input[:idx],cache)\n",
    "            #if probs[i]=1 then there should be a 100% chance that sample[i]=1\n",
    "            #if probs[i]=0 then there should be a 0% chance that sample[i]=1\n",
    "            #stands that we generate a random uniform u and take int(u<probs) as our sample\n",
    "            probs=self.lin(output[-1,:,:])\n",
    "            sample = (torch.rand([B,1],device=device)<probs).to(torch.float32)\n",
    "            input[idx,:,:]=sample\n",
    "        #input's first entry is zero to get a predction for the first atom\n",
    "        #print(\".\",end=\"\")\n",
    "        return input.transpose(1,0)[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8773ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t64\n",
      "Q                             \t\t\t1\n",
      "K                             \t\t\t32\n",
      "B                             \t\t\t32\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.96875\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t1\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tout\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Settings\n",
    "op=Opt()\n",
    "Lx=8\n",
    "op.L=Lx*Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.Q=1\n",
    "op.K=32\n",
    "op.USEQUEUE=0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c92289",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsformer = RNNTransformer(Lx,Lx,Nh=op.Nh,num_layers=1)\n",
    "sampleformer= RNNTransformer(Lx,Lx,Nh=op.Nh,num_layers=1)\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96afbc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012640096247196198 1.080641587473709\n",
      "tensor(0.0008, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b60b81b408>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B=32\n",
    "\n",
    "s = trainsformer.sample(B,op.L)\n",
    "\n",
    "probs = super(RNNTransformer,trainsformer)._off_diag_labels(s,B,op.L,False,D=1)[1]\n",
    "\n",
    "p2 = trainsformer._off_diag_labels(s,B,op.L,False,D=8)[1]\n",
    "\n",
    "print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "print(abs(probs-p2).max())\n",
    "plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd1ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.40522 64\n",
      "0,2.51|48,-0.31|96,-0.34|144,-0.38|191,-0.39|239,-0.39|287,-0.38|335,-0.38|383,-0.39|431,-0.39|480,-0.39|532,-0.39|585,-0.40|638,-0.39|691,-0.40|743,-0.40|796,-0.39|849,-0.39|901,-0.39|954,-0.39|1007,-0.39|1059,-0.39|1112,-0.40|1164,-0.39|1212.0738739967346 12000\n"
     ]
    }
   ],
   "source": [
    "op.dir=\"HYBRID\"\n",
    "#op.steps=100\n",
    "op.NLOOPS=16\n",
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f22180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb1f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
