{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30b9c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02694a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PE2D(nn.Module):\n",
    "    def __init__(self, d_model, Lx,Ly,device,n_encode=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(Lx*Ly, d_model)\n",
    "        \n",
    "        if type(n_encode)==type(None):\n",
    "            n_encode=3*d_model//4\n",
    "        for pos in range(Lx*Ly):\n",
    "            x=pos//Ly\n",
    "            y=pos%Ly\n",
    "            # Only going to fill 3/4 of the matrix so the\n",
    "            # occupation values are preserved\n",
    "            for i in range(0, n_encode, 4):\n",
    "                \n",
    "                #x direction encoding\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(x / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(x / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                #y direction encoding\n",
    "                pe[pos, i+2] = \\\n",
    "                math.sin(y / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 3] = \\\n",
    "                math.cos(y / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(0).to(device)\n",
    "        self.L=Lx*Ly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:,:self.L,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406b661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowTransformer(Sampler):\n",
    "    def __init__(self,Lx,Ly,device=device,Nh=128,decoder=False,dropout=0.0,num_layers=3, **kwargs):\n",
    "        super(SlowTransformer, self).__init__(device=device)\n",
    "        \n",
    "        self.pe = PE2D(Nh, Lx,Ly,device)\n",
    "        \n",
    "        if decoder:\n",
    "            #Decoder only transformer\n",
    "            self.decoder_layer = nn.TransformerDecoderLayer(d_model=Nh, nhead=8, dropout=dropout)\n",
    "            self.transformer = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        else:\n",
    "            #Encoder only transformer\n",
    "            #misinterperetation on encoder made it so this code does not work\n",
    "            self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=8, dropout=dropout)\n",
    "            self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.set_mask(Lx*Ly)\n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones(L,L))).to(device)\n",
    "        self.pe.L=L\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]!=self.L:\n",
    "            self.set_mask(input.shape[1])\n",
    "        \n",
    "        input=self.pe(input).transpose(1,0)\n",
    "        output = self.transformer(input,self.mask)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        #Input should have shape [B,L,1]\n",
    "        B,L,one=input.shape\n",
    "        \n",
    "        #first prediction is with the zero input vector\n",
    "        data=torch.zeros([B,L,one],device=self.device)\n",
    "        #data is the input vector shifted one to the right, with the very first entry set to zero instead of using pbc\n",
    "        data[:,1:,:]=input[:,:-1,:]\n",
    "        \n",
    "        #real is going to be a set of actual values\n",
    "        real=input\n",
    "        #and pred is going to be a set of probabilities\n",
    "        #if real[i]=1 than you multiply your conditional probability by pred[i]\n",
    "        #if real[i]=0 than you multiply by 1-pred[i]\n",
    "        \n",
    "        #probability predictions may be done WITH gradients\n",
    "        #with torch.no_grad():\n",
    "        \n",
    "        pred = self.forward(data)\n",
    "        ones = real*pred\n",
    "        zeros=(1-real)*(1-pred)\n",
    "        total = ones+zeros\n",
    "        #this is the sum you see in the cell above\n",
    "        #add 1e-10 to the prediction to avoid nans when total=0\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp\n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        \n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [N,L+1,1] \n",
    "        input = torch.zeros([B,L+1,1],device=self.device)\n",
    "        \n",
    "        self.set_mask(L)\n",
    "        #sampling can be done without gradients\n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #run the rnn on shape [B,1,1]   \n",
    "            #encode the input to the proper shape\n",
    "            encoded_input = input[:,:idx,:]+self.pe.pe[:,:idx,:]\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output = self.transformer(encoded_input.transpose(1,0),self.mask[:idx,:idx])\n",
    "            #if probs[i]=1 then there should be a 100% chance that sample[i]=1\n",
    "            #if probs[i]=0 then there should be a 0% chance that sample[i]=1\n",
    "            #stands that we generate a random uniform u and take int(u<probs) as our sample\n",
    "            probs=self.lin(output.transpose(1,0)[:,-1,:])\n",
    "            sample = (torch.rand([B,1],device=device)<probs).to(torch.float32)\n",
    "            input[:,idx,:]=sample\n",
    "        #input's first entry is zero to get a predction for the first atom\n",
    "        return input[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7403eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t64\n",
      "Q                             \t\t\t1\n",
      "K                             \t\t\t32\n",
      "B                             \t\t\t32\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.96875\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t1\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tout\n",
      "Nh                            \t\t\t64\n",
      "lr                            \t\t\t0.0005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Settings\n",
    "op=Opt()\n",
    "Lx=8\n",
    "op.L=Lx*Lx\n",
    "op.Nh=64\n",
    "op.lr=5e-4\n",
    "op.Q=1\n",
    "op.K=32\n",
    "op.USEQUEUE=0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9a845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsformer = SlowTransformer(Lx,Lx,Nh=op.Nh,num_layers=2)\n",
    "sampleformer= SlowTransformer(Lx,Lx,Nh=op.Nh,num_layers=2)\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1119cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847681\n"
     ]
    }
   ],
   "source": [
    "print(sum (p.numel () for p in trainsformer.parameters ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55984674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847774\n",
      "Output folder path established\n",
      "-0.40522 64\n",
      "1,2.10|45,-0.31|89,-0.33|134,-0.38|178,-0.40|223,-0.39|267,-0.40|312,-0.39|356,-0.39|400,-0.40|445,-0.40|489,-0.40|533,-0.41|578,-0.40|622,-0.40|667,-0.40|711,-0.40|756,-0.40|800,-0.40|844,-0.41|889,-0.41|933,-0.41|978,-0.40|1022,-0.41|1067.2162992954254 12000\n"
     ]
    }
   ],
   "source": [
    "# train a gru\n",
    "op.dir=\"GRU\"\n",
    "trainrnn,optimizer=new_rnn_with_optim(\"GRU\",459,lr=op.lr)\n",
    "print(sum (p.numel () for p in trainrnn.parameters ()))\n",
    "reg_train(op,(trainrnn,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b7dcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413121\n",
      "Output folder path established\n",
      "-0.45776822 16\n",
      "0,2.01|5,-0.38|10,-0.42|15,-0.44|20,-0.45|25,-0.45|31,-0.45|36,-0.45|41,-0.46|46,-0.45|51,-0.46|56,-0.45|61,-0.45|66,-0.46|71,-0.46|76,-0.46|82,-0.46|87,-0.44|92,-0.45|97,-0.46|102,-0.46|107,-0.45|112,-0.45|117,-0.46|122.32494974136353 12000\n"
     ]
    }
   ],
   "source": [
    "Lx=4\n",
    "op.L=Lx*Lx\n",
    "op.dir=\"GRU\"\n",
    "trainrnn,optimizer=new_rnn_with_optim(\"GRU\",320,lr=op.lr)\n",
    "print(sum (p.numel () for p in trainrnn.parameters ()))\n",
    "reg_train(op,(trainrnn,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ea5f78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.40522 64\n",
      "0,2.38|133,-0.31|267,-0.30|400,-0.35|536,-0.37|670,-0.40|804,-0.41|938,-0.41|1072,-0.41|1206,-0.40|1340,-0.41|1474,-0.41|1609,-0.41|1743,-0.41|1877,-0.41|2011,-0.41|2146,-0.41|2285,-0.42|2426,-0.41|2560,-0.40|2695,-0.41|2829,-0.41|2963,-0.41|3097,-0.41|3231.5833308696747 12000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#op.steps=4000\n",
    "op.dir=\"TF\"\n",
    "\n",
    "\n",
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d524bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [ 2.,  2.,  2.,  2.],\n",
       "         [ 3.,  3.,  3.,  3.],\n",
       "         [ 4.,  4.,  4.,  4.]],\n",
       "\n",
       "        [[ 5.,  5.,  5.,  5.],\n",
       "         [ 6.,  6.,  6.,  6.],\n",
       "         [ 7.,  7.,  7.,  7.],\n",
       "         [ 8.,  8.,  8.,  8.]],\n",
       "\n",
       "        [[ 9.,  9.,  9.,  9.],\n",
       "         [10., 10., 10., 10.],\n",
       "         [11., 11., 11., 11.],\n",
       "         [12., 12., 12., 12.]],\n",
       "\n",
       "        [[13., 13., 13., 13.],\n",
       "         [14., 14., 14., 14.],\n",
       "         [15., 15., 15., 15.],\n",
       "         [16., 16., 16., 16.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4*4).reshape([4,4,1]) + torch.ones([1,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec01ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 40]) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "pe = PE2D(40,6,6,torch.device('cpu')).pe\n",
    "print(pe.shape,pe.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2714445c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ff48262d88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARMAAAD5CAYAAAAEAdHoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARpUlEQVR4nO3df4yV1Z3H8c+X6TA4I4kzq1ICBH+RzRhX0UzQxKXp1tpF0wRc2kZJLH8YsUnd7HRtssRNtnb/0W5QI/9IcCXFxlpM1WAau+uEbGPcNNRRcMTF1kGgnTI7KNQIRhi5890/7kN2Cudh7p37fS73zn2/ksm99zvPPec8wX76zD33nMfcXQBQq1nnewAAZgbCBEAIwgRACMIEQAjCBEAIwgRAiC/U8mYzWyHpCUltkv7d3R851/FtXV3+hZ6es+p/1fNh8vh3jl6SrDf78Y04Jo6PPX6672kG4yMjH7n7WSdh0/2eiZm1SfqdpFsljUh6Q9Jd7v4/ee/pWLTIF3yv/6z68JpNyeOv+ul3kvVmP74Rx8TxscdP9z3NYP8D33/T3fvOrNfyZ84yScPu/oG7j0v6maSVNbQHoInVEiYLJP1h0uuRrPZnzGydmQ2a2WDp009r6A5AI6slTCxRO+tvJnff7O597t7X1tVVQ3cAGlktYTIiadGk1wslHaptOACaVS2zOW9IWmJml0v6o6Q7Ja0JGVVm2+qNyfrQ+ERI+6WumHZypa7dgBlq2mHi7qfM7H5J/6ny1PAWd383bGQAmkpN3zNx91ckvRI0FgBNjG/AAghBmAAIQZgACFHTZyZFm9c2nqz3H1wV0r51ngppJ48zm4MWwpUJgBCECYAQhAmAEIQJgBCECYAQDT2bs3ygP1lvH2tPv2F2dRs9dXR+nqyXPGjNDrM5aCFcmQAIQZgACEGYAAhBmAAIQZgACNHQszm9Dx9J1k8uPvveO5J08LacWZ4ccztPJOsTZ29lOz3M5qCFcGUCIARhAiAEYQIgBGECIARhAiBETbM5ZnZA0jFJJUmnUjczrkVpeH+yPntkNP2G25ZW1X73nM+S9QnFrM1hpzW0koip4b9x948C2gHQxPgzB0CIWsPEJb1qZm+a2bqIAQFoTrX+mXOzux8ys0slDZjZe+7+2uQDspBZJ0lt3d01dgegUdV0ZeLuh7LHw5JekrQsccxmd+9z9762rq5augPQwKZ9ZWJmXZJmufux7PnXJP3rud5zRfdhPbN641n1ofH07Mm+DTcl68NrNiXrl2+/9pxjPqv9XQuT9Y7enDU+Vc7OzEpv5AbMSLX8mTNP0ktmdrqdn7r7f4SMCkDTmXaYuPsHkq4LHAuAJsbUMIAQhAmAEIQJgBANvdPatsTMj5Q/+1OtUlfQ/XHysDYHLYQrEwAhCBMAIQgTACEIEwAhCBMAIRp6Nmde23iy3n9wVUj71nkqpJ087LSGVsKVCYAQhAmAEIQJgBCECYAQhAmAEA09m7N8oD9Zbx/L2Qmtu1RV+x2d6a3QSh60ZofZHLQQrkwAhCBMAIQgTACEIEwAhCBMAIQgTACEmHJq2My2SPq6pMPufk1W65G0TdJlkg5I+pa7/yl6cL0PH0nWTy7uSdY/+EZbVe3P7TyRrE/Iq2onF1PDaCGVXJn8WNKKM2rrJe1w9yWSdmSvAbSwKcMkuxH50TPKKyVtzZ5vlbQqdlgAms10PzOZ5+6jkpQ9Xpp3oJmtM7NBMxv8+Gh131AF0DwK/wDW3Te7e5+7913UU91nGgCax3TDZMzM5ktS9ng4bkgAmtF0F/q9LGmtpEeyx+1hI5qkNLw/WZ89Mpp+wzeurar97jmfJesTilnox7aNaCVTXpmY2XOSfi3pL81sxMzuUTlEbjWz9yXdmr0G0MKmvDJx97tyfnVL8FgANDG+AQsgBGECIARhAiBEXbdtbNdE8sZaeTfV2rfhqmR9eM2mZP2KgaurGs++XQuT9Y7e9LaQ1c7OzErvCgnMSFyZAAhBmAAIQZgACEGYAAhBmAAI0dA34dq2emOyPjQes3am1BV0s608rM1BC+HKBEAIwgRACMIEQAjCBEAIwgRAiIaezUmt45Hy1/JUyzpPhbSTh53W0Eq4MgEQgjABEIIwARCCMAEQgjABEGLK2Rwz2yLp65IOu/s1We0hSfdK+jA77EF3fyV6cMsH+pP19rH0TmhacLKq9js601uhlTxozQ6zOWghlVyZ/FjSikT9cXdfmv2EBwmA5jJlmLj7a5KO1mEsAJpYLZ+Z3G9mQ2a2xcy68w4ys3VmNmhmg0ePFrzkH8B5M90weVLSlZKWShqV9Gjege6+2d373L2vp4fPe4GZalr/63b3MXcvufuEpKckLYsdFoBmM621OWY2391Hs5d3SNoTN6T/1/vwkWT95OKeZH3/t6trf27niWR9Ql5dQ3mYzUELqWRq+DlJX5Z0sZmNSPqBpC+b2VJJLumApPuKGyKAZjBlmLj7XYny0wWMBUAT4xNRACEIEwAhCBMAIRp6p7XS8P5kffbIaLKub19dVfvdcz5L1icU8+U6dlpDK+HKBEAIwgRACMIEQAjCBEAIwgRAiLrO5vz2k3laPvD3Z9Xzdk6b2HBVsj68ZlOy3vvfS5L1vJ3T9u1amKx39Obs5Fbl7Mys9EZuwIzElQmAEIQJgBCECYAQhAmAEIQJgBANvTZn2+qNyfrQeMzamVJXwRtcszYHLYQrEwAhCBMAIQgTACEIEwAhCBMAISq51cUiSc9I+qKkCUmb3f0JM+uRtE3SZSrf7uJb7v6nyMHNaxtP1vsPrgpp3zpPhbSTh53W0EoquTI5JekBd++VdJOk75rZ1ZLWS9rh7ksk7cheA2hRU4aJu4+6+1vZ82OS9kpaIGmlpK3ZYVslrSpojACaQFWfmZjZZZKul7RT0rzTtwjNHi/Nec86Mxs0s8HS8U9rHC6ARlVxmJjZhZJekNTv7p9U+j533+zufe7e13Zh13TGCKAJVBQmZtaucpA86+4vZuUxM5uf/X6+pMPFDBFAM6hkNsdUvrfwXnd/bNKvXpa0VtIj2eP26MEtH+hP1vN2Zmtbcryq9js601uh5e3MVjVmc9BCKlnod7OkuyW9Y2a7s9qDKofI82Z2j6TfS/pmISME0BSmDBN3f135/x97S+xwADQrvgELIARhAiAEYQIgREPvtNb78JFk/eTinmT9UPq2Obnmdp5I1ifk1TWUh9kctBCuTACEIEwAhCBMAIQgTACEIEwAhGjo2ZzS8P5kffbIaPoN36luOqd7zmfJ+oRi1uaw0xpaCVcmAEIQJgBCECYAQhAmAEIQJgBC1HU2Z87/nkqut8lba7Nvw03J+vCaTcn6sl2LkvW8tTb7di1M1jt60zu5VbvWZlZ6IzdgRuLKBEAIwgRACMIEQAjCBEAIwgRAiErum7NI0jOSvihpQtJmd3/CzB6SdK+kD7NDH3T3VyIHt231xmR9aDxm7UypK+j+OHlYm4MWUsnU8ClJD7j7W2Y2V9KbZjaQ/e5xd99Q3PAANItK7pszKun0DcqPmdleSQuKHhiA5lLVZyZmdpmk6yXtzEr3m9mQmW0xs+6c96wzs0EzGxwvpZf8A2h+FYeJmV2o8s3L+939E0lPSrpS0lKVr1weTb3P3Te7e5+7981uu6D2EQNoSBWFiZm1qxwkz7r7i5Lk7mPuXnL3CUlPSVpW3DABNLpKZnNM0tOS9rr7Y5Pq87PPUyTpDkl7ogc3r208We8/uCqkfes8FdJOHnZaQyupZDbnZkl3S3rHzHZntQcl3WVmSyW5pAOS7itgfACaRCWzOa8r/Y2J0O+UAGhufAMWQAjCBEAIwgRAiIa+b87ygf5kvX0svRPaRdd9VFX7HZ3prdBKHrRmh9kctBCuTACEIEwAhCBMAIQgTACEIEwAhCBMAIRo6Knh1A27pPybdh27rrr253aeSNbzbtpVNaaG0UK4MgEQgjABEIIwARCCMAEQgjABEKKhZ3NKw/uT9dkjo8m6/nFRVe13z0nvlj+hmIV+bNuIVsKVCYAQhAmAEIQJgBCECYAQU4aJmc0xs9+Y2dtm9q6Z/TCr95jZgJm9nz0mbw8KoDVUMptzUtJX3P14dme/183sl5L+TtIOd3/EzNZLWi/pn87VkJ8cT87Q5M3OvL/hpmR9eM2mZP1v916crOfNzuzbtTBZ7+hNbwtZ7ezMrPSukMCMNOWViZcdz162Zz8uaaWkrVl9q6RVRQwQQHOo9F7Dbdnd/A5LGnD3nZLmnb49aPZ4aWGjBNDwKgqT7AblSyUtlLTMzK6ptAMzW2dmg2Y2+LlOTnOYABpdVbM57v6xpF9JWiFpzMzmS+WbmKt81ZJ6z2Z373P3vnZ11DZaAA2rktmcS8zsouz5BZK+Kuk9SS9LWpsdtlbS9oLGCKAJVDKbM1/SVjNrUzl8nnf3X5jZryU9b2b3SPq9pG9GD27b6o3J+tB4zNqZUlfQzbbysDYHLWTKMHH3IUnXJ+pHJN1SxKAANB++AQsgBGECIARhAiAEYQIgREPvtDavbTxZ7z+4KqR96zwV0k4edlpDK+HKBEAIwgRACMIEQAjCBEAIwgRAiIaezVk+0J+st4+ld0K7/MY/VNV+R2d6K7SSB63ZYTYHLYQrEwAhCBMAIQgTACEIEwAhCBMAIRp6Nqf34SPJ+snFPek33Fhd+3M7TyTrE/LqGsrDbA5aCFcmAEIQJgBCECYAQhAmAEIQJgBCTDmbY2ZzJL0mqSM7/ufu/gMze0jSvZI+zA590N1fiRxcaXh/sj57ZDRZn9DFVbXfPeeznHZi1uaw0xpaSSVTwyclfcXdj5tZu6TXzeyX2e8ed/cNxQ0PQLOo5CZcLul49rI9+wn6IgaAmaKiz0zMrM3Mdqt8c/IBd9+Z/ep+Mxsysy1m1p3z3nVmNmhmg5/rZMyoATScisLE3UvuvlTSQknLzOwaSU9KulLSUkmjkh7Nee9md+9z9752dYQMGkDjqWo2x90/lvQrSSvcfSwLmQlJT0laFj88AM3Cyh+JnOMAs0skfe7uH5vZBZJelfQjSW+6+2h2zPck3ejud07R1oeSDmYvL5b0UY3jn45W6/d89k2/M7Pfxe5+yZnFSmZz5kvaamZtKl/JPO/uvzCzn5jZUpU/jD0g6b6pGpo8ADMbdPe+CgcfptX6PZ990+/M7vdMlczmDEm6PlG/u5ARAWhKfAMWQIjzGSab6XfG902/M7vfPzPlB7AAUAn+zAEQgjABEKLuYWJmK8zst2Y2bGbr69z3ATN7x8x2m9lggf1sMbPDZrZnUq3HzAbM7P3sMbn8oIB+HzKzP2bnvNvMbi+g30Vm9l9mttfM3jWzf8jqhZ7zOfqtxznPMbPfmNnbWd8/zOpFn3Nev4Wf85TcvW4/ktok7ZN0haTZkt6WdHUd+z8g6eI69PMlSTdI2jOp9m+S1mfP10v6UZ36fUjS9ws+3/mSbsiez5X0O0lXF33O5+i3Hudski7MnrdL2inppjqcc16/hZ/zVD/1vjJZJmnY3T9w93FJP5O0ss5jKJy7vybp6BnllZK2Zs+3SlpVp34L5+6j7v5W9vyYpL2SFqjgcz5Hv4XzstRq+qLPOa/f867eYbJA0uS7i4+oTv/4GZf0qpm9aWbr6tivJM3zbPlB9nhpHfuecnV3FDO7TOUvOe5UHc/5jH6lOpxzzmr6ws+5llX8Rap3mKT2Hqtnqt7s7jdIuk3Sd83sS3Xs+3ypaHV3BDO7UNILkvrd/ZOi+qmg37qcs6dX0xcup9+6/TvnqXeYjEhaNOn1QkmH6tW5ux/KHg9Lekn1Xek8ZmbzJSl7PFyPTr1Oq7uzXfhekPSsu7+YlQs/51S/9Trn03zSanrV8d/ZG2wVf73D5A1JS8zscjObLelOSS/Xo2Mz6zKzuaefS/qapD3nfleolyWtzZ6vlbS9Hp2e/g87c4cKOGczM0lPS9rr7o9N+lWh55zXb53O+RIzuyh7foGkr0p6T8Wfc7LfepzzlOr9ia+k21X+1H2fpH+uY79XqDx79Lakd4vsW9JzKl9qfq7y1dg9kv5C0g5J72ePPXXq9yeS3pE0pPJ/6PML6PevVf5zdUjS7uzn9qLP+Rz91uOcr5W0K+tjj6R/yepFn3Nev4Wf81Q/fJ0eQAi+AQsgBGECIARhAiAEYQIgBGECIARhAiAEYQIgxP8BF3SvUTjVcd8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3780fac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([1,2,3]).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc274ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
