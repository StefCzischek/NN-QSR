{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b75c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "         [ 3,  4,  5,  9, 10, 11, 15, 16, 17],\n",
      "         [18, 19, 20, 24, 25, 26, 30, 31, 32],\n",
      "         [21, 22, 23, 27, 28, 29, 33, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *\n",
    "from Patched_TF import PE2D\n",
    "def patch2D(x,n,Lx):\n",
    "    # type: (Tensor,int,int) -> Tensor\n",
    "    \"\"\"patch your sequence into chunks of 4\"\"\"\n",
    "    #make the input 2D then break it into 2x2 chunks \n",
    "    #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "    return x.view([x.shape[0],Lx,Lx]).unfold(-2,n,n).unfold(-2,n,n).reshape([x.shape[0],int(Lx*Lx//n**2),int(n**2)])\n",
    "\n",
    "def unpatch2D(x,n,Lx):\n",
    "    # type: (Tensor,int,int) -> Tensor\n",
    "    \"\"\"inverse function for patch\"\"\"\n",
    "    # original sequence order can be retrieved by chunking twice more\n",
    "    #in the x-direction you should have chunks of size 2, but in y it should\n",
    "    #be chunks of size Ly//2\n",
    "    return x.unfold(-2,Lx//n,Lx//n).unfold(-2,n,n).reshape([x.shape[0],Lx*Lx])\n",
    "\n",
    "\n",
    "Lx=6\n",
    "a = torch.arange(Lx**2).unsqueeze(0)\n",
    "print(a)\n",
    "print(a.view([Lx,Lx]))\n",
    "b = patch2D(a,3,Lx)\n",
    "print(b)\n",
    "c = unpatch2D(b,3,Lx)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef655c",
   "metadata": {},
   "source": [
    "# The below functions are only for patches of size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def patch2idx(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return out.to(torch.int64)\n",
    "\n",
    "@torch.jit.script\n",
    "def patch2onehot(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a2d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTFRNN(Sampler):#(torch.jit.ScriptModule):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    \"\"\"\n",
    "    Base class for the two patch transformer architectures \n",
    "    \n",
    "    Architexture wise this is how a patched transformer works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each patch given it and all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    Outputs should either be size 1 (the probability of the current patch which is input) or size 16 (for 2x2 patches where \n",
    "    the probability represented is of each potential patch)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,px=4,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8,rnn_patch=4,rnntype=\"GRU\", **kwargs):\n",
    "        super(PTFRNN, self).__init__()\n",
    "        #print(nhead)\n",
    "        self.pe = PE2D(Nh, Lx,Lx,device)\n",
    "        self.device=device\n",
    "        #Encoder only transformer\n",
    "        #misinterperetation on encoder made it so this code does not work\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        assert rnn_patch==4\n",
    "        #have to do more work to implement other values but I may want to do so later\n",
    "        \n",
    "        assert rnntype!=\"LSTM\"\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=rnn_patch,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,2**rnn_patch),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "        #sequence size in x\n",
    "        self.Lx=Lx\n",
    "        #transformer patch size in x\n",
    "        self.px=px\n",
    "        #transformer patch size\n",
    "        self.p=px**2\n",
    "        #rnn patch size\n",
    "        self.prnn=rnn_patch\n",
    "        \n",
    "        self.set_mask(Lx**2//self.p)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "        \n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones([L,L],device=self.device)))\n",
    "        self.pe.L=L\n",
    "        \n",
    "    \n",
    "    def rnnforward(self,hidden,input):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            hidden - shape [L/p,B,Nh] tensor\n",
    "            input - shape [L/p,B,p] tensor\n",
    "        Outputs\n",
    "            out - shape [L/p,B,p/4,Nh] tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        Lp,B,Nh=hidden.shape\n",
    "        h0 = hidden.view([1,Lp*B,Nh])\n",
    "        input = patch2D(input.reshape([Lp*B,self.p]),2,self.px)\n",
    "        #probably need this?\n",
    "        input[:,1:]=input[:,:-1]\n",
    "        input[:,0]=0\n",
    "        \n",
    "        out,h=self.rnn(input,h0)\n",
    "        return out.view([Lp,B,self.p//4,Nh])\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]//self.p!=self.L:\n",
    "            self.set_mask(input.shape[1]//self.p)\n",
    "        \n",
    "        input=patch2D(input.squeeze(-1),self.px,self.Lx).transpose(1,0)\n",
    "        #pe should be sequence first [L/p,B,Nh]\n",
    "        hidden = self.transformer(self.pe(input),self.mask)        \n",
    "        rnnout = self.rnnforward(hidden,input)\n",
    "        \n",
    "        #[L/p,B,p/4,Nh] -> [B,L/4,16]\n",
    "        output = self.lin(rnnout).transpose(1,0).view([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "        return output\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        # type: (Tensor,Optional[Tensor],int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,1]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,1]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        #HMMM\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i,layer in enumerate(self.transformer.layers):\n",
    "            \n",
    "            tgt=output\n",
    "            #have to merge the functions into one\n",
    "            src = tgt[idx:, :, :]\n",
    "            mask = None if idx==-1 else self.mask[idx:]\n",
    "\n",
    "            # self attention part\n",
    "            src2 = layer.self_attn(\n",
    "                src,#only do attention with the last elem of the sequence\n",
    "                tgt,\n",
    "                tgt,\n",
    "                attn_mask=mask,  \n",
    "                key_padding_mask=None,\n",
    "            )[0]\n",
    "            #straight from torch transformer encoder code\n",
    "            src = src + layer.dropout1(src2)\n",
    "            src = layer.norm1(src)\n",
    "            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "            src = src + layer.dropout2(src2)\n",
    "            src = layer.norm2(src)\n",
    "            #return src\n",
    "            \n",
    "            output = src#self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        return self.sampleDebug(B,L,cache)[0]\n",
    "    @torch.jit.export\n",
    "    def sampleDebug(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tuple[Tensor,Tensor]\n",
    "\n",
    "\n",
    "        #length is divided by four due to patching\n",
    "        L=L//self.p\n",
    "        \n",
    "        DEBUG=torch.zeros([B],device=self.device)\n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([L+1,B,self.p],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            \n",
    "            h = output[-1,:,:].unsqueeze(0)\n",
    "            \n",
    "            rnnseq = torch.zeros([B,self.p//4+1,4],device=self.device)\n",
    "            for rdx in range(1,self.p//4+1):\n",
    "                \n",
    "                out,h=self.rnn(rnnseq[:,rdx-1:rdx,:],h)\n",
    "                #check out the probability of all 16 vectors\n",
    "                probs=self.lin(out).view([B,16])\n",
    "                #sample from the probability distribution\n",
    "                indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "                #extract samples\n",
    "                sample = self.options[indices]\n",
    "                #add sample to sequence\n",
    "                rnnseq[:,rdx] = sample\n",
    "                \n",
    "                #debugging info\n",
    "                real=patch2onehot(sample)\n",
    "                total = torch.sum(real*probs,dim=-1)\n",
    "                DEBUG+=torch.log(total)\n",
    "                \n",
    "            #set input to the (unpatched) rnn sequence\n",
    "            input[idx] = unpatch2D(rnnseq[:,1:],2,self.px)\n",
    "            \n",
    "        #remove the leading zero in the input    \n",
    "        input=input[1:]\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch2D(input.transpose(1,0),self.px,self.Lx).unsqueeze(-1),DEBUG\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "                \n",
    "        if input.shape[1]//self.p!=self.L:\n",
    "            self.set_mask(input.shape[1]//self.p)\n",
    "        \n",
    "        #shape is modified to [L//p,B,p]\n",
    "        input = patch2D(input.squeeze(-1),self.px,self.Lx).transpose(1,0)\n",
    "        \n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        data[1:]=input[:-1]\n",
    "        \n",
    "        #[L//p,B,p] -> [L//p,B,Nh]\n",
    "        encoded=self.pe(data)\n",
    "        # [L//p,B,Nh] -> [L//p,B,Nh]\n",
    "        hidden = self.transformer(encoded,self.mask)     \n",
    "        # [L//p,B,Nh],[L//p,B,p] -> [L/p,B,p/4,Nh]\n",
    "        rnnout = self.rnnforward(hidden,input)\n",
    "        \n",
    "        #[L/p,B,p/4,Nh] -> [B,L/4,16]\n",
    "        output = self.lin(rnnout).transpose(1,0).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [B,L//4,16]\n",
    "        \n",
    "        Lp,B,Nh=hidden.shape\n",
    "        \n",
    "        \n",
    "        \n",
    "        #reshaping the input to match the shape of the output (idk if this is correct tbh)\n",
    "        susman = patch2D(input.reshape([Lp*B,self.p]),2,self.px).view([Lp,B,self.p//4,4]).transpose(1,0)\n",
    "\n",
    "        \n",
    "        #print(susman[:,0])\n",
    "        \n",
    "        real=patch2onehot(susman).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "        \n",
    "        #print(real.shape,output.shape)\n",
    "        \n",
    "        #[B,L//4,16] -> [B,L//4]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[B,L//4] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1)\n",
    "        return logp\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch2D(sample.squeeze(-1),self.px,self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//self.p,self.p],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//self.p):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(self.p):\n",
    "                sflip[:,j*self.p+j2] = sample*1.0\n",
    "                sflip[:,j*self.p+j2,j,j2] = 1-sflip[:,j*self.p+j2,j,j2]\n",
    "            \n",
    "        #switch sample into sequence-first\n",
    "        sample = sample.transpose(1,0)\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            data[1:]=sample[:-1]\n",
    "            \n",
    "            #[L//4,B,4] -> [L//4,B,Nh]\n",
    "            encoded=self.pe(data)\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.make_cache(encoded)\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L//self.p,B*L//D,cache.shape[-1])\n",
    "\n",
    "            \n",
    "            \n",
    "            rnnout = self.rnnforward(out,sample)\n",
    "            #[L/p,B,p/4,Nh] -> [B,L/4,16]\n",
    "            pred0 = self.lin(rnnout).transpose(1,0).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "            #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "            #shape will be [B,L//4,16]\n",
    "            Lp,B,Nh=out.shape\n",
    "            #reshaping the input to match the shape of the output (idk if this is correct tbh)\n",
    "            susman = patch2D(sample.reshape([Lp*B,self.p]),2,self.px).view([Lp,B,self.p//4,4]).transpose(1,0)\n",
    "            real=patch2onehot(susman).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "\n",
    "            \n",
    "            #[B,L//4,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//self.p,self.p]).transpose(1,0)\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[1:]=tmp[:-1]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample)\n",
    "                #grab your transformer output\n",
    "                out,_=self.next_with_cache(tgt,cache[:,:N//self.p],N//self.p)\n",
    "                #only the newly computed parts are necessary\n",
    "                out = out [N//self.p:]\n",
    "                \n",
    "                rnnout = self.rnnforward(out,tmp[N//self.p:])\n",
    "                \n",
    "                Lp,Bp,Nh=out.shape\n",
    "                \n",
    "                \n",
    "                # grab output for the new part\n",
    "                output = self.lin(rnnout).transpose(1,0).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "                \n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                \n",
    "                susman = patch2D(tmp[N//self.p:].reshape([Lp*Bp,self.p]),2,self.px).view([Lp,Bp,self.p//4,4]).transpose(1,0)\n",
    "        \n",
    "                real=patch2onehot(susman).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "                \n",
    "                real = real.view([B,L//D,(L-N)//4,16])\n",
    "                \n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                \n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab439d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 64, 1])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "pb = PTFRNN(8)\n",
    "sample,p0 = pb.sampleDebug(6,8*8)\n",
    "\n",
    "print(sample.shape)\n",
    "\n",
    "ps = pb.logprobability(sample)\n",
    "\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0386ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tensor([-44.7609, -44.6124, -44.1222, -43.8909, -44.3229, -44.9748],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>) \n",
      " tensor([-44.7608, -44.6122, -44.1223, -43.8911, -44.3229, -44.9747],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"\",ps,'\\n',p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373cff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(px,Lx):\n",
    "    sflip = torch.arange(Lx*Lx,device=device).to(torch.int64).reshape([1,Lx,Lx])\n",
    "    sflip = patch2D(sflip,px,Lx).reshape(Lx*Lx)\n",
    "    \n",
    "    return sflip\n",
    "\n",
    "if False:\n",
    "    B=32\n",
    "\n",
    "    s = pb.sample(B,8*8)\n",
    "    probs = super(PTFRNN,pb)._off_diag_labels(s,B,8*8,False,D=4)[1][:,get_indices(4,8)]\n",
    "    \n",
    "    p2 = pb._off_diag_labels(s,B,8*8,False,D=4)[1]\n",
    "\n",
    "    print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "    print(probs.mean(),p2.mean())\n",
    "    print(abs(probs-p2).max())\n",
    "    plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4cd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t576\n",
      "Q                             \t\t\t1\n",
      "K                             \t\t\t256\n",
      "B                             \t\t\t256\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.9\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t36\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPTFRNN\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=24\n",
    "op.L=Lx*Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.M=0.9\n",
    "op.Q=1\n",
    "op.K=256\n",
    "op.USEQUEUE=0\n",
    "op.kl=0.0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "#op.steps=4000\n",
    "op.dir=\"PTFRNN\"\n",
    "#op.steps=100\n",
    "op.NLOOPS=36\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445c37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "trainsformer = torch.jit.script(PTFRNN(Lx,Nh=op.Nh))\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.3724 576\n",
      "1,2.79|636,-0.32|1268,-0.33|1900,-0.33|2561,-0.34|3196,-0.35|3830,-0.37|4464,-0.37|5098,-0.37|5735,-0.37|6371,-0.37|7421,-0.37|8634,-0.37|9869,-0.37|11073,-0.37|12012,-0.37|"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
