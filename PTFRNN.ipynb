{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b75c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Ti', major=8, minor=6, total_memory=12287MB, multi_processor_count=80)\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "         [ 3,  4,  5,  9, 10, 11, 15, 16, 17],\n",
      "         [18, 19, 20, 24, 25, 26, 30, 31, 32],\n",
      "         [21, 22, 23, 27, 28, 29, 33, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *\n",
    "from Patched_TF import PE2D\n",
    "def patch2D(x,n,Lx):\n",
    "    # type: (Tensor,int,int) -> Tensor\n",
    "    \"\"\"patch your sequence into chunks of 4\"\"\"\n",
    "    #make the input 2D then break it into 2x2 chunks \n",
    "    #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "    return x.view([x.shape[0],Lx,Lx]).unfold(-2,n,n).unfold(-2,n,n).reshape([x.shape[0],int(Lx*Lx//n**2),int(n**2)])\n",
    "\n",
    "def unpatch2D(x,n,Lx):\n",
    "    # type: (Tensor,int,int) -> Tensor\n",
    "    \"\"\"inverse function for patch\"\"\"\n",
    "    # original sequence order can be retrieved by chunking twice more\n",
    "    #in the x-direction you should have chunks of size 2, but in y it should\n",
    "    #be chunks of size Ly//2\n",
    "    return x.unfold(-2,Lx//n,Lx//n).unfold(-2,n,n).reshape([x.shape[0],Lx*Lx])\n",
    "\n",
    "\n",
    "Lx=6\n",
    "a = torch.arange(Lx**2).unsqueeze(0)\n",
    "print(a)\n",
    "print(a.view([Lx,Lx]))\n",
    "b = patch2D(a,3,Lx)\n",
    "print(b)\n",
    "c = unpatch2D(b,3,Lx)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef655c",
   "metadata": {},
   "source": [
    "# The below functions are only for patches of size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def patch2idx(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return out.to(torch.int64)\n",
    "\n",
    "@torch.jit.script\n",
    "def patch2onehot(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a2d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTFRNN(Sampler):#(torch.jit.ScriptModule):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    \"\"\"\n",
    "    \n",
    "    Sampler class which uses a transformer for long term information and an RNN for short term information.\n",
    "    \n",
    "    The sequence is broken into 2D patches (4x4 by default), each patch is expanded to a tensor of size Nh (be repeating it),\\\n",
    "    then a positional encoding is added. You then apply masked self-attention to the patches num_layers times, with the final\n",
    "    outputs fed in as the initial hidden state of an rnn.\n",
    "    \n",
    "    Going with 4x4 patches, you can use these patches as a sequence to get a factorized probability of the entire\n",
    "    4x4 patch by feeding the 2x2 patches in one at a time and outputting a size 16 tensor \n",
    "    (probability of all possible next 2x2 patches) for each patch. The output is obtained by applying two FC layers to\n",
    "    the hidden state of the rnn.\n",
    "    \n",
    "    \n",
    "    Here is an example of how everything comes together\n",
    "    \n",
    "    Say you have a 16x16 input and Nh=128, this input is broken into 16 4x4 patches which are repeated 8 times and\n",
    "    given a positional encoding. Masked self attention is done between the 16 patches (size Nh) for N layers, then\n",
    "    16 RNNs are given the outputs in parallel as the hidden state. Now the original input is broken into 16 sets of 4 2x2\n",
    "    patches. These length 4 sequences are given to the rnns (16 in parallel all sharing the same weights) and the outputs\n",
    "    are then grouped together such that you end up with a length 64 sequence of vectors of size 16. this gives your probability.\n",
    "    You can easily calculate it by taking the output (of 16) corresponding to each 2x2 patch and multiplying all 64 of them\n",
    "    together (or adding them in logscale).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,px=4,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8,rnn_patch=4,rnntype=\"GRU\", **kwargs):\n",
    "        super(PTFRNN, self).__init__()\n",
    "        #print(nhead)\n",
    "        self.pe = PE2D(Nh, Lx,Lx,device)\n",
    "        self.device=device\n",
    "        #Encoder only transformer\n",
    "        #misinterperetation on encoder made it so this code does not work\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        assert rnn_patch==4\n",
    "        #have to do more work to implement other values but I may want to do so later\n",
    "        \n",
    "        assert rnntype!=\"LSTM\"\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=rnn_patch,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,2**rnn_patch),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "        #sequence size in x\n",
    "        self.Lx=Lx\n",
    "        #transformer patch size in x\n",
    "        self.px=px\n",
    "        #transformer patch size\n",
    "        self.p=px**2\n",
    "        #rnn patch size\n",
    "        self.prnn=rnn_patch\n",
    "        \n",
    "        self.set_mask(Lx**2//self.p)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "        \n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones([L,L],device=self.device)))\n",
    "        self.pe.L=L\n",
    "        \n",
    "    \n",
    "    def rnnforward(self,hidden,input):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            hidden - shape [L/p,B,Nh] tensor\n",
    "            input - shape [L/p,B,p] tensor\n",
    "        Outputs\n",
    "            out - shape [L/p,B,p/4,Nh] tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        Lp,B,Nh=hidden.shape\n",
    "        h0 = hidden.view([1,Lp*B,Nh])\n",
    "        input = patch2D(input.reshape([Lp*B,self.p]),2,self.px)\n",
    "        #probably need this?\n",
    "        input[:,1:]=input[:,:-1]\n",
    "        input[:,0]=0\n",
    "        \n",
    "        out,h=self.rnn(input,h0)\n",
    "        return out.view([Lp,B,self.p//4,Nh])\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]//self.p!=self.L:\n",
    "            self.set_mask(input.shape[1]//self.p)\n",
    "        \n",
    "        input=patch2D(input.squeeze(-1),self.px,self.Lx).transpose(1,0)\n",
    "        #pe should be sequence first [L/p,B,Nh]\n",
    "        hidden = self.transformer(self.pe(input),self.mask)        \n",
    "        rnnout = self.rnnforward(hidden,input)\n",
    "        \n",
    "        #[L/p,B,p/4,Nh] -> [B,L/4,16]\n",
    "        output = self.lin(rnnout).transpose(1,0).view([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "        return output\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        # type: (Tensor,Optional[Tensor],int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,1]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,1]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        #HMMM\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i,layer in enumerate(self.transformer.layers):\n",
    "            \n",
    "            tgt=output\n",
    "            #have to merge the functions into one\n",
    "            src = tgt[idx:, :, :]\n",
    "            mask = None if idx==-1 else self.mask[idx:]\n",
    "\n",
    "            # self attention part\n",
    "            src2 = layer.self_attn(\n",
    "                src,#only do attention with the last elem of the sequence\n",
    "                tgt,\n",
    "                tgt,\n",
    "                attn_mask=mask,  \n",
    "                key_padding_mask=None,\n",
    "            )[0]\n",
    "            #straight from torch transformer encoder code\n",
    "            src = src + layer.dropout1(src2)\n",
    "            src = layer.norm1(src)\n",
    "            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "            src = src + layer.dropout2(src2)\n",
    "            src = layer.norm2(src)\n",
    "            #return src\n",
    "            \n",
    "            output = src#self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        return self.sampleDebug(B,L,cache)[0]\n",
    "    @torch.jit.export\n",
    "    def sampleDebug(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tuple[Tensor,Tensor]\n",
    "\n",
    "\n",
    "        #length is divided by four due to patching\n",
    "        L=L//self.p\n",
    "        \n",
    "        DEBUG=torch.zeros([B],device=self.device)\n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([L+1,B,self.p],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            \n",
    "            h = output[-1,:,:].unsqueeze(0)\n",
    "            \n",
    "            rnnseq = torch.zeros([B,self.p//4+1,4],device=self.device)\n",
    "            for rdx in range(1,self.p//4+1):\n",
    "                \n",
    "                out,h=self.rnn(rnnseq[:,rdx-1:rdx,:],h)\n",
    "                #check out the probability of all 16 vectors\n",
    "                probs=self.lin(out).view([B,16])\n",
    "                #sample from the probability distribution\n",
    "                indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "                #extract samples\n",
    "                sample = self.options[indices]\n",
    "                #add sample to sequence\n",
    "                rnnseq[:,rdx] = sample\n",
    "                \n",
    "                #debugging info\n",
    "                real=patch2onehot(sample)\n",
    "                total = torch.sum(real*probs,dim=-1)\n",
    "                DEBUG+=torch.log(total)\n",
    "                \n",
    "            #set input to the (unpatched) rnn sequence\n",
    "            input[idx] = unpatch2D(rnnseq[:,1:],2,self.px)\n",
    "            \n",
    "        #remove the leading zero in the input    \n",
    "        input=input[1:]\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch2D(input.transpose(1,0),self.px,self.Lx).unsqueeze(-1),DEBUG\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "                \n",
    "        if input.shape[1]//self.p!=self.L:\n",
    "            self.set_mask(input.shape[1]//self.p)\n",
    "        \n",
    "        #shape is modified to [L//p,B,p]\n",
    "        input = patch2D(input.squeeze(-1),self.px,self.Lx).transpose(1,0)\n",
    "        \n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        data[1:]=input[:-1]\n",
    "        \n",
    "        #[L//p,B,p] -> [L//p,B,Nh]\n",
    "        encoded=self.pe(data)\n",
    "        # [L//p,B,Nh] -> [L//p,B,Nh]\n",
    "        hidden = self.transformer(encoded,self.mask)     \n",
    "        # [L//p,B,Nh],[L//p,B,p] -> [L/p,B,p/4,Nh]\n",
    "        rnnout = self.rnnforward(hidden,input)\n",
    "        \n",
    "        #[L/p,B,p/4,Nh] -> [B,L/4,16]\n",
    "        output = self.lin(rnnout).transpose(1,0).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [B,L//4,16]\n",
    "        \n",
    "        Lp,B,Nh=hidden.shape\n",
    "        \n",
    "        \n",
    "        \n",
    "        #reshaping the input to match the shape of the output (idk if this is correct tbh)\n",
    "        susman = patch2D(input.reshape([Lp*B,self.p]),2,self.px).view([Lp,B,self.p//4,4]).transpose(1,0)\n",
    "\n",
    "        \n",
    "        #print(susman[:,0])\n",
    "        \n",
    "        real=patch2onehot(susman).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "        \n",
    "        #print(real.shape,output.shape)\n",
    "        \n",
    "        #[B,L//4,16] -> [B,L//4]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[B,L//4] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1)\n",
    "        return logp\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch2D(sample.squeeze(-1),self.px,self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//self.p,self.p],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//self.p):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(self.p):\n",
    "                sflip[:,j*self.p+j2] = sample*1.0\n",
    "                sflip[:,j*self.p+j2,j,j2] = 1-sflip[:,j*self.p+j2,j,j2]\n",
    "            \n",
    "        #switch sample into sequence-first\n",
    "        sample = sample.transpose(1,0)\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            data[1:]=sample[:-1]\n",
    "            \n",
    "            #[L//4,B,4] -> [L//4,B,Nh]\n",
    "            encoded=self.pe(data)\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.make_cache(encoded)\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L//self.p,B*L//D,cache.shape[-1])\n",
    "\n",
    "            \n",
    "            \n",
    "            rnnout = self.rnnforward(out,sample)\n",
    "            #[L/p,B,p/4,Nh] -> [B,L/4,16]\n",
    "            pred0 = self.lin(rnnout).transpose(1,0).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "            #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "            #shape will be [B,L//4,16]\n",
    "            Lp,B,Nh=out.shape\n",
    "            #reshaping the input to match the shape of the output (idk if this is correct tbh)\n",
    "            susman = patch2D(sample.reshape([Lp*B,self.p]),2,self.px).view([Lp,B,self.p//4,4]).transpose(1,0)\n",
    "            real=patch2onehot(susman).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "\n",
    "            \n",
    "            #[B,L//4,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//self.p,self.p]).transpose(1,0)\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[1:]=tmp[:-1]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample)\n",
    "                #grab your transformer output\n",
    "                out,_=self.next_with_cache(tgt,cache[:,:N//self.p],N//self.p)\n",
    "                #only the newly computed parts are necessary\n",
    "                out = out [N//self.p:]\n",
    "                \n",
    "                rnnout = self.rnnforward(out,tmp[N//self.p:])\n",
    "                \n",
    "                Lp,Bp,Nh=out.shape\n",
    "                \n",
    "                \n",
    "                # grab output for the new part\n",
    "                output = self.lin(rnnout).transpose(1,0).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "                \n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                \n",
    "                susman = patch2D(tmp[N//self.p:].reshape([Lp*Bp,self.p]),2,self.px).view([Lp,Bp,self.p//4,4]).transpose(1,0)\n",
    "        \n",
    "                real=patch2onehot(susman).reshape([rnnout.shape[1],rnnout.shape[0]*rnnout.shape[2],16])\n",
    "                \n",
    "                real = real.view([B,L//D,(L-N)//4,16])\n",
    "                \n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                \n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs\n",
    "    @torch.jit.export\n",
    "    def ffq(self,samplequeue,sump_queue,sqrtp_queue,Q,K,L,NLOOPS):\n",
    "        # type: (Tensor,Tensor,Tensor,int,int,int,int)\n",
    "        for i in range(Q):\n",
    "            sample,sump,sqrtp = self.sample_with_labelsALT(K,L,grad=False,nloops=NLOOPS)\n",
    "            samplequeue[i*K:(i+1)*K]=sample\n",
    "            sump_queue[i*K:(i+1)*K]=sump\n",
    "            sqrtp_queue[i*K:(i+1)*K]=sqrtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab439d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 64, 1])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "pb = PTFRNN(8)\n",
    "sample,p0 = pb.sampleDebug(6,8*8)\n",
    "\n",
    "print(sample.shape)\n",
    "\n",
    "ps = pb.logprobability(sample)\n",
    "\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0386ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tensor([-43.9952, -44.3971, -44.2783, -44.2749, -43.9387, -44.4712],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>) \n",
      " tensor([-43.9952, -44.3972, -44.2783, -44.2749, -43.9389, -44.4713],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"\",ps,'\\n',p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373cff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(px,Lx):\n",
    "    sflip = torch.arange(Lx*Lx,device=device).to(torch.int64).reshape([1,Lx,Lx])\n",
    "    sflip = patch2D(sflip,px,Lx).reshape(Lx*Lx)\n",
    "    \n",
    "    return sflip\n",
    "\n",
    "if False:\n",
    "    B=32\n",
    "\n",
    "    s = pb.sample(B,8*8)\n",
    "    probs = super(PTFRNN,pb)._off_diag_labels(s,B,8*8,False,D=4)[1][:,get_indices(4,8)]\n",
    "    \n",
    "    p2 = pb._off_diag_labels(s,B,8*8,False,D=4)[1]\n",
    "\n",
    "    print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "    print(probs.mean(),p2.mean())\n",
    "    print(abs(probs-p2).max())\n",
    "    plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4cd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t2304\n",
      "Q                             \t\t\t8\n",
      "K                             \t\t\t32\n",
      "B                             \t\t\t256\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.9\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t36\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPTFRNN\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "ffq                           \t\t\tTrue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=48\n",
    "op.L=Lx*Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.M=0.9\n",
    "op.Q=8\n",
    "op.K=32\n",
    "op.USEQUEUE=0\n",
    "op.kl=0.0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "#op.steps=4000\n",
    "op.dir=\"PTFRNN\"\n",
    "#op.steps=100\n",
    "op.NLOOPS=36\n",
    "op.ffq=True\n",
    "print(op)\n",
    "op.patch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1525e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(\"python PTFRNN.py \"+op.cmd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "445c37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "trainsformer = torch.jit.script(PTFRNN(Lx,Nh=op.Nh,px=op.patch_size))\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1631558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849104\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.numel() for p in trainsformer.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d9572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.3645 2304\n",
      "13,2.941|\n",
      "5485,-0.318|10996,-0.321|16488,-0.324|21986,-0.325|27511,-0.328|33017,-0.332|38513,-0.335|44010,-0.339|\n",
      "49503,-0.341|54995,-0.342|60483,-0.345|65971,-0.347|71460,-0.348|76977,-0.348|82495,-0.349|87981,-0.351|\n",
      "93473,-0.353|98957,-0.355|104445,-0.356|109936,-0.356|115428,-0.356|120919,-0.356|126409,-0.357|131889.2290611267 12000\n"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb79745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder path established\n",
      "-0.3645 2304\n",
      "11,-0.358|\n",
      "5496,-0.360|10982,-0.361|16466,-0.360|21947.088606595993 2000\n"
     ]
    }
   ],
   "source": [
    "op.steps=2000\n",
    "reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd62b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Ti', major=8, minor=6, total_memory=12287MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "from PTFRNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db10e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849104\n"
     ]
    }
   ],
   "source": [
    "op=Opt(K=256,Q=1,Nh=128,dir=\"<NONE>\",hamiltonian=\"TFIM\",J=1,h=-1)\n",
    "op.B=op.K*op.Q\n",
    "op.patch=20\n",
    "op.L= 100\n",
    "op.sgrad=False\n",
    "op.NLOOPS=op.L//op.patch\n",
    "\n",
    "pb = PTFRNN(op.L,op.patch,_2D=op._2D,Nh=op.Nh,num_layers=2,rnnargs={\"p\":4})\n",
    "\n",
    "pb1 = torch.jit.script(pb)\n",
    "\n",
    "print(sum([p.numel() for p in pb.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b727a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1])\n",
      "0.000909268856048584 8.305120648834622\n",
      "--------------------\n",
      "7.437526801368222e-05 8.080736457574726\n",
      "tensor(-34.2419, device='cuda:0') tensor(-34.2419, device='cuda:0')\n",
      "tensor(0.0009, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1970e520388>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B=32\n",
    "\n",
    "sample,p0 = pb.sample(B,100)\n",
    "print(sample.shape)\n",
    "ps = pb.logprobability(sample)\n",
    "\n",
    "pvar,pmean = torch.var_mean(p0)\n",
    "\n",
    "var,mean = torch.var_mean(abs(p0-ps))\n",
    "\n",
    "print(mean.item(),pvar.item()**0.5)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "probs = super(PTFRNN,pb).off_diag_labels(sample,nloops=op.NLOOPS)\n",
    "p2 = pb.off_diag_labels(sample,nloops=op.NLOOPS)\n",
    "\n",
    "print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "print(probs.mean(),p2.mean())\n",
    "print(abs(probs-p2).max())\n",
    "plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80b5d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2732919061200112 100\n",
      "0,-1.002|\n",
      "7.844459772109985 100\n"
     ]
    }
   ],
   "source": [
    "op.steps=100\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "pb.parameters(), \n",
    "lr=op.lr,\n",
    "betas=(beta1,beta2)\n",
    ")\n",
    "\n",
    "debug = reg_train(op,(pb,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3c462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
