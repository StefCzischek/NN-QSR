{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e25b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bcc58",
   "metadata": {},
   "source": [
    "# Reshaping the 2D sequence tensor into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90f6218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  6,  7],\n",
      "         [ 2,  3,  8,  9],\n",
      "         [ 4,  5, 10, 11],\n",
      "         [12, 13, 18, 19],\n",
      "         [14, 15, 20, 21],\n",
      "         [16, 17, 22, 23],\n",
      "         [24, 25, 30, 31],\n",
      "         [26, 27, 32, 33],\n",
      "         [28, 29, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n"
     ]
    }
   ],
   "source": [
    "def patch(x,Lx):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    \"\"\"patch your sequence into chunks of 4\"\"\"\n",
    "    #make the input 2D then break it into 2x2 chunks \n",
    "    #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "    return x.view([x.shape[0],Lx,Lx]).unfold(-2,2,2).unfold(-2,2,2).reshape([x.shape[0],Lx*Lx//4,4])\n",
    "\n",
    "def unpatch(x,Lx):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    \"\"\"inverse function for patch\"\"\"\n",
    "    # original sequence order can be retrieved by chunking twice more\n",
    "    #in the x-direction you should have chunks of size 2, but in y it should\n",
    "    #be chunks of size Ly//2\n",
    "    return x.unfold(-2,Lx//2,Lx//2).unfold(-2,2,2).reshape([x.shape[0],Lx*Lx])\n",
    "\n",
    "Lx=6\n",
    "a = torch.arange(Lx**2).unsqueeze(0)\n",
    "print(a)\n",
    "print(a.view([Lx,Lx]))\n",
    "b = patch(a,Lx)\n",
    "print(b)\n",
    "c = unpatch(b,Lx)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78462733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PE2D(nn.Module):\n",
    "    #TODO: Positional encoding is wrong because the spins are at index i+1 when we sample and get probabilities\n",
    "    def __init__(self, d_model, Lx,Ly,device,n_encode=None):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        assert (d_model%4==0)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(Lx*Ly, d_model)\n",
    "        \n",
    "        if type(n_encode)==type(None):\n",
    "            n_encode=3*d_model//4\n",
    "        for pos in range(Lx*Ly):\n",
    "            x=pos//Ly\n",
    "            y=pos%Ly\n",
    "            # Only going to fill 3/4 of the matrix so the\n",
    "            # occupation values are preserved\n",
    "            for i in range(0, n_encode, 4):\n",
    "                \n",
    "                #x direction encoding\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(x / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(x / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                #y direction encoding\n",
    "                pe[pos, i+2] = \\\n",
    "                math.sin(y / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 3] = \\\n",
    "                math.cos(y / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(1).to(device)\n",
    "        self.L=Lx*Ly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.repeat(1,1,self.d_model//4) + self.pe[:x.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c562c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def patch2idx(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return out.to(torch.int64)\n",
    "\n",
    "@torch.jit.script\n",
    "def patch2onehot(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2567828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerBase(Sampler):#(torch.jit.ScriptModule):\n",
    "    \"\"\"\n",
    "    Base class for the two patch transformer architectures \n",
    "    \n",
    "    Architexture wise this is how a patched transformer works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each patch given it and all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    Outputs should either be size 1 (the probability of the current patch which is input) or size 16 (for 2x2 patches where \n",
    "    the probability represented is of each potential patch)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8,outsize=1, **kwargs):\n",
    "        super(PatchTransformerBase, self).__init__()\n",
    "        #print(nhead)\n",
    "        self.pe = PE2D(Nh, Lx,Lx,device)\n",
    "        self.device=device\n",
    "        #Encoder only transformer\n",
    "        #misinterperetation on encoder made it so this code does not work\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,outsize),\n",
    "                nn.Sigmoid() if outsize==1 else nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "        self.Lx=Lx\n",
    "        self.set_mask(Lx**2//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "        \n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones([L,L],device=self.device)))\n",
    "        self.pe.L=L\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        input=self.pe(patch(input.squeeze(-1),self.Lx).transpose(1,0))\n",
    "        output = self.transformer(input,self.mask)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        # type: (Tensor,Optional[Tensor],int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,1]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,1]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        #HMMM\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i,layer in enumerate(self.transformer.layers):\n",
    "            \n",
    "            tgt=output\n",
    "            #have to merge the functions into one\n",
    "            src = tgt[idx:, :, :]\n",
    "            mask = None if idx==-1 else self.mask[idx:]\n",
    "\n",
    "            # self attention part\n",
    "            src2 = layer.self_attn(\n",
    "                src,#only do attention with the last elem of the sequence\n",
    "                tgt,\n",
    "                tgt,\n",
    "                attn_mask=mask,  \n",
    "                key_padding_mask=None,\n",
    "            )[0]\n",
    "            #straight from torch transformer encoder code\n",
    "            src = src + layer.dropout1(src2)\n",
    "            src = layer.norm1(src)\n",
    "            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "            src = src + layer.dropout2(src2)\n",
    "            src = layer.norm2(src)\n",
    "            #return src\n",
    "            \n",
    "            output = src#self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b4525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformer(PatchTransformerBase):\n",
    "    \"\"\"Note: logprobability is not normalized \n",
    "    the cost of normalization is 16x the cost of getting un normalized labels due to the output being size 1\n",
    "    \n",
    "    Note: To actually make sure the probability is normalized at each step you would need to run the network on all 16 \n",
    "         possible 2x2 patches at each step.\n",
    "         \n",
    "    This is done in sample, where you feed it your batch x 16 samples (one for each possible patch), get your probabilities\n",
    "    of each patch, sum them to normalize, then sample from your distribution. Once your sample is chosen, you can just\n",
    "    copy the sample that was chosen,as well as your cached self attention 15 samples / cached portions that weren't chosen.\n",
    "    \n",
    "    \n",
    "    You probably want to add in a term which asks normalization to be 1 in training, as if this doesn't hold then the sample \n",
    "    probability and label probability will be different which is very bad.\n",
    "    \n",
    "    An alternate form of this network (still in production) could have 16 outputs for probability instead of 1, and\n",
    "    give the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    This model would always be normalized (just use softmax on the output) but total probabilities would be a bit more\n",
    "    difficult to calculate as you have to match each patch to it's respective output (of the 16 total probability outputs)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx, **kwargs):\n",
    "        #only important bit is that outsize = 1\n",
    "        super(PatchTransformer, self).__init__(Lx,outsize=1,**kwargs)\n",
    "        \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        total = self.forward(input)\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp   \n",
    "    \n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        #transformers don't do batch first so to save a bunch of transpose calls \n",
    "        input = torch.zeros([L,B*16,4],device=self.device)\n",
    "        #self.set_mask(L)\n",
    "        batchoptions=self.options.repeat(B,1)\n",
    "        cache=None\n",
    "        \n",
    "        normalization=torch.zeros([B],device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          for idx in range(L):\n",
    "            \n",
    "            input[idx] = batchoptions\n",
    "            \n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx+1,:,:])\n",
    "                        \n",
    "            \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "            \n",
    "            #get the probability sum and add it's log to the normalization \n",
    "            psum=torch.sum(probs,dim=1)\n",
    "            normalization+=torch.log(psum)\n",
    "            \n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs/psum.unsqueeze(1),1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample.view([B,1,4]).repeat(1,16,1).view([B*16,4])\n",
    "            \n",
    "            #set cache to the samples that are used\n",
    "            #cache is shape [?,L,B*16,Nh]\n",
    "            \n",
    "            \n",
    "            tmp = cache[:,idx].view(cache.shape[0],B,16,cache.shape[-1])\n",
    "            \n",
    "            tmp[:] = getcache(tmp,indices) #make sure this sets things properly\n",
    "            # tmp should still be a view of cache[:,idx] (same memory)\n",
    "        self.normalization = normalization\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input.view([L,B,16,4])[:,:,0].transpose(1,0),self.Lx).unsqueeze(-1)\n",
    "    \n",
    "@torch.jit.script\n",
    "def getcachealt(oldcache,indices):\n",
    "    sx,B,_,sy=oldcache.shape\n",
    "    newcache = oldcache*1\n",
    "    for i in range(B):\n",
    "        #print(oldcache[:,i,indices[i]].shape,newcache.shape)\n",
    "        newcache[:,i,:]=oldcache[:,i,indices[i]].unsqueeze(1)\n",
    "    return newcache\n",
    "    \n",
    "    \n",
    "@torch.jit.script\n",
    "def getcache(oldcache,indices):\n",
    "    \"\"\"Supposed to take the cache at the selected index and copy it to the other 15\"\"\"\n",
    "    sx,B,_,sy=oldcache.shape\n",
    "    newcache = oldcache*0\n",
    "    for i in range(16):\n",
    "        #add the cache at position i only if it is the right index???\n",
    "        tmp=(oldcache[:,:,i]*((indices==i).view(1,B,1))).view(sx,B,1,sy)\n",
    "        newcache+=tmp\n",
    "    return newcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2311e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerB(PatchTransformerBase):\n",
    "    \"\"\"Note: logprobability IS normalized \n",
    "    \n",
    "    Architexture wise this is how it works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each potential patch given all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    \n",
    "    This model has 16 outputs, which describes the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        #only important bit is that outsize = 16\n",
    "        super(PatchTransformerB, self).__init__(Lx,outsize=16,**kwargs)\n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        \n",
    "        #shape is modified to [L//4,B,4]\n",
    "        input = patch(input.squeeze(-1),self.Lx).transpose(1,0)\n",
    "        \n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        data[1:]=input[:-1]\n",
    "        \n",
    "        #[L//4,B,4] -> [L//4,B,Nh]\n",
    "        encoded=self.pe(data)\n",
    "        #shape is preserved\n",
    "        output = self.transformer(encoded,self.mask)\n",
    "        # [L//4,B,Nh] -> [L//4,B,16]\n",
    "        output = self.lin(output)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [L//4,B,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[L//4,B,16] -> [L//4,B]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[L//4,B] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=0)\n",
    "        return logp   \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([L+1,B,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample\n",
    "            \n",
    "        #remove the leading zero in the input    \n",
    "        input=input[1:]\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input.transpose(1,0),self.Lx).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch(sample.squeeze(-1),self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #switch sample into sequence-first\n",
    "        sample = sample.transpose(1,0)\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            data[1:]=sample[:-1]\n",
    "            \n",
    "            #[L//4,B,4] -> [L//4,B,Nh]\n",
    "            encoded=self.pe(data)\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.make_cache(encoded)\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L//4,B*L//D,cache.shape[-1])\n",
    "\n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [L//4,B,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[L//4,B,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1).transpose(1,0)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4]).transpose(1,0)\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[1:]=tmp[:-1]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample)\n",
    "                #grab your transformer output\n",
    "                out,_=self.next_with_cache(tgt,cache[:,:N//4],N//4)\n",
    "\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out[N//4:].transpose(1,0))\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                \n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs\n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e37fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchedRNN(Sampler):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    def __init__(self,Lx,rnntype=\"GRU\",Nh=128,device=device, **kwargs):\n",
    "        super(PatchedRNN, self).__init__(device=device)\n",
    "        \n",
    "        \n",
    "        assert rnntype!=\"LSTM\"\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=4,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,16),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "        self.Nh=Nh\n",
    "        self.rnntype=rnntype\n",
    "        \n",
    "        \n",
    "        self.Lx=Lx\n",
    "        self.L = (Lx**2//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "            \n",
    "        \n",
    "        self.to(device)\n",
    "    def forward(self, input):\n",
    "        # h0 is shape [d*numlayers,B,H] but D=numlayers=1 so\n",
    "        # h0 has shape [1,B,H]\n",
    "        \n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h0=[torch.zeros([1,input.shape[0],self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,input.shape[0],self.Nh],device=self.device)]\n",
    "            #h0 and c0\n",
    "        #else:\n",
    "        h0=torch.zeros([1,input.shape[0],self.Nh],device=self.device)\n",
    "        out,h=self.rnn(input,h0)\n",
    "        return self.lin(out)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "                \n",
    "        #shape is modified to [B,L//4,4]\n",
    "        input = patch(input.squeeze(-1),self.Lx)\n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        #batch first\n",
    "        data[:,1:]=input[:,:-1]\n",
    "        # [B,L//4,Nh] -> [B,L//4,16]\n",
    "        output = self.forward(data)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [B,L//4,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[B,L//4,16] -> [B,L//4]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[B,L//4] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1)\n",
    "        return logp\n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h=[torch.zeros([1,B,self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,B,self.Nh],device=self.device)]\n",
    "            #h is h0 and c0\n",
    "        #else:\n",
    "        h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "        #Sample set will have shape [B,L,4]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([B,L+1,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #out should be batch first [B,L,Nh]\n",
    "            out,h=self.rnn(input[:,idx-1:idx,:],h)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(out[:,0,:]).view([B,16])\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            input[:,idx] = sample\n",
    "        #remove the leading zero in the input    \n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input[:,1:],self.Lx).unsqueeze(-1)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch(sample.squeeze(-1),self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            \n",
    "            data[:,1:]=sample[:,:-1]\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            \n",
    "            h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "            \n",
    "            out,_=self.rnn(data,h)\n",
    "            \n",
    "            #cache for the rnn is the output in this sense\n",
    "            #shape [B,L//4,Nh]\n",
    "            cache=out\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(1)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,L//D,1,1).reshape(B*L//D,L//4,cache.shape[-1])\n",
    "                        \n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [B,L//4,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[B,L//4,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4])\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[:,1:]=tmp[:,:-1]\n",
    "                #grab your rnn output\n",
    "                if k==0:\n",
    "                    out,_=self.rnn(fsample,cache[:,0].unsqueeze(0)*0.0)\n",
    "                else:\n",
    "                    out,_=self.rnn(fsample[:,N//4:],cache[:,N//4-1].unsqueeze(0)*1.0)\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out)\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055fad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PatchTransformer(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a207d8",
   "metadata": {},
   "source": [
    "# Testing torch multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e52639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  9,  5,  4,  0,  2, 15,  1], device='cuda:0')\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "B=8\n",
    "batchoptions=p.options.repeat(B,1)\n",
    "sm = batchoptions.reshape([B,16,4])\n",
    "probs = torch.rand([B,16],device=device)\n",
    "#get the probability sum and add it's log to the normalization \n",
    "psum=torch.sum(probs,dim=1).unsqueeze(1)\n",
    "\n",
    "indices = torch.multinomial(probs/psum,1,False)\n",
    "\n",
    "print(indices.squeeze(1))\n",
    "\n",
    "print(p.options[indices.squeeze(1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba1164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16, 3])\n",
      "torch.Size([4, 8, 16, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([4,8,16,3],device=device)\n",
    "\n",
    "print(getcache(x,indices.squeeze(1)).shape)\n",
    "\n",
    "print(getcachealt(x,indices.squeeze(1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c71fc",
   "metadata": {},
   "source": [
    "# Testing getcache functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93889178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst = (torch.eye(16)*(torch.arange(16)+1)).to(device)\n",
    "\n",
    "indices2=torch.arange(16,device=device)\n",
    "print(indices2)\n",
    "plt.imshow(tst.cpu())\n",
    "plt.show()\n",
    "plt.imshow(getcache(tst.view([1,16,16,1]),indices2).view([16,16]).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b226fc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sample(8,4*4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd9745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.logprobability(p.sample(8,4*4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e8a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62a4ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.zeros([1,2,3,4])\n",
    "x.transpose(-1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364d3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0')\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]], device='cuda:0')\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(patch2idx(p.options))\n",
    "\n",
    "print((p.options[patch2idx(p.options)]==p.options))\n",
    "\n",
    "print(patch2onehot(p.options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a184c81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb = PatchedRNN(8)\n",
    "\n",
    "pb.logprobability(pb.sample(12,8*8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177afd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "032d21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(Lx):\n",
    "    sflip = torch.arange(Lx*Lx,device=device).to(torch.int64).reshape([1,Lx,Lx])\n",
    "    sflip = patch(sflip,Lx).reshape(Lx*Lx)\n",
    "    \n",
    "    return sflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f23cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_indices(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c2d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7639249563217163e-05 0.21736802653215292\n",
      "tensor(-44.3169, device='cuda:0') tensor(-44.3169, device='cuda:0')\n",
      "tensor(0.0001, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    B=32\n",
    "\n",
    "    s = pb.sample(B,8*8)\n",
    "    probs = super(PatchedRNN,pb)._off_diag_labels(s,B,8*8,False,D=8)[1][:,get_indices(8)]\n",
    "    \n",
    "    p2 = pb._off_diag_labels(s,B,8*8,False,D=8)[1]\n",
    "\n",
    "    print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "    print(probs.mean(),p2.mean())\n",
    "    print(abs(probs-p2).max())\n",
    "    plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f27c6cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t1024\n",
      "Q                             \t\t\t4\n",
      "K                             \t\t\t64\n",
      "B                             \t\t\t256\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.9\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t256\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPTF\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=32\n",
    "op.L=Lx*Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.M=0.9\n",
    "op.Q=4\n",
    "op.K=64\n",
    "op.USEQUEUE=0\n",
    "op.kl=0.0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "#op.steps=4000\n",
    "op.dir=\"PTF\"\n",
    "#op.steps=100\n",
    "op.NLOOPS=256\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24754787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(\"python Patched_TF.py \"+op.cmd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad96476d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "trainsformer = torch.jit.script(PatchTransformerB(Lx,Nh=op.Nh,num_layers=2))\n",
    "\n",
    "#trainsformer = torch.jit.script(PatchedRNN(Lx,Nh=op.Nh))\n",
    "#trainsformer = RNN(Nh=op.Nh)\n",
    "#sampleformer= PatchedRNN(Lx,Nh=op.Nh)\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f34a36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.3687 1024\n",
      "17,2.70|11095,-0.33|18837,-0.33|26579,-0.34|34319,-0.34|42066,-0.37|54534,-0.37|69547,-0.37|81270,-0.37|"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66bf7a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9e1622b385b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3af53",
   "metadata": {},
   "source": [
    "# Test Class for the patch transformer which makes sure all probabilities are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testPTF(PatchTransformerB):\n",
    "    \"\"\"Just adds some functions which make sure all probability labels are consistent\"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        super(testPTF,self).__init__(Lx,**kwargs)\n",
    "        self.reset(1)\n",
    "#functions below aren't really necessary anymore since there was no issue with masking (they serve to avoid using a mask)\n",
    "    def reset(self,B):\n",
    "        # type: (int) -> Tensor\n",
    "        \"\"\"Setup for an autoregressive transformer\"\"\"\n",
    "        self._input = torch.zeros([self.L+1,B,4],device=self.device)\n",
    "        self._i=1\n",
    "        encoded_input = self.pe(self._input[:self._i,:,:])\n",
    "        output,self._cache = self.next_with_cache(encoded_input,None)\n",
    "        probs=self.lin(output[-1,:,:])\n",
    "        return probs\n",
    "    def getnext(self,vect):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Get probability for the next output in an autoregressive transformer\"\"\"\n",
    "        self._input[self._i]=vect\n",
    "        self._i+=1\n",
    "        encoded_input = self.pe(self._input[:self._i,:,:])\n",
    "        output,self._cache = self.next_with_cache(encoded_input,self._cache)\n",
    "        probs=self.lin(output[-1,:,:])\n",
    "        return probs\n",
    "    @torch.jit.export\n",
    "    def testsample(self,B):\n",
    "        # type: (int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Generate states with their probabilities in logscale\"\"\"\n",
    "        #set up variables\n",
    "        L=self.L\n",
    "        probs=self.reset(B).squeeze(0)\n",
    "        sprobs=torch.zeros([B],device=self.device)\n",
    "        samples = torch.zeros([B,L,4],device=self.device)\n",
    "        with torch.no_grad():\n",
    "          for idx in range(L):\n",
    "            #loop through L sequence elements and generate next in sequence based off of probabilities\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            samples[:,idx] = sample\n",
    "            real=patch2onehot(sample)\n",
    "            total = torch.sum(real*probs,dim=-1)\n",
    "            sprobs+=torch.log(total)\n",
    "            if idx!=L-1: probs = self.getnext(sample)\n",
    "                \n",
    "        return unpatch(samples,self.Lx).unsqueeze(-1),sprobs\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def testlabels(self,samples,B):\n",
    "        # type: (Tensor,int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Get logscale probabilities of all states with one spin flipped at position j\"\"\"\n",
    "        with torch.no_grad():\n",
    "            #print(\"|\",end=\"\")\n",
    "            L=self.L\n",
    "            orig=samples\n",
    "            samples=patch(samples.squeeze(-1),self.Lx)\n",
    "            \n",
    "            logprobs=torch.zeros([B,L*4],device=self.device)\n",
    "            for k in range(L*4):\n",
    "                #loop cross L flipped states (batched)\n",
    "                probs=self.reset(B).squeeze(0)\n",
    "                sprobs = torch.zeros([B],device=self.device)\n",
    "                #loop across sequence\n",
    "                for idx in range(L):\n",
    "                    \n",
    "                    sample = samples[:,idx] \n",
    "                    #kth state is flipped\n",
    "                    if idx==k//4:\n",
    "                        #multiply by 1 as a way to copy the tensor to new memory\n",
    "                        sample=sample*1.0\n",
    "                        sample[:,k%4] = 1-sample[:,k%4]\n",
    "                    real=patch2onehot(sample)\n",
    "                    sprobs+=torch.log(torch.sum(real*probs,dim=-1))\n",
    "                    if idx!=L-1: probs = self.getnext(sample)\n",
    "                logprobs[:,k]=sprobs\n",
    "        return orig,logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd565c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydir = setup_dir(op)\n",
    "#mydir = \"PTF/Rydberg/64-NoQ-B=128-K=128-Nh=128-kl=0.00/0\"#\n",
    "mydir = \"PTF/Rydberg/576-NoQ-B=256-K=256-Nh=128-kl=0.00/0\"\n",
    "\n",
    "op.L=576\n",
    "Lx=24\n",
    "tst=torch.jit.load(mydir+\"/T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ad572",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesTF = testPTF(24,Nh=op.Nh,num_layers=2)\n",
    "tesTF = torch.jit.script(tesTF)\n",
    "momentum_update(0,tesTF,tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f226335",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tesTF.Lx==8:\n",
    "    s,p = tesTF.testsample(8)\n",
    "\n",
    "    print(p)\n",
    "    print(tesTF.logprobability(s))\n",
    "\n",
    "    s2,p2=tesTF.testlabels(s,8)\n",
    "\n",
    "\n",
    "\n",
    "    s3,p3=tesTF._off_diag_labels(s,8,64,False,1)\n",
    "\n",
    "    print(s.shape,s2.shape,p2.shape,p3.shape)\n",
    "\n",
    "    print(torch.sum(p3,dim=-1)/64)\n",
    "    print(torch.sum(p2,dim=-1)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486573b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "op.K=256\n",
    "op.Q=32\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "\n",
    "\n",
    "# Hamiltonian parameters\n",
    "N = op.L   # Total number of atoms\n",
    "V = 7.0     # Strength of Van der Waals interaction\n",
    "Omega = 1.0 # Rabi frequency\n",
    "delta = 1.0 # Detuning \n",
    "\n",
    "if op.hamiltonian==\"Rydberg\":\n",
    "    Lx=Ly=int(op.L**0.5)\n",
    "    op.L=Lx*Ly\n",
    "    h = Rydberg(Lx,Ly,V,Omega,delta)\n",
    "else:\n",
    "    #hope for the best here since there aren't defaults\n",
    "    h = TFIM(op.L,op.h,op.J)\n",
    "\n",
    "\n",
    "E_queue = torch.zeros([op.B],device=device)\n",
    "def fill_queue(net):\n",
    "    for i in range(op.Q):\n",
    "        print(\"|\",end=\"\")\n",
    "        if False:\n",
    "            sample,lp = net.testsample(op.K)\n",
    "            _,probs= net.testlabels(sample,op.K)\n",
    "            sqrtp=probs.mean(dim=1)/2\n",
    "            sump = torch.exp(probs/2-sqrtp.unsqueeze(1)).sum(dim=1)  \n",
    "        else:\n",
    "            sample,sump,sqrtp = net.sample_with_labelsALT(op.K,op.L,grad=False,nloops=144)\n",
    "            with torch.no_grad():\n",
    "                lp=net.logprobability(sample)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            E_i=h.localenergyALT(sample,lp,sump,sqrtp)\n",
    "            E_queue[i*op.K:(i+1)*op.K]=E_i\n",
    "t=time.time()\n",
    "fill_queue(tesTF)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errformat(m,s):\n",
    "    exp = -int(np.floor(np.log(s)/np.log(10)))\n",
    "    print( str(round(m,exp))+\" +/- \"+str(round(s,exp)))\n",
    "\n",
    "var,mean = torch.var_mean(E_queue/op.L)\n",
    "\n",
    "print(h.ground(),op.B)\n",
    "stdv=((var/op.B)**0.5).item()\n",
    "errformat(mean.item(),stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c42b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
