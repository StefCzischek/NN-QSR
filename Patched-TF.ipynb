{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e25b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bcc58",
   "metadata": {},
   "source": [
    "# Reshaping the 2D sequence tensor into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90f6218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  6,  7],\n",
      "         [ 2,  3,  8,  9],\n",
      "         [ 4,  5, 10, 11],\n",
      "         [12, 13, 18, 19],\n",
      "         [14, 15, 20, 21],\n",
      "         [16, 17, 22, 23],\n",
      "         [24, 25, 30, 31],\n",
      "         [26, 27, 32, 33],\n",
      "         [28, 29, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n"
     ]
    }
   ],
   "source": [
    "def patch(x,Lx):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    \"\"\"patch your sequence into chunks of 4\"\"\"\n",
    "    #make the input 2D then break it into 2x2 chunks \n",
    "    #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "    return x.view([x.shape[0],Lx,Lx]).unfold(-2,2,2).unfold(-2,2,2).reshape([x.shape[0],Lx*Lx//4,4])\n",
    "\n",
    "def unpatch(x,Lx):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    \"\"\"inverse function for patch\"\"\"\n",
    "    # original sequence order can be retrieved by chunking twice more\n",
    "    #in the x-direction you should have chunks of size 2, but in y it should\n",
    "    #be chunks of size Ly//2\n",
    "    return x.unfold(-2,Lx//2,Lx//2).unfold(-2,2,2).reshape([x.shape[0],Lx*Lx])\n",
    "\n",
    "Lx=6\n",
    "a = torch.arange(Lx**2).unsqueeze(0)\n",
    "print(a)\n",
    "print(a.view([Lx,Lx]))\n",
    "b = patch(a,Lx)\n",
    "print(b)\n",
    "c = unpatch(b,Lx)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78462733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PE2D(nn.Module):\n",
    "    #TODO: Positional encoding is wrong because the spins are at index i+1 when we sample and get probabilities\n",
    "    def __init__(self, d_model, Lx,Ly,device,n_encode=None):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        assert (d_model%4==0)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(Lx*Ly, d_model)\n",
    "        \n",
    "        if type(n_encode)==type(None):\n",
    "            n_encode=3*d_model//4\n",
    "        for pos in range(Lx*Ly):\n",
    "            x=pos//Ly\n",
    "            y=pos%Ly\n",
    "            # Only going to fill 3/4 of the matrix so the\n",
    "            # occupation values are preserved\n",
    "            for i in range(0, n_encode, 4):\n",
    "                \n",
    "                #x direction encoding\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(x / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(x / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                #y direction encoding\n",
    "                pe[pos, i+2] = \\\n",
    "                math.sin(y / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 3] = \\\n",
    "                math.cos(y / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(1).to(device)\n",
    "        self.L=Lx*Ly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.repeat(1,1,self.d_model//4) + self.pe[:x.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c562c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def patch2idx(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return out.to(torch.int64)\n",
    "\n",
    "@torch.jit.script\n",
    "def patch2onehot(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2567828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerBase(Sampler):#(torch.jit.ScriptModule):\n",
    "    \"\"\"\n",
    "    Base class for the two patch transformer architectures \n",
    "    \n",
    "    Architexture wise this is how a patched transformer works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each patch given it and all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    Outputs should either be size 1 (the probability of the current patch which is input) or size 16 (for 2x2 patches where \n",
    "    the probability represented is of each potential patch)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8,outsize=1, **kwargs):\n",
    "        super(PatchTransformerBase, self).__init__()\n",
    "        #print(nhead)\n",
    "        self.pe = PE2D(Nh, Lx,Lx,device)\n",
    "        self.device=device\n",
    "        #Encoder only transformer\n",
    "        #misinterperetation on encoder made it so this code does not work\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,outsize),\n",
    "                nn.Sigmoid() if outsize==1 else nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "        self.Lx=Lx\n",
    "        self.set_mask(Lx**2//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "        \n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones([L,L],device=self.device)))\n",
    "        self.pe.L=L\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        input=self.pe(patch(input.squeeze(-1),self.Lx).transpose(1,0))\n",
    "        output = self.transformer(input,self.mask)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        # type: (Tensor,Optional[Tensor],int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,1]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,1]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        #HMMM\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i,layer in enumerate(self.transformer.layers):\n",
    "            \n",
    "            tgt=output\n",
    "            #have to merge the functions into one\n",
    "            src = tgt[idx:, :, :]\n",
    "            mask = None if idx==-1 else self.mask[idx:]\n",
    "\n",
    "            # self attention part\n",
    "            src2 = layer.self_attn(\n",
    "                src,#only do attention with the last elem of the sequence\n",
    "                tgt,\n",
    "                tgt,\n",
    "                attn_mask=mask,  \n",
    "                key_padding_mask=None,\n",
    "            )[0]\n",
    "            #straight from torch transformer encoder code\n",
    "            src = src + layer.dropout1(src2)\n",
    "            src = layer.norm1(src)\n",
    "            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "            src = src + layer.dropout2(src2)\n",
    "            src = layer.norm2(src)\n",
    "            #return src\n",
    "            \n",
    "            output = src#self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b4525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformer(PatchTransformerBase):\n",
    "    \"\"\"Note: logprobability is not normalized \n",
    "    the cost of normalization is 16x the cost of getting un normalized labels due to the output being size 1\n",
    "    \n",
    "    Note: To actually make sure the probability is normalized at each step you would need to run the network on all 16 \n",
    "         possible 2x2 patches at each step.\n",
    "         \n",
    "    This is done in sample, where you feed it your batch x 16 samples (one for each possible patch), get your probabilities\n",
    "    of each patch, sum them to normalize, then sample from your distribution. Once your sample is chosen, you can just\n",
    "    copy the sample that was chosen,as well as your cached self attention 15 samples / cached portions that weren't chosen.\n",
    "    \n",
    "    \n",
    "    You probably want to add in a term which asks normalization to be 1 in training, as if this doesn't hold then the sample \n",
    "    probability and label probability will be different which is very bad.\n",
    "    \n",
    "    An alternate form of this network (still in production) could have 16 outputs for probability instead of 1, and\n",
    "    give the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    This model would always be normalized (just use softmax on the output) but total probabilities would be a bit more\n",
    "    difficult to calculate as you have to match each patch to it's respective output (of the 16 total probability outputs)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx, **kwargs):\n",
    "        #only important bit is that outsize = 1\n",
    "        super(PatchTransformer, self).__init__(Lx,outsize=1,**kwargs)\n",
    "        \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        total = self.forward(input)\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp   \n",
    "    \n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        #transformers don't do batch first so to save a bunch of transpose calls \n",
    "        input = torch.zeros([L,B*16,4],device=self.device)\n",
    "        #self.set_mask(L)\n",
    "        batchoptions=self.options.repeat(B,1)\n",
    "        cache=None\n",
    "        \n",
    "        normalization=torch.zeros([B],device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          for idx in range(L):\n",
    "            \n",
    "            input[idx] = batchoptions\n",
    "            \n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx+1,:,:])\n",
    "                        \n",
    "            \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "            \n",
    "            #get the probability sum and add it's log to the normalization \n",
    "            psum=torch.sum(probs,dim=1)\n",
    "            normalization+=torch.log(psum)\n",
    "            \n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs/psum.unsqueeze(1),1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample.view([B,1,4]).repeat(1,16,1).view([B*16,4])\n",
    "            \n",
    "            #set cache to the samples that are used\n",
    "            #cache is shape [?,L,B*16,Nh]\n",
    "            \n",
    "            \n",
    "            tmp = cache[:,idx].view(cache.shape[0],B,16,cache.shape[-1])\n",
    "            \n",
    "            tmp[:] = getcache(tmp,indices) #make sure this sets things properly\n",
    "            # tmp should still be a view of cache[:,idx] (same memory)\n",
    "        self.normalization = normalization\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input.view([L,B,16,4])[:,:,0].transpose(1,0),self.Lx).unsqueeze(-1)\n",
    "    \n",
    "@torch.jit.script\n",
    "def getcachealt(oldcache,indices):\n",
    "    sx,B,_,sy=oldcache.shape\n",
    "    newcache = oldcache*1\n",
    "    for i in range(B):\n",
    "        #print(oldcache[:,i,indices[i]].shape,newcache.shape)\n",
    "        newcache[:,i,:]=oldcache[:,i,indices[i]].unsqueeze(1)\n",
    "    return newcache\n",
    "    \n",
    "    \n",
    "@torch.jit.script\n",
    "def getcache(oldcache,indices):\n",
    "    \"\"\"Supposed to take the cache at the selected index and copy it to the other 15\"\"\"\n",
    "    sx,B,_,sy=oldcache.shape\n",
    "    newcache = oldcache*0\n",
    "    for i in range(16):\n",
    "        #add the cache at position i only if it is the right index???\n",
    "        tmp=(oldcache[:,:,i]*((indices==i).view(1,B,1))).view(sx,B,1,sy)\n",
    "        newcache+=tmp\n",
    "    return newcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2311e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerB(PatchTransformerBase):\n",
    "    \"\"\"Note: logprobability IS normalized \n",
    "    \n",
    "    Architexture wise this is how it works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each potential patch given all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    \n",
    "    This model has 16 outputs, which describes the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        #only important bit is that outsize = 16\n",
    "        super(PatchTransformerB, self).__init__(Lx,outsize=16,**kwargs)\n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        \n",
    "        #shape is modified to [L//4,B,4]\n",
    "        input = patch(input.squeeze(-1),self.Lx).transpose(1,0)\n",
    "        \n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        data[1:]=input[:-1]\n",
    "        \n",
    "        #[L//4,B,4] -> [L//4,B,Nh]\n",
    "        encoded=self.pe(data)\n",
    "        #shape is preserved\n",
    "        output = self.transformer(encoded,self.mask)\n",
    "        # [L//4,B,Nh] -> [L//4,B,16]\n",
    "        output = self.lin(output)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [L//4,B,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[L//4,B,16] -> [L//4,B]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[L//4,B] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=0)\n",
    "        return logp   \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([L+1,B,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample\n",
    "            \n",
    "        #remove the leading zero in the input    \n",
    "        input=input[1:]\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input.transpose(1,0),self.Lx).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch(sample.squeeze(-1),self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #switch sample into sequence-first\n",
    "        sample = sample.transpose(1,0)\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            data[1:]=sample[:-1]\n",
    "            \n",
    "            #[L//4,B,4] -> [L//4,B,Nh]\n",
    "            encoded=self.pe(data)\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.make_cache(encoded)\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L//4,B*L//D,cache.shape[-1])\n",
    "\n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [L//4,B,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[L//4,B,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1).transpose(1,0)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4]).transpose(1,0)\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[1:]=tmp[:-1]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample)\n",
    "                #grab your transformer output\n",
    "                out,_=self.next_with_cache(tgt,cache[:,:N//4],N//4)\n",
    "\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out[N//4:].transpose(1,0))\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                \n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs\n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e37fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchedRNN(Sampler):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    def __init__(self,Lx,rnntype=\"GRU\",Nh=128,device=device, **kwargs):\n",
    "        super(PatchedRNN, self).__init__(device=device)\n",
    "        \n",
    "        \n",
    "        assert rnntype!=\"LSTM\"\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=4,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,16),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        self.Nh=Nh\n",
    "        self.rnntype=rnntype\n",
    "        \n",
    "        \n",
    "        self.Lx=Lx\n",
    "        self.L = (Lx**2//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "            \n",
    "        \n",
    "        self.to(device)\n",
    "    def forward(self, input):\n",
    "        # h0 is shape [d*numlayers,B,H] but D=numlayers=1 so\n",
    "        # h0 has shape [1,B,H]\n",
    "        \n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h0=[torch.zeros([1,input.shape[0],self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,input.shape[0],self.Nh],device=self.device)]\n",
    "            #h0 and c0\n",
    "        #else:\n",
    "        h0=torch.zeros([1,input.shape[0],self.Nh],device=self.device)\n",
    "        out,h=self.rnn(input,h0)\n",
    "        return self.lin(out)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "                \n",
    "        #shape is modified to [B,L//4,4]\n",
    "        input = patch(input.squeeze(-1),self.Lx)\n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        #batch first\n",
    "        data[:,1:]=input[:,:-1]\n",
    "        # [B,L//4,Nh] -> [B,L//4,16]\n",
    "        output = self.forward(data)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [B,L//4,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[B,L//4,16] -> [B,L//4]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[B,L//4] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1)\n",
    "        return logp\n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h=[torch.zeros([1,B,self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,B,self.Nh],device=self.device)]\n",
    "            #h is h0 and c0\n",
    "        #else:\n",
    "        h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "        #Sample set will have shape [B,L,4]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([B,L+1,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #out should be batch first [B,L,Nh]\n",
    "            out,h=self.rnn(input[:,idx-1:idx,:],h)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(out[:,0,:]).view([B,16])\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            input[:,idx] = sample\n",
    "        #remove the leading zero in the input    \n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input[:,1:],self.Lx).unsqueeze(-1)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch(sample.squeeze(-1),self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            \n",
    "            data[:,1:]=sample[:,:-1]\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            \n",
    "            h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "            \n",
    "            out,_=self.rnn(data,h)\n",
    "            \n",
    "            #cache for the rnn is the output in this sense\n",
    "            #shape [B,L//4,Nh]\n",
    "            cache=out\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(1)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,L//D,1,1).reshape(B*L//D,L//4,cache.shape[-1])\n",
    "                        \n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [B,L//4,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[B,L//4,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4])\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[:,1:]=tmp[:,:-1]\n",
    "                #grab your rnn output\n",
    "                if k==0:\n",
    "                    out,_=self.rnn(fsample,cache[:,0].unsqueeze(0)*0.0)\n",
    "                else:\n",
    "                    out,_=self.rnn(fsample[:,N//4:],cache[:,N//4-1].unsqueeze(0)*1.0)\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out)\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055fad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PatchTransformer(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a207d8",
   "metadata": {},
   "source": [
    "# Testing torch multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e52639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 0, 6, 4, 4, 4, 4], device='cuda:0')\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "B=8\n",
    "batchoptions=p.options.repeat(B,1)\n",
    "sm = batchoptions.reshape([B,16,4])\n",
    "probs = torch.rand([B,16],device=device)\n",
    "#get the probability sum and add it's log to the normalization \n",
    "psum=torch.sum(probs,dim=1).unsqueeze(1)\n",
    "\n",
    "indices = torch.multinomial(probs/psum,1,False)\n",
    "\n",
    "print(indices.squeeze(1))\n",
    "\n",
    "print(p.options[indices.squeeze(1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba1164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16, 3])\n",
      "torch.Size([4, 8, 16, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([4,8,16,3],device=device)\n",
    "\n",
    "print(getcache(x,indices.squeeze(1)).shape)\n",
    "\n",
    "print(getcachealt(x,indices.squeeze(1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c71fc",
   "metadata": {},
   "source": [
    "# Testing getcache functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93889178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst = (torch.eye(16)*(torch.arange(16)+1)).to(device)\n",
    "\n",
    "indices2=torch.arange(16,device=device)\n",
    "print(indices2)\n",
    "plt.imshow(tst.cpu())\n",
    "plt.show()\n",
    "plt.imshow(getcache(tst.view([1,16,16,1]),indices2).view([16,16]).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b226fc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sample(8,4*4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd9745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.logprobability(p.sample(8,4*4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e8a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62a4ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.zeros([1,2,3,4])\n",
    "x.transpose(-1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364d3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0')\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]], device='cuda:0')\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(patch2idx(p.options))\n",
    "\n",
    "print((p.options[patch2idx(p.options)]==p.options))\n",
    "\n",
    "print(patch2onehot(p.options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a184c81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb = PatchedRNN(8)\n",
    "\n",
    "pb.logprobability(pb.sample(12,8*8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177afd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "032d21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(Lx):\n",
    "    sflip = torch.arange(Lx*Lx,device=device).to(torch.int64).reshape([1,Lx,Lx])\n",
    "    sflip = patch(sflip,Lx).reshape(Lx*Lx)\n",
    "    \n",
    "    return sflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f23cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_indices(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c2d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.71480530500412e-07 0.14302759653142283\n",
      "tensor(-10.9516, device='cuda:0') tensor(-10.9516, device='cuda:0')\n",
      "tensor(1.9073e-06, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    B=32\n",
    "\n",
    "    s = pb.sample(B,8*8)\n",
    "    probs = super(PatchedRNN,pb)._off_diag_labels(s,B,8*8,False,D=8)[1][:,get_indices(8)]\n",
    "    \n",
    "    p2 = pb._off_diag_labels(s,B,8*8,False,D=8)[1]\n",
    "\n",
    "    print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "    print(probs.mean(),p2.mean())\n",
    "    print(abs(probs-p2).max())\n",
    "    plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f27c6cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t256\n",
      "Q                             \t\t\t1\n",
      "K                             \t\t\t512\n",
      "B                             \t\t\t512\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.9\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t32\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPRNN\n",
      "Nh                            \t\t\t256\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=16\n",
    "op.L=Lx*Lx\n",
    "op.Nh=256\n",
    "op.lr=5e-4\n",
    "op.M=0.9\n",
    "op.Q=1\n",
    "op.K=512\n",
    "op.USEQUEUE=0\n",
    "op.kl=0.0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "#op.steps=4000\n",
    "op.dir=\"PRNN\"\n",
    "#op.steps=100\n",
    "op.NLOOPS=32\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24754787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(\"python Patched_TF.py \"+op.cmd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad96476d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainsformer = torch.jit.script(PatchedRNN(Lx,Nh=op.Nh))\n",
    "\n",
    "#trainsformer = RNN(Nh=op.Nh)\n",
    "\n",
    "sampleformer= PatchedRNN(Lx,Nh=op.Nh)\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.38052 256\n",
      "1,2.79|"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\")\n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3af53",
   "metadata": {},
   "source": [
    "# Test Class for the patch transformer which makes sure all probabilities are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testPTF(PatchTransformerB):\n",
    "    \"\"\"Just adds some functions which make sure all probability labels are consistent\"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        super(testPTF,self).__init__(Lx,**kwargs)\n",
    "        self.reset(1)\n",
    "#functions below aren't really necessary anymore since there was no issue with masking (they serve to avoid using a mask)\n",
    "    def reset(self,B):\n",
    "        # type: (int) -> Tensor\n",
    "        \"\"\"Setup for an autoregressive transformer\"\"\"\n",
    "        self._input = torch.zeros([self.L+1,B,4],device=self.device)\n",
    "        self._i=1\n",
    "        encoded_input = self.pe(self._input[:self._i,:,:])\n",
    "        output,self._cache = self.next_with_cache(encoded_input,None)\n",
    "        probs=self.lin(output[-1,:,:])\n",
    "        return probs\n",
    "    def getnext(self,vect):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Get probability for the next output in an autoregressive transformer\"\"\"\n",
    "        self._input[self._i]=vect\n",
    "        self._i+=1\n",
    "        encoded_input = self.pe(self._input[:self._i,:,:])\n",
    "        output,self._cache = self.next_with_cache(encoded_input,self._cache)\n",
    "        probs=self.lin(output[-1,:,:])\n",
    "        return probs\n",
    "    @torch.jit.export\n",
    "    def testsample(self,B):\n",
    "        # type: (int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Generate states with their probabilities in logscale\"\"\"\n",
    "        #set up variables\n",
    "        L=self.L\n",
    "        probs=self.reset(B).squeeze(0)\n",
    "        sprobs=torch.zeros([B],device=self.device)\n",
    "        samples = torch.zeros([B,L,4],device=self.device)\n",
    "        with torch.no_grad():\n",
    "          for idx in range(L):\n",
    "            #loop through L sequence elements and generate next in sequence based off of probabilities\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            samples[:,idx] = sample\n",
    "            real=patch2onehot(sample)\n",
    "            total = torch.sum(real*probs,dim=-1)\n",
    "            sprobs+=torch.log(total)\n",
    "            if idx!=L-1: probs = self.getnext(sample)\n",
    "                \n",
    "        return unpatch(samples,self.Lx).unsqueeze(-1),sprobs\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def testlabels(self,samples,B):\n",
    "        # type: (Tensor,int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Get logscale probabilities of all states with one spin flipped at position j\"\"\"\n",
    "        with torch.no_grad():\n",
    "            #print(\"|\",end=\"\")\n",
    "            L=self.L\n",
    "            orig=samples\n",
    "            samples=patch(samples.squeeze(-1),self.Lx)\n",
    "            \n",
    "            logprobs=torch.zeros([B,L*4],device=self.device)\n",
    "            for k in range(L*4):\n",
    "                #loop cross L flipped states (batched)\n",
    "                probs=self.reset(B).squeeze(0)\n",
    "                sprobs = torch.zeros([B],device=self.device)\n",
    "                #loop across sequence\n",
    "                for idx in range(L):\n",
    "                    \n",
    "                    sample = samples[:,idx] \n",
    "                    #kth state is flipped\n",
    "                    if idx==k//4:\n",
    "                        #multiply by 1 as a way to copy the tensor to new memory\n",
    "                        sample=sample*1.0\n",
    "                        sample[:,k%4] = 1-sample[:,k%4]\n",
    "                    real=patch2onehot(sample)\n",
    "                    sprobs+=torch.log(torch.sum(real*probs,dim=-1))\n",
    "                    if idx!=L-1: probs = self.getnext(sample)\n",
    "                logprobs[:,k]=sprobs\n",
    "        return orig,logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd565c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydir = setup_dir(op)\n",
    "#mydir = \"PTF/Rydberg/64-NoQ-B=128-K=128-Nh=128-kl=0.00/0\"#\n",
    "mydir = \"PTF/Rydberg/576-NoQ-B=256-K=256-Nh=128-kl=0.00/0\"\n",
    "\n",
    "op.L=576\n",
    "Lx=24\n",
    "tst=torch.jit.load(mydir+\"/T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ad572",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesTF = testPTF(24,Nh=op.Nh,num_layers=2)\n",
    "tesTF = torch.jit.script(tesTF)\n",
    "momentum_update(0,tesTF,tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f226335",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tesTF.Lx==8:\n",
    "    s,p = tesTF.testsample(8)\n",
    "\n",
    "    print(p)\n",
    "    print(tesTF.logprobability(s))\n",
    "\n",
    "    s2,p2=tesTF.testlabels(s,8)\n",
    "\n",
    "\n",
    "\n",
    "    s3,p3=tesTF._off_diag_labels(s,8,64,False,1)\n",
    "\n",
    "    print(s.shape,s2.shape,p2.shape,p3.shape)\n",
    "\n",
    "    print(torch.sum(p3,dim=-1)/64)\n",
    "    print(torch.sum(p2,dim=-1)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486573b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "op.K=256\n",
    "op.Q=32\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "\n",
    "\n",
    "# Hamiltonian parameters\n",
    "N = op.L   # Total number of atoms\n",
    "V = 7.0     # Strength of Van der Waals interaction\n",
    "Omega = 1.0 # Rabi frequency\n",
    "delta = 1.0 # Detuning \n",
    "\n",
    "if op.hamiltonian==\"Rydberg\":\n",
    "    Lx=Ly=int(op.L**0.5)\n",
    "    op.L=Lx*Ly\n",
    "    h = Rydberg(Lx,Ly,V,Omega,delta)\n",
    "else:\n",
    "    #hope for the best here since there aren't defaults\n",
    "    h = TFIM(op.L,op.h,op.J)\n",
    "\n",
    "\n",
    "E_queue = torch.zeros([op.B],device=device)\n",
    "def fill_queue(net):\n",
    "    for i in range(op.Q):\n",
    "        print(\"|\",end=\"\")\n",
    "        if False:\n",
    "            sample,lp = net.testsample(op.K)\n",
    "            _,probs= net.testlabels(sample,op.K)\n",
    "            sqrtp=probs.mean(dim=1)/2\n",
    "            sump = torch.exp(probs/2-sqrtp.unsqueeze(1)).sum(dim=1)  \n",
    "        else:\n",
    "            sample,sump,sqrtp = net.sample_with_labelsALT(op.K,op.L,grad=False,nloops=144)\n",
    "            with torch.no_grad():\n",
    "                lp=net.logprobability(sample)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            E_i=h.localenergyALT(sample,lp,sump,sqrtp)\n",
    "            E_queue[i*op.K:(i+1)*op.K]=E_i\n",
    "t=time.time()\n",
    "fill_queue(tesTF)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errformat(m,s):\n",
    "    exp = -int(np.floor(np.log(s)/np.log(10)))\n",
    "    print( str(round(m,exp))+\" +/- \"+str(round(s,exp)))\n",
    "\n",
    "var,mean = torch.var_mean(E_queue/op.L)\n",
    "\n",
    "print(h.ground(),op.B)\n",
    "stdv=((var/op.B)**0.5).item()\n",
    "errformat(mean.item(),stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c42b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
