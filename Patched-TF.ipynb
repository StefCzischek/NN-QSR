{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e25b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Ti', major=8, minor=6, total_memory=12287MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "from RNN_QSR import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bcc58",
   "metadata": {},
   "source": [
    "# Reshaping the 2D sequence tensor into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90f6218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "tensor([[[ 0,  1,  6,  7],\n",
      "         [ 2,  3,  8,  9],\n",
      "         [ 4,  5, 10, 11],\n",
      "         [12, 13, 18, 19],\n",
      "         [14, 15, 20, 21],\n",
      "         [16, 17, 22, 23],\n",
      "         [24, 25, 30, 31],\n",
      "         [26, 27, 32, 33],\n",
      "         [28, 29, 34, 35]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n"
     ]
    }
   ],
   "source": [
    "def patch(x,Lx):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    \"\"\"patch your sequence into chunks of 4\"\"\"\n",
    "    #make the input 2D then break it into 2x2 chunks \n",
    "    #afterwards reshape the 2x2 chunks to vectors of size 4 and flatten the 2d bit\n",
    "    return x.view([x.shape[0],Lx,Lx]).unfold(-2,2,2).unfold(-2,2,2).reshape([x.shape[0],Lx*Lx//4,4])\n",
    "\n",
    "def unpatch(x,Lx):\n",
    "    # type: (Tensor,int) -> Tensor\n",
    "    \"\"\"inverse function for patch\"\"\"\n",
    "    # original sequence order can be retrieved by chunking twice more\n",
    "    #in the x-direction you should have chunks of size 2, but in y it should\n",
    "    #be chunks of size Ly//2\n",
    "    return x.unfold(-2,Lx//2,Lx//2).unfold(-2,2,2).reshape([x.shape[0],Lx*Lx])\n",
    "\n",
    "Lx=6\n",
    "a = torch.arange(Lx**2).unsqueeze(0)\n",
    "print(a)\n",
    "print(a.view([Lx,Lx]))\n",
    "b = patch(a,Lx)\n",
    "print(b)\n",
    "c = unpatch(b,Lx)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78462733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PE2D(nn.Module):\n",
    "    #TODO: Positional encoding is wrong because the spins are at index i+1 when we sample and get probabilities\n",
    "    def __init__(self, d_model, Lx,Ly,device,n_encode=None):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        assert (d_model%4==0)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(Lx*Ly, d_model)\n",
    "        \n",
    "        if type(n_encode)==type(None):\n",
    "            n_encode=3*d_model//4\n",
    "        for pos in range(Lx*Ly):\n",
    "            x=pos//Ly\n",
    "            y=pos%Ly\n",
    "            # Only going to fill 3/4 of the matrix so the\n",
    "            # occupation values are preserved\n",
    "            for i in range(0, n_encode, 4):\n",
    "                \n",
    "                #x direction encoding\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(x / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(x / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                #y direction encoding\n",
    "                pe[pos, i+2] = \\\n",
    "                math.sin(y / (10000 ** ((2 * i)/n_encode)))\n",
    "                pe[pos, i + 3] = \\\n",
    "                math.cos(y / (10000 ** ((2 * (i + 1))/n_encode)))\n",
    "                \n",
    "        self.pe = pe.unsqueeze(1).to(device)\n",
    "        self.L=Lx*Ly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.repeat(1,1,self.d_model//4) + self.pe[:x.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c562c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def patch2idx(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return out.to(torch.int64)\n",
    "\n",
    "@torch.jit.script\n",
    "def patch2onehot(patch):\n",
    "    #moving the last dimension to the front\n",
    "    patch=patch.unsqueeze(0).transpose(-1,0).squeeze(-1)\n",
    "    out=torch.zeros(patch.shape[1:],device=patch.device)\n",
    "    for i in range(4):\n",
    "        out+=patch[i]<<i\n",
    "    return nn.functional.one_hot(out.to(torch.int64), num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2567828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerBase(Sampler):#(torch.jit.ScriptModule):\n",
    "    \"\"\"\n",
    "    Base class for the two patch transformer architectures \n",
    "    \n",
    "    Architexture wise this is how a patched transformer works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each patch given it and all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    Outputs should either be size 1 (the probability of the current patch which is input) or size 16 (for 2x2 patches where \n",
    "    the probability represented is of each potential patch)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,device=device,Nh=128,dropout=0.0,num_layers=2,nhead=8,outsize=1, **kwargs):\n",
    "        super(PatchTransformerBase, self).__init__()\n",
    "        #print(nhead)\n",
    "        self.pe = PE2D(Nh, Lx,Lx,device)\n",
    "        self.device=device\n",
    "        #Encoder only transformer\n",
    "        #misinterperetation on encoder made it so this code does not work\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=Nh, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,outsize),\n",
    "                nn.Sigmoid() if outsize==1 else nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "        self.Lx=Lx\n",
    "        self.set_mask(Lx**2//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "        \n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def set_mask(self, L):\n",
    "        # type: (int)\n",
    "        # take the log of a lower triangular matrix\n",
    "        self.L=L\n",
    "        self.mask = torch.log(torch.tril(torch.ones([L,L],device=self.device)))\n",
    "        self.pe.L=L\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input is shape [B,L,1]\n",
    "        # add positional encoding to get shape [B,L,Nh]\n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        input=self.pe(patch(input.squeeze(-1),self.Lx).transpose(1,0))\n",
    "        output = self.transformer(input,self.mask)\n",
    "        output = self.lin(output.transpose(1,0))\n",
    "        return output\n",
    "    \n",
    "    def next_with_cache(self,tgt,cache=None,idx=-1):\n",
    "        # type: (Tensor,Optional[Tensor],int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Efficiently calculates the next output of a transformer given the input sequence and \n",
    "        cached intermediate layer encodings of the input sequence\n",
    "        \n",
    "        Inputs:\n",
    "            tgt - Tensor of shape [L,B,1]\n",
    "            cache - Tensor of shape ?\n",
    "            idx - index from which to start\n",
    "            \n",
    "        Outputs:\n",
    "            output - Tensor of shape [?,B,1]\n",
    "            new_cache - Tensor of shape ?\n",
    "        \"\"\"\n",
    "        #HMMM\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i,layer in enumerate(self.transformer.layers):\n",
    "            \n",
    "            tgt=output\n",
    "            #have to merge the functions into one\n",
    "            src = tgt[idx:, :, :]\n",
    "            mask = None if idx==-1 else self.mask[idx:]\n",
    "\n",
    "            # self attention part\n",
    "            src2 = layer.self_attn(\n",
    "                src,#only do attention with the last elem of the sequence\n",
    "                tgt,\n",
    "                tgt,\n",
    "                attn_mask=mask,  \n",
    "                key_padding_mask=None,\n",
    "            )[0]\n",
    "            #straight from torch transformer encoder code\n",
    "            src = src + layer.dropout1(src2)\n",
    "            src = layer.norm1(src)\n",
    "            src2 = layer.linear2(layer.dropout(layer.activation(layer.linear1(src))))\n",
    "            src = src + layer.dropout2(src2)\n",
    "            src = layer.norm2(src)\n",
    "            #return src\n",
    "            \n",
    "            output = src#self.next_attn(output,layer,idx)\n",
    "            new_token_cache.append(output)\n",
    "            if cache is not None:\n",
    "                #layers after layer 1 need to use a cache of the previous layer's output on each input\n",
    "                output = torch.cat([cache[i], output], dim=0)\n",
    "\n",
    "        #update cache with new output\n",
    "        if cache is not None:\n",
    "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
    "        else:\n",
    "            new_cache = torch.stack(new_token_cache, dim=0)\n",
    "\n",
    "        return output, new_cache\n",
    "    \n",
    "    def make_cache(self,tgt):\n",
    "        output = tgt\n",
    "        new_token_cache = []\n",
    "        #go through each layer and apply self attention only to the last input\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            output = layer(output,src_mask=self.mask)#self.next_attn(output,layer,0)\n",
    "            new_token_cache.append(output)\n",
    "        #create cache with tensor\n",
    "        new_cache = torch.stack(new_token_cache, dim=0)\n",
    "        return output, new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b4525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformer(PatchTransformerBase):\n",
    "    \"\"\"Note: logprobability is not normalized \n",
    "    the cost of normalization is 16x the cost of getting un normalized labels due to the output being size 1\n",
    "    \n",
    "    Note: To actually make sure the probability is normalized at each step you would need to run the network on all 16 \n",
    "         possible 2x2 patches at each step.\n",
    "         \n",
    "    This is done in sample, where you feed it your batch x 16 samples (one for each possible patch), get your probabilities\n",
    "    of each patch, sum them to normalize, then sample from your distribution. Once your sample is chosen, you can just\n",
    "    copy the sample that was chosen,as well as your cached self attention 15 samples / cached portions that weren't chosen.\n",
    "    \n",
    "    \n",
    "    You probably want to add in a term which asks normalization to be 1 in training, as if this doesn't hold then the sample \n",
    "    probability and label probability will be different which is very bad.\n",
    "    \n",
    "    An alternate form of this network (still in production) could have 16 outputs for probability instead of 1, and\n",
    "    give the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    This model would always be normalized (just use softmax on the output) but total probabilities would be a bit more\n",
    "    difficult to calculate as you have to match each patch to it's respective output (of the 16 total probability outputs)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx, **kwargs):\n",
    "        #only important bit is that outsize = 1\n",
    "        super(PatchTransformer, self).__init__(Lx,outsize=1,**kwargs)\n",
    "        \n",
    "    def logprobability(self,input):\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        total = self.forward(input)\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1).squeeze(1)\n",
    "        return logp   \n",
    "    \n",
    "    def sample(self,B,L):\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        #transformers don't do batch first so to save a bunch of transpose calls \n",
    "        input = torch.zeros([L,B*16,4],device=self.device)\n",
    "        #self.set_mask(L)\n",
    "        batchoptions=self.options.repeat(B,1)\n",
    "        cache=None\n",
    "        \n",
    "        normalization=torch.zeros([B],device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          for idx in range(L):\n",
    "            \n",
    "            input[idx] = batchoptions\n",
    "            \n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx+1,:,:])\n",
    "                        \n",
    "            \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "            \n",
    "            #get the probability sum and add it's log to the normalization \n",
    "            psum=torch.sum(probs,dim=1)\n",
    "            normalization+=torch.log(psum)\n",
    "            \n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs/psum.unsqueeze(1),1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample.view([B,1,4]).repeat(1,16,1).view([B*16,4])\n",
    "            \n",
    "            #set cache to the samples that are used\n",
    "            #cache is shape [?,L,B*16,Nh]\n",
    "            \n",
    "            \n",
    "            tmp = cache[:,idx].view(cache.shape[0],B,16,cache.shape[-1])\n",
    "            \n",
    "            tmp[:] = getcache(tmp,indices) #make sure this sets things properly\n",
    "            # tmp should still be a view of cache[:,idx] (same memory)\n",
    "        self.normalization = normalization\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input.view([L,B,16,4])[:,:,0].transpose(1,0),self.Lx).unsqueeze(-1)\n",
    "    \n",
    "@torch.jit.script\n",
    "def getcachealt(oldcache,indices):\n",
    "    sx,B,_,sy=oldcache.shape\n",
    "    newcache = oldcache*1\n",
    "    for i in range(B):\n",
    "        #print(oldcache[:,i,indices[i]].shape,newcache.shape)\n",
    "        newcache[:,i,:]=oldcache[:,i,indices[i]].unsqueeze(1)\n",
    "    return newcache\n",
    "    \n",
    "    \n",
    "@torch.jit.script\n",
    "def getcache(oldcache,indices):\n",
    "    \"\"\"Supposed to take the cache at the selected index and copy it to the other 15\"\"\"\n",
    "    sx,B,_,sy=oldcache.shape\n",
    "    newcache = oldcache*0\n",
    "    for i in range(16):\n",
    "        #add the cache at position i only if it is the right index???\n",
    "        tmp=(oldcache[:,:,i]*((indices==i).view(1,B,1))).view(sx,B,1,sy)\n",
    "        newcache+=tmp\n",
    "    return newcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2311e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTransformerB(PatchTransformerBase):\n",
    "    \"\"\"Note: logprobability IS normalized \n",
    "    \n",
    "    Architexture wise this is how it works:\n",
    "    \n",
    "    You give it a (2D) state and it patches it into groups of 4 (think of a 2x2 cnn filter with stride 2). It then tells you\n",
    "    the probability of each potential patch given all previous patches in your sequence using masked attention.\n",
    "    \n",
    "    \n",
    "    This model has 16 outputs, which describes the probability distrubition for the nth patch when given the first n-1 patches\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        #only important bit is that outsize = 16\n",
    "        super(PatchTransformerB, self).__init__(Lx,outsize=16,**kwargs)\n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "        \n",
    "        if input.shape[1]//4!=self.L:\n",
    "            self.set_mask(input.shape[1]//4)\n",
    "        #pe should be sequence first [L,B,Nh]\n",
    "        \n",
    "        #shape is modified to [L//4,B,4]\n",
    "        input = patch(input.squeeze(-1),self.Lx).transpose(1,0)\n",
    "        \n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        data[1:]=input[:-1]\n",
    "        \n",
    "        #[L//4,B,4] -> [L//4,B,Nh]\n",
    "        encoded=self.pe(data)\n",
    "        #shape is preserved\n",
    "        output = self.transformer(encoded,self.mask)\n",
    "        # [L//4,B,Nh] -> [L//4,B,16]\n",
    "        output = self.lin(output)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [L//4,B,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[L//4,B,16] -> [L//4,B]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[L//4,B] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=0)\n",
    "        return logp   \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        \n",
    "        #return (torch.rand([B,L,1],device=device)<0.5).to(torch.float32)\n",
    "        #Sample set will have shape [B,L,1]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([L+1,B,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            \n",
    "            #pe should be sequence first [L,B,Nh]\n",
    "            encoded_input = self.pe(input[:idx,:,:])\n",
    "                        \n",
    "            #Get transformer output\n",
    "            output,cache = self.next_with_cache(encoded_input,cache)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(output[-1,:,:]).view([B,16])\n",
    "\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            \n",
    "            #set input to the sample that was actually chosen\n",
    "            input[idx] = sample\n",
    "            \n",
    "        #remove the leading zero in the input    \n",
    "        input=input[1:]\n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input.transpose(1,0),self.Lx).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch(sample.squeeze(-1),self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #switch sample into sequence-first\n",
    "        sample = sample.transpose(1,0)\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            data[1:]=sample[:-1]\n",
    "            \n",
    "            #[L//4,B,4] -> [L//4,B,Nh]\n",
    "            encoded=self.pe(data)\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            out,cache=self.make_cache(encoded)\n",
    "\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(2)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,1,L//D,1,1).transpose(2,3).reshape(cache.shape[0],L//4,B*L//D,cache.shape[-1])\n",
    "\n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [L//4,B,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[L//4,B,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1).transpose(1,0)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4]).transpose(1,0)\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[1:]=tmp[:-1]\n",
    "                # put sequence before batch so you can use it with your transformer\n",
    "                tgt=self.pe(fsample)\n",
    "                #grab your transformer output\n",
    "                out,_=self.next_with_cache(tgt,cache[:,:N//4],N//4)\n",
    "\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out[N//4:].transpose(1,0))\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                \n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs\n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e37fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchedRNN(Sampler):\n",
    "    TYPES={\"GRU\":nn.GRU,\"ELMAN\":nn.RNN,\"LSTM\":nn.LSTM}\n",
    "    def __init__(self,Lx,rnntype=\"GRU\",Nh=128,device=device, **kwargs):\n",
    "        super(PatchedRNN, self).__init__(device=device)\n",
    "        \n",
    "        \n",
    "        assert rnntype!=\"LSTM\"\n",
    "        #rnn takes input shape [B,L,1]\n",
    "        self.rnn = RNN.TYPES[rnntype](input_size=4,hidden_size=Nh,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "                nn.Linear(Nh,Nh),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(Nh,16),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "        self.Nh=Nh\n",
    "        self.rnntype=rnntype\n",
    "        \n",
    "        \n",
    "        self.Lx=Lx\n",
    "        self.L = (Lx**2//4)\n",
    "        \n",
    "        self.options=torch.zeros([16,4],device=self.device)\n",
    "        tmp=torch.arange(16,device=self.device)\n",
    "        for i in range(4):\n",
    "            self.options[:,i]=(tmp>>i)%2\n",
    "            \n",
    "        \n",
    "        self.to(device)\n",
    "    def forward(self, input):\n",
    "        # h0 is shape [d*numlayers,B,H] but D=numlayers=1 so\n",
    "        # h0 has shape [1,B,H]\n",
    "        \n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h0=[torch.zeros([1,input.shape[0],self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,input.shape[0],self.Nh],device=self.device)]\n",
    "            #h0 and c0\n",
    "        #else:\n",
    "        h0=torch.zeros([1,input.shape[0],self.Nh],device=self.device)\n",
    "        out,h=self.rnn(input,h0)\n",
    "        return self.lin(out)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def logprobability(self,input):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Compute the logscale probability of a given state\n",
    "            Inputs:\n",
    "                input - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            Returns:\n",
    "                logp - [B] size vector of logscale probability labels\n",
    "        \"\"\"\n",
    "                \n",
    "        #shape is modified to [B,L//4,4]\n",
    "        input = patch(input.squeeze(-1),self.Lx)\n",
    "        data=torch.zeros(input.shape,device=self.device)\n",
    "        #batch first\n",
    "        data[:,1:]=input[:,:-1]\n",
    "        # [B,L//4,Nh] -> [B,L//4,16]\n",
    "        output = self.forward(data)\n",
    "        \n",
    "        #real is going to be a onehot with the index of the appropriate patch set to 1\n",
    "        #shape will be [B,L//4,16]\n",
    "        real=patch2onehot(input)\n",
    "        \n",
    "        #[B,L//4,16] -> [B,L//4]\n",
    "        total = torch.sum(real*output,dim=-1)\n",
    "        #[B,L//4] -> [B]\n",
    "        logp=torch.sum(torch.log(total+1e-10),dim=1)\n",
    "        return logp\n",
    "    @torch.jit.export\n",
    "    def sample(self,B,L,cache=None):\n",
    "        # type: (int,int,Optional[Tensor]) -> Tensor\n",
    "        \"\"\" Generates a set states\n",
    "        Inputs:\n",
    "            B (int)            - The number of states to generate in parallel\n",
    "            L (int)            - The length of generated vectors\n",
    "        Returns:\n",
    "            samples - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "        \"\"\"\n",
    "        #length is divided by four due to patching\n",
    "        L=L//4\n",
    "        #if self.rnntype==\"LSTM\":\n",
    "        #    h=[torch.zeros([1,B,self.Nh],device=self.device),\n",
    "        #       torch.zeros([1,B,self.Nh],device=self.device)]\n",
    "            #h is h0 and c0\n",
    "        #else:\n",
    "        h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "        #Sample set will have shape [B,L,4]\n",
    "        #need one extra zero batch at the start for first pred hence input is [L+1,B,1] \n",
    "        input = torch.zeros([B,L+1,4],device=self.device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "          for idx in range(1,L+1):\n",
    "            #out should be batch first [B,L,Nh]\n",
    "            out,h=self.rnn(input[:,idx-1:idx,:],h)\n",
    "            #check out the probability of all 16 vectors\n",
    "            probs=self.lin(out[:,0,:]).view([B,16])\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            input[:,idx] = sample\n",
    "        #remove the leading zero in the input    \n",
    "        #sample is repeated 16 times at 3rd index so we just take the first one\n",
    "        return unpatch(input[:,1:],self.Lx).unsqueeze(-1)\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def sample_with_labelsALT(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor,Tensor]\n",
    "        sample,probs = self.sample_with_labels(B,L,grad,nloops)\n",
    "        logsqrtp=probs.mean(dim=1)/2\n",
    "        sumsqrtp = torch.exp(probs/2-logsqrtp.unsqueeze(1)).sum(dim=1)\n",
    "        return sample,sumsqrtp,logsqrtp\n",
    "    @torch.jit.export\n",
    "    def sample_with_labels(self,B,L,grad=False,nloops=1):\n",
    "        # type: (int,int,bool,int) -> Tuple[Tensor,Tensor]\n",
    "        sample=self.sample(B,L,None)\n",
    "        return self._off_diag_labels(sample,B,L,grad,nloops)\n",
    "    \n",
    "    \n",
    "    @torch.jit.export\n",
    "    def _off_diag_labels(self,sample,B,L,grad,D=1):\n",
    "        # type: (Tensor,int,int,bool,int) -> Tuple[Tensor, Tensor]\n",
    "        \"\"\"label all of the flipped states  - set D as high as possible without it slowing down runtime\n",
    "        Parameters:\n",
    "            sample - [B,L,1] matrix of zeros and ones for ground/excited states\n",
    "            B,L (int) - batch size and sequence length\n",
    "            D (int) - Number of partitions sequence-wise. We must have L%D==0 (D divides L)\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "            sample - same as input\n",
    "            probs - [B,L] matrix of probabilities of states with the jth excitation flipped\n",
    "        \"\"\"\n",
    "        sample0=sample\n",
    "        #sample is batch first at the moment\n",
    "        sample = patch(sample.squeeze(-1),self.Lx)\n",
    "        \n",
    "        sflip = torch.zeros([B,L,L//4,4],device=self.device)\n",
    "        #collect all of the flipped states into one array\n",
    "        for j in range(L//4):\n",
    "            #have to change the order of in which states are flipped for the cache to be useful\n",
    "            for j2 in range(4):\n",
    "                sflip[:,j*4+j2] = sample*1.0\n",
    "                sflip[:,j*4+j2,j,j2] = 1-sflip[:,j*4+j2,j,j2]\n",
    "            \n",
    "        #compute all of their logscale probabilities\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            data=torch.zeros(sample.shape,device=self.device)\n",
    "            \n",
    "            data[:,1:]=sample[:,:-1]\n",
    "            \n",
    "            #add positional encoding and make the cache\n",
    "            \n",
    "            h=torch.zeros([1,B,self.Nh],device=self.device)\n",
    "            \n",
    "            out,_=self.rnn(data,h)\n",
    "            \n",
    "            #cache for the rnn is the output in this sense\n",
    "            #shape [B,L//4,Nh]\n",
    "            cache=out\n",
    "            probs=torch.zeros([B,L],device=self.device)\n",
    "            #expand cache to group L//D flipped states\n",
    "            cache=cache.unsqueeze(1)\n",
    "\n",
    "            #this line took like 1 hour to write I'm so sad\n",
    "            #the cache has to be shaped such that the batch parts line up\n",
    "                        \n",
    "            cache=cache.repeat(1,L//D,1,1).reshape(B*L//D,L//4,cache.shape[-1])\n",
    "                        \n",
    "            pred0 = self.lin(out)\n",
    "            #shape will be [B,L//4,16]\n",
    "            real=patch2onehot(sample)\n",
    "            #[B,L//4,16] -> [B,L//4]\n",
    "            total0 = torch.sum(real*pred0,dim=-1)\n",
    "\n",
    "            for k in range(D):\n",
    "\n",
    "                N = k*L//D\n",
    "                #next couple of steps are crucial          \n",
    "                #get the samples from N to N+L//D\n",
    "                #Note: samples are the same as the original up to the Nth spin\n",
    "                real = sflip[:,N:(k+1)*L//D]\n",
    "                #flatten it out and set to sequence first\n",
    "                tmp = real.reshape([B*L//D,L//4,4])\n",
    "                #set up next state predction\n",
    "                fsample=torch.zeros(tmp.shape,device=self.device)\n",
    "                fsample[:,1:]=tmp[:,:-1]\n",
    "                #grab your rnn output\n",
    "                if k==0:\n",
    "                    out,_=self.rnn(fsample,cache[:,0].unsqueeze(0)*0.0)\n",
    "                else:\n",
    "                    out,_=self.rnn(fsample[:,N//4:],cache[:,N//4-1].unsqueeze(0)*1.0)\n",
    "                # grab output for the new part\n",
    "                output = self.lin(out)\n",
    "                # reshape output separating batch from spin flip grouping\n",
    "                pred = output.view([B,L//D,(L-N)//4,16])\n",
    "                real = patch2onehot(real[:,:,N//4:])\n",
    "                total = torch.sum(real*pred,dim=-1)\n",
    "                #sum across the sequence for probabilities\n",
    "                #print(total.shape,total0.shape)\n",
    "                logp=torch.sum(torch.log(total+1e-10),dim=-1)\n",
    "                logp+=torch.sum(torch.log(total0[:,:N//4]+1e-10),dim=-1).unsqueeze(-1)\n",
    "                probs[:,N:(k+1)*L//D]=logp\n",
    "                \n",
    "        return sample0,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055fad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PatchTransformer(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a207d8",
   "metadata": {},
   "source": [
    "# Testing torch multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e52639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  9,  2,  8,  1,  7,  0,  3], device='cuda:0')\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "B=8\n",
    "batchoptions=p.options.repeat(B,1)\n",
    "sm = batchoptions.reshape([B,16,4])\n",
    "probs = torch.rand([B,16],device=device)\n",
    "#get the probability sum and add it's log to the normalization \n",
    "psum=torch.sum(probs,dim=1).unsqueeze(1)\n",
    "\n",
    "indices = torch.multinomial(probs/psum,1,False)\n",
    "\n",
    "print(indices.squeeze(1))\n",
    "\n",
    "print(p.options[indices.squeeze(1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba1164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16, 3])\n",
      "torch.Size([4, 8, 16, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([4,8,16,3],device=device)\n",
    "\n",
    "print(getcache(x,indices.squeeze(1)).shape)\n",
    "\n",
    "print(getcachealt(x,indices.squeeze(1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c71fc",
   "metadata": {},
   "source": [
    "# Testing getcache functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93889178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANWUlEQVR4nO3dbcxkZX3H8e9v73sXdnlwF6iIQAsaSwLUVrKhPjTWlGKQGrBJX2BqS6sJMSmtNjWKIam+rLW1j0ZDlZa2RJIqVmKwhVCNaVOIy3Z5clUeSnVlBa0WBAK76/77Yg7NcHPf7OycM4fdXt9PspmHc91z/fea+c05c+acuVJVSGrPuhe6AEkvDMMvNcrwS40y/FKjDL/UqOUxO9uQI+pIjhqzS6kpT/EEe+rpzNJ21PAfyVH8bM4bs0upKbfVLTO3dbNfapThlxrVK/xJLkjy9ST3JbliqKIkLd7c4U+yBHwUeBNwJvDWJGcOVZikxeqz5j8XuK+qHqiqPcB1wMXDlCVp0fqE/2TgW1O3d3X3PUuSy5JsS7JtL0/36E7SkPqEf7XvEp9zimBVXVVVW6tq63qO6NGdpCH1Cf8u4NSp26cAD/UrR9JY+oT/K8ArkpyeZANwCXDDMGVJWrS5j/Crqn1JLgf+GVgCrq6qewarTNJC9Tq8t6puBG4cqBZJI/IIP6lRo57Yk/XLLJ9w4phdSk3J92aPtGt+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRo16Yg/Ly+x/8ZZRu5Sa8j+e2CPpAAy/1CjDLzWqz4w9pyb5YpKdSe5J8q4hC5O0WH12+O0Dfq+qtic5Brg9yc1V9dWBapO0QHOv+atqd1Vt767/ENjJKjP2SDo0DfJVX5LTgFcBt62y7DLgMoAj1x87RHeSBtB7h1+So4HPAO+uqsdWLp+ermvD8lF9u5M0kF7hT7KeSfCvrarrhylJ0hj67O0P8ElgZ1V9ZLiSJI2hz5r/dcCvAb+QZEf378KB6pK0YH3m6vtXVp+mW9JhwCP8pEaNelbf/uV17DnBPf7Soux/YPb1uWt+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRo16Yk8th6eOH3eGMKkltTz7Wfau+aVGGX6pUYZfatQQP929lOQ/knx+iIIkjWOINf+7mMzWI+kw0vd3+08Bfgn4xDDlSBpL3zX/nwLvBfb3L0XSmPpM2vFm4JGquv0A7S5Lsi3Jtr1PPz5vd5IG1nfSjouSPAhcx2Tyjr9f2Wh6rr71RxzdoztJQ+ozRff7q+qUqjoNuAT4l6p622CVSVoov+eXGjXIgfZV9SXgS0M8lqRxuOaXGjXudF1L8NQW32+kRdm/NHtbkyg1yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81auS5+uDpLbPPJSbp4NRBJNo1v9Qowy81yvBLjeo7Y8/mJJ9O8rUkO5O8ZqjCJC1W3x1+fwb8U1X9SpINwKYBapI0grnDn+RY4PXAbwBU1R5gzzBlSVq0Ppv9LwO+C/x1N0X3J5IctbLR9HRd+558okd3kobUJ/zLwDnAx6rqVcATwBUrG01P17W86TnvDZJeIH3CvwvYVVW3dbc/zeTNQNJhoM9cfd8BvpXkjO6u84CvDlKVpIXru7f/t4Fruz39DwC/2b8kSWPoFf6q2gFsHaYUSWMa98SeJdizucbsUmpKOV2XpAMx/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40a+ay+Yu/mH43ZpdSUWpr9rFnX/FKjDL/UKMMvNarvdF2/m+SeJHcn+VSSI4cqTNJizR3+JCcDvwNsraqzgSXgkqEKk7RYfTf7l4GNSZaZzNP3UP+SJI2hz+/2fxv4I+CbwG7g0aq6aWW76em6fvS403VJh4o+m/1bgIuB04GXAkcledvKdtPTdS0d7XRd0qGiz2b/LwL/WVXfraq9wPXAa4cpS9Ki9Qn/N4FXJ9mUJEym69o5TFmSFq3PZ/7bmEzOuR24q3usqwaqS9KC9Z2u6wPABwaqRdKIPMJPatSoZ/WxVCy/aM+oXUpN8aw+SQdi+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfapThlxpl+KVGGX6pUYZfatSoJ/YsL+/n+M2Pj9ml1JTvLO+fua1rfqlRhl9qlOGXGnXA8Ce5OskjSe6euu+4JDcnube73LLYMiUNbZY1/98AF6y47wrglqp6BXBLd1vSYeSA4a+qLwPfX3H3xcA13fVrgLcMW5akRZv3M/+JVbUboLt88VoNp6fr2vfok3N2J2loC9/hNz1d1/KLNi26O0kzmjf8Dyc5CaC7fGS4kiSNYd7w3wBc2l2/FPjcMOVIGsssX/V9Cvh34Iwku5K8A/gD4Pwk9wLnd7clHUYOeGx/Vb11jUXnDVyLpBF5hJ/UqFHP6tuwbh8/fuwPxuxSasq96/bN3NY1v9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqNGPbHnyKV9/OTR/uiPtCj/tuSJPZIOwPBLjTL8UqPmna7rw0m+luTOJJ9NsnmhVUoa3LzTdd0MnF1VrwS+Abx/4LokLdhc03VV1U1V9cxuxVuBUxZQm6QFGuIz/9uBL6y1cHq6rid/8PQA3UkaQq/wJ7kS2Adcu1ab6em6Nm05ok93kgY090E+SS4F3gycV1U1XEmSxjBX+JNcALwP+Pmqcupd6TA073RdfwkcA9ycZEeSjy+4TkkDm3e6rk8uoBZJI/IIP6lRo57Vt3HdHs7euGvMLqWmbFy3Z+a2rvmlRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRo17Vl/28lNHPDRml1JTNmbvzG1d80uNMvxSo+aarmtq2XuSVJITFlOepEWZd7oukpwKnA98c+CaJI1grum6On8CvBfwN/ulw9Bcn/mTXAR8u6rumKHt/03X9YPv75+nO0kLcNBf9SXZBFwJvHGW9lV1FXAVwFmv3OBWgnSImGfN/3LgdOCOJA8ymaF3e5KXDFmYpMU66DV/Vd0FvPiZ290bwNaq+t6AdUlasHmn65J0mJt3uq7p5acNVo2k0XiEn9SokU/sWcdZGzaO2aXUlI2ZfX3uml9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qVKrG+1m9JN8F/muNxScAh8KvAVnHs1nHsx3qdfxEVf3YLA8wavifT5JtVbXVOqzDOsapw81+qVGGX2rUoRT+q17oAjrW8WzW8Wz/b+o4ZD7zSxrXobTmlzQiwy81atTwJ7kgydeT3JfkilWWJ8mfd8vvTHLOAmo4NckXk+xMck+Sd63S5g1JHk2yo/v3+0PXMdXXg0nu6vrZtsryhY5JkjOm/p87kjyW5N0r2ixsPJJcneSRJHdP3XdckpuT3Ntdblnjb5/39TRAHR9O8rVu3D+bZPMaf/u8z+EAdXwwybenxv/CNf724Majqkb5BywB9wMvAzYAdwBnrmhzIfAFIMCrgdsWUMdJwDnd9WOAb6xSxxuAz480Lg8CJzzP8oWPyYrn6DtMDhQZZTyA1wPnAHdP3feHwBXd9SuAD83zehqgjjcCy931D61WxyzP4QB1fBB4zwzP3UGNx5hr/nOB+6rqgaraA1wHXLyizcXA39bErcDmJCcNWURV7a6q7d31HwI7gZOH7GNgCx+TKecB91fVWkdhDq6qvgx8f8XdFwPXdNevAd6yyp/O8nrqVUdV3VRV+7qbtzKZlHah1hiPWRz0eIwZ/pOBb03d3sVzQzdLm8EkOQ14FXDbKotfk+SOJF9IctaiagAKuCnJ7UkuW2X5mGNyCfCpNZaNNR4AJ1bVbpi8WTM1MeyUUV8rwNuZbIGt5kDP4RAu7z5+XL3Gx6CDHo8xw59V7lv5PeMsbQaR5GjgM8C7q+qxFYu3M9n0/WngL4B/XEQNnddV1TnAm4DfSvL6laWu8jeDj0mSDcBFwD+ssnjM8ZjVmK+VK4F9wLVrNDnQc9jXx4CXAz8D7Ab+eLUyV7nvecdjzPDvAk6dun0K8NAcbXpLsp5J8K+tqutXLq+qx6rq8e76jcD6JCcMXUf3+A91l48An2Wy+TZtlDFh8sLdXlUPr1LjaOPRefiZjzbd5SOrtBnrtXIp8GbgV6v7cL3SDM9hL1X1cFX9qKr2A3+1xuMf9HiMGf6vAK9Icnq3lrkEuGFFmxuAX+/2cL8aePSZzb+hJAnwSWBnVX1kjTYv6dqR5Fwm4/TfQ9bRPfZRSY555jqTHUx3r2i28DHpvJU1NvnHGo8pNwCXdtcvBT63SptZXk+9JLkAeB9wUVU9uUabWZ7DvnVM7+P55TUe/+DHY4g9lAexJ/NCJnvX7weu7O57J/DO7nqAj3bL7wK2LqCGn2OyOXQnsKP7d+GKOi4H7mGyx/RW4LULGo+XdX3c0fX3Qo3JJiZhftHUfaOMB5M3nN3AXiZrr3cAxwO3APd2l8d1bV8K3Ph8r6eB67iPyefoZ14nH19Zx1rP4cB1/F333N/JJNAnDTEeHt4rNcoj/KRGGX6pUYZfapThlxpl+KVGGX6pUYZfatT/AguH3ZgjQFwcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst = (torch.eye(16)*(torch.arange(16)+1)).to(device)\n",
    "\n",
    "indices2=torch.arange(16,device=device)\n",
    "print(indices2)\n",
    "plt.imshow(tst.cpu())\n",
    "plt.show()\n",
    "plt.imshow(getcache(tst.view([1,16,16,1]),indices2).view([16,16]).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b226fc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sample(8,4*4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd9745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.logprobability(p.sample(8,4*4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e8a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62a4ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.zeros([1,2,3,4])\n",
    "x.transpose(-1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364d3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0')\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]], device='cuda:0')\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(patch2idx(p.options))\n",
    "\n",
    "print((p.options[patch2idx(p.options)]==p.options))\n",
    "\n",
    "print(patch2onehot(p.options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a184c81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb = PatchedRNN(8)\n",
    "\n",
    "pb.logprobability(pb.sample(12,8*8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177afd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "032d21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(Lx):\n",
    "    sflip = torch.arange(Lx*Lx,device=device).to(torch.int64).reshape([1,Lx,Lx])\n",
    "    sflip = patch(sflip,Lx).reshape(Lx*Lx)\n",
    "    \n",
    "    return sflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f23cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_indices(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c2d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.181475818157196e-05 0.2242434963167789\n",
      "tensor(-44.2986, device='cuda:0') tensor(-44.2986, device='cuda:0')\n",
      "tensor(8.3923e-05, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    B=32\n",
    "\n",
    "    s = pb.sample(B,8*8)\n",
    "    probs = super(PatchedRNN,pb)._off_diag_labels(s,B,8*8,False,D=8)[1][:,get_indices(8)]\n",
    "    \n",
    "    p2 = pb._off_diag_labels(s,B,8*8,False,D=8)[1]\n",
    "\n",
    "    print(abs(probs-p2).mean().item(),torch.var_mean(probs)[0].item()**0.5)\n",
    "    print(probs.mean(),p2.mean())\n",
    "    print(abs(probs-p2).max())\n",
    "    plt.imshow(abs(probs-p2).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27c6cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L                             \t\t\t1024\n",
      "Q                             \t\t\t2\n",
      "K                             \t\t\t128\n",
      "B                             \t\t\t256\n",
      "TOL                           \t\t\t0.15\n",
      "M                             \t\t\t0.9\n",
      "USEQUEUE                      \t\t\t0\n",
      "NLOOPS                        \t\t\t256\n",
      "hamiltonian                   \t\t\tRydberg\n",
      "steps                         \t\t\t12000\n",
      "dir                           \t\t\tPTF\n",
      "Nh                            \t\t\t128\n",
      "lr                            \t\t\t0.0005\n",
      "kl                            \t\t\t0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op=Opt()\n",
    "Lx=32\n",
    "op.L=Lx*Lx\n",
    "op.Nh=128\n",
    "op.lr=5e-4\n",
    "op.M=0.9\n",
    "op.Q=2\n",
    "op.K=128\n",
    "op.USEQUEUE=0\n",
    "op.kl=0.0\n",
    "#op.apply(sys.argv[1:])\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "#op.steps=4000\n",
    "op.dir=\"PTF\"\n",
    "#op.steps=100\n",
    "op.NLOOPS=256\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24754787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(\"python Patched_TF.py \"+op.cmd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad96476d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_recursive.py:229: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  \"Consider removing it.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "trainsformer = torch.jit.script(PatchTransformerB(Lx,Nh=op.Nh,num_layers=2))\n",
    "\n",
    "#trainsformer = torch.jit.script(PatchedRNN(Lx,Nh=op.Nh))\n",
    "#trainsformer = RNN(Nh=op.Nh)\n",
    "#sampleformer= PatchedRNN(Lx,Nh=op.Nh)\n",
    "\n",
    "beta1=0.9;beta2=0.999\n",
    "optimizer = torch.optim.Adam(\n",
    "trainsformer.parameters(), \n",
    "lr=op.lr, \n",
    "betas=(beta1,beta2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "985e6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797648\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.numel() for p in trainsformer.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34a36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training. . .\n",
      "Output folder path established\n",
      "-0.3687 1024\n",
      "17,3.05|\n",
      "7199,-0.33|14369,-0.33|21510,-0.34|28656,-0.34|35800,-0.37|42952,-0.37|50104,-0.37|57257,-0.37|\n",
      "64409,-0.37|71543,-0.37|78675,-0.37|85817,-0.37|92970,-0.37|100117,-0.37|107259,-0.37|114403,-0.37|\n",
      "121550,-0.37|128699,-0.37|135847,-0.37|142995,-0.37|150136,-0.37|157267,-0.37|164409,-0.37|171532.01886630058 12000\n"
     ]
    }
   ],
   "source": [
    "if op.USEQUEUE:\n",
    "    queue_train(op,(trainsformer,sampleformer,optimizer))\n",
    "else:\n",
    "    print(\"Training. . .\") \n",
    "    reg_train(op,(trainsformer,optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66bf7a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9e1622b385b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3af53",
   "metadata": {},
   "source": [
    "# Test Class for the patch transformer which makes sure all probabilities are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testPTF(PatchTransformerB):\n",
    "    \"\"\"Just adds some functions which make sure all probability labels are consistent\"\"\"\n",
    "    def __init__(self,Lx,**kwargs):\n",
    "        super(testPTF,self).__init__(Lx,**kwargs)\n",
    "        self.reset(1)\n",
    "#functions below aren't really necessary anymore since there was no issue with masking (they serve to avoid using a mask)\n",
    "    def reset(self,B):\n",
    "        # type: (int) -> Tensor\n",
    "        \"\"\"Setup for an autoregressive transformer\"\"\"\n",
    "        self._input = torch.zeros([self.L+1,B,4],device=self.device)\n",
    "        self._i=1\n",
    "        encoded_input = self.pe(self._input[:self._i,:,:])\n",
    "        output,self._cache = self.next_with_cache(encoded_input,None)\n",
    "        probs=self.lin(output[-1,:,:])\n",
    "        return probs\n",
    "    def getnext(self,vect):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        \"\"\"Get probability for the next output in an autoregressive transformer\"\"\"\n",
    "        self._input[self._i]=vect\n",
    "        self._i+=1\n",
    "        encoded_input = self.pe(self._input[:self._i,:,:])\n",
    "        output,self._cache = self.next_with_cache(encoded_input,self._cache)\n",
    "        probs=self.lin(output[-1,:,:])\n",
    "        return probs\n",
    "    @torch.jit.export\n",
    "    def testsample(self,B):\n",
    "        # type: (int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Generate states with their probabilities in logscale\"\"\"\n",
    "        #set up variables\n",
    "        L=self.L\n",
    "        probs=self.reset(B).squeeze(0)\n",
    "        sprobs=torch.zeros([B],device=self.device)\n",
    "        samples = torch.zeros([B,L,4],device=self.device)\n",
    "        with torch.no_grad():\n",
    "          for idx in range(L):\n",
    "            #loop through L sequence elements and generate next in sequence based off of probabilities\n",
    "            #sample from the probability distribution\n",
    "            indices = torch.multinomial(probs,1,False).squeeze(1)\n",
    "            #extract samples\n",
    "            sample = self.options[indices]\n",
    "            #set input to the sample that was actually chosen\n",
    "            samples[:,idx] = sample\n",
    "            real=patch2onehot(sample)\n",
    "            total = torch.sum(real*probs,dim=-1)\n",
    "            sprobs+=torch.log(total)\n",
    "            if idx!=L-1: probs = self.getnext(sample)\n",
    "                \n",
    "        return unpatch(samples,self.Lx).unsqueeze(-1),sprobs\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def testlabels(self,samples,B):\n",
    "        # type: (Tensor,int) -> Tuple[Tensor,Tensor]\n",
    "        \"\"\"Get logscale probabilities of all states with one spin flipped at position j\"\"\"\n",
    "        with torch.no_grad():\n",
    "            #print(\"|\",end=\"\")\n",
    "            L=self.L\n",
    "            orig=samples\n",
    "            samples=patch(samples.squeeze(-1),self.Lx)\n",
    "            \n",
    "            logprobs=torch.zeros([B,L*4],device=self.device)\n",
    "            for k in range(L*4):\n",
    "                #loop cross L flipped states (batched)\n",
    "                probs=self.reset(B).squeeze(0)\n",
    "                sprobs = torch.zeros([B],device=self.device)\n",
    "                #loop across sequence\n",
    "                for idx in range(L):\n",
    "                    \n",
    "                    sample = samples[:,idx] \n",
    "                    #kth state is flipped\n",
    "                    if idx==k//4:\n",
    "                        #multiply by 1 as a way to copy the tensor to new memory\n",
    "                        sample=sample*1.0\n",
    "                        sample[:,k%4] = 1-sample[:,k%4]\n",
    "                    real=patch2onehot(sample)\n",
    "                    sprobs+=torch.log(torch.sum(real*probs,dim=-1))\n",
    "                    if idx!=L-1: probs = self.getnext(sample)\n",
    "                logprobs[:,k]=sprobs\n",
    "        return orig,logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd565c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydir = setup_dir(op)\n",
    "#mydir = \"PTF/Rydberg/64-NoQ-B=128-K=128-Nh=128-kl=0.00/0\"#\n",
    "mydir = \"PTF/Rydberg/576-NoQ-B=256-K=256-Nh=128-kl=0.00/0\"\n",
    "\n",
    "op.L=576\n",
    "Lx=24\n",
    "tst=torch.jit.load(mydir+\"/T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ad572",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesTF = testPTF(24,Nh=op.Nh,num_layers=2)\n",
    "tesTF = torch.jit.script(tesTF)\n",
    "momentum_update(0,tesTF,tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f226335",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tesTF.Lx==8:\n",
    "    s,p = tesTF.testsample(8)\n",
    "\n",
    "    print(p)\n",
    "    print(tesTF.logprobability(s))\n",
    "\n",
    "    s2,p2=tesTF.testlabels(s,8)\n",
    "\n",
    "\n",
    "\n",
    "    s3,p3=tesTF._off_diag_labels(s,8,64,False,1)\n",
    "\n",
    "    print(s.shape,s2.shape,p2.shape,p3.shape)\n",
    "\n",
    "    print(torch.sum(p3,dim=-1)/64)\n",
    "    print(torch.sum(p2,dim=-1)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486573b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "op.K=256\n",
    "op.Q=32\n",
    "op.B=op.K*op.Q\n",
    "\n",
    "\n",
    "\n",
    "# Hamiltonian parameters\n",
    "N = op.L   # Total number of atoms\n",
    "V = 7.0     # Strength of Van der Waals interaction\n",
    "Omega = 1.0 # Rabi frequency\n",
    "delta = 1.0 # Detuning \n",
    "\n",
    "if op.hamiltonian==\"Rydberg\":\n",
    "    Lx=Ly=int(op.L**0.5)\n",
    "    op.L=Lx*Ly\n",
    "    h = Rydberg(Lx,Ly,V,Omega,delta)\n",
    "else:\n",
    "    #hope for the best here since there aren't defaults\n",
    "    h = TFIM(op.L,op.h,op.J)\n",
    "\n",
    "\n",
    "E_queue = torch.zeros([op.B],device=device)\n",
    "def fill_queue(net):\n",
    "    for i in range(op.Q):\n",
    "        print(\"|\",end=\"\")\n",
    "        if False:\n",
    "            sample,lp = net.testsample(op.K)\n",
    "            _,probs= net.testlabels(sample,op.K)\n",
    "            sqrtp=probs.mean(dim=1)/2\n",
    "            sump = torch.exp(probs/2-sqrtp.unsqueeze(1)).sum(dim=1)  \n",
    "        else:\n",
    "            sample,sump,sqrtp = net.sample_with_labelsALT(op.K,op.L,grad=False,nloops=144)\n",
    "            with torch.no_grad():\n",
    "                lp=net.logprobability(sample)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            E_i=h.localenergyALT(sample,lp,sump,sqrtp)\n",
    "            E_queue[i*op.K:(i+1)*op.K]=E_i\n",
    "t=time.time()\n",
    "fill_queue(tesTF)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errformat(m,s):\n",
    "    exp = -int(np.floor(np.log(s)/np.log(10)))\n",
    "    print( str(round(m,exp))+\" +/- \"+str(round(s,exp)))\n",
    "\n",
    "var,mean = torch.var_mean(E_queue/op.L)\n",
    "\n",
    "print(h.ground(),op.B)\n",
    "stdv=((var/op.B)**0.5).item()\n",
    "errformat(mean.item(),stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c42b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
